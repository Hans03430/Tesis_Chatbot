
Reading dataset 'csv'...

Final shared vocab size: 33274

Splitting 67395 samples into training & validation sets (20.0% used for validation)...
Training set: 53916 samples. Validation set: 13479 samples.
Sorting training & validation sets to increase training efficiency...
Initializing model...


Creating checkpoint batch files...
Initializing training...
Epochs: 100
Batch Size: 128
Optimizer: sgd
Epoch:   1/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  8.185, Training Time: 33 seconds), Stats for epoch: (Training Loss:  8.185, Training Time: 33 seconds)
Epoch:   1/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  7.163, Training Time: 33 seconds), Stats for epoch: (Training Loss:  7.674, Training Time: 66 seconds)
Epoch:   1/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  7.071, Training Time: 36 seconds), Stats for epoch: (Training Loss:  7.473, Training Time: 102 seconds)
Epoch:   1/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  7.112, Training Time: 41 seconds), Stats for epoch: (Training Loss:  7.383, Training Time: 144 seconds)
Epoch:   1/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  7.027, Training Time: 9 seconds), Stats for epoch: (Training Loss:  7.364, Training Time: 154 seconds)
Epoch:   1/100, Validation loss:  7.501, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  1.000 to  0.998
Training loss improved!
Validation loss improved!
Epoch:   2/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.875, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.875, Training Time: 30 seconds)
Epoch:   2/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.751, Training Time: 32 seconds), Stats for epoch: (Training Loss:  6.813, Training Time: 63 seconds)
Epoch:   2/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.835, Training Time: 35 seconds), Stats for epoch: (Training Loss:  6.820, Training Time: 99 seconds)
Epoch:   2/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.946, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.852, Training Time: 141 seconds)
Epoch:   2/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.853, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.852, Training Time: 151 seconds)
Epoch:   2/100, Validation loss:  7.282, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.998 to  0.995
Training loss improved!
Validation loss improved!
Epoch:   3/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.697, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.697, Training Time: 30 seconds)
Epoch:   3/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.668, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.683, Training Time: 63 seconds)
Epoch:   3/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.760, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.708, Training Time: 99 seconds)
Epoch:   3/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.882, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.752, Training Time: 141 seconds)
Epoch:   3/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.789, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.754, Training Time: 151 seconds)
Epoch:   3/100, Validation loss:  7.116, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.995 to  0.993
Training loss improved!
Validation loss improved!
Epoch:   4/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.632, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.632, Training Time: 30 seconds)
Epoch:   4/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.617, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.625, Training Time: 63 seconds)
Epoch:   4/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.713, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.654, Training Time: 99 seconds)
Epoch:   4/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.845, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.702, Training Time: 141 seconds)
Epoch:   4/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.751, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.705, Training Time: 151 seconds)
Epoch:   4/100, Validation loss:  7.161, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.993 to  0.990
Training loss improved!
Epoch:   5/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.578, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.578, Training Time: 30 seconds)
Epoch:   5/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.582, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.580, Training Time: 63 seconds)
Epoch:   5/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.684, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.615, Training Time: 100 seconds)
Epoch:   5/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.819, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.666, Training Time: 141 seconds)
Epoch:   5/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.699, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.667, Training Time: 151 seconds)
Epoch:   5/100, Validation loss:  7.136, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.990 to  0.988
Training loss improved!
Epoch:   6/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.545, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.545, Training Time: 30 seconds)
Epoch:   6/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.553, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.549, Training Time: 63 seconds)
Epoch:   6/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.664, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.587, Training Time: 99 seconds)
Epoch:   6/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.797, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.640, Training Time: 141 seconds)
Epoch:   6/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.673, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.642, Training Time: 151 seconds)
Epoch:   6/100, Validation loss:  6.946, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.988 to  0.985
Training loss improved!
Validation loss improved!
Epoch:   7/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.512, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.512, Training Time: 30 seconds)
Epoch:   7/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.539, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.526, Training Time: 63 seconds)
Epoch:   7/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.649, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.567, Training Time: 100 seconds)
Epoch:   7/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.781, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.620, Training Time: 141 seconds)
Epoch:   7/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.637, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.621, Training Time: 151 seconds)
Epoch:   7/100, Validation loss:  6.906, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.985 to  0.983
Training loss improved!
Validation loss improved!
Epoch:   8/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.470, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.470, Training Time: 30 seconds)
Epoch:   8/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.522, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.496, Training Time: 63 seconds)
Epoch:   8/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.637, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.543, Training Time: 99 seconds)
Epoch:   8/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.765, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.599, Training Time: 141 seconds)
Epoch:   8/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.620, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.600, Training Time: 151 seconds)
Epoch:   8/100, Validation loss:  6.953, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.983 to  0.980
Training loss improved!
Epoch:   9/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.472, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.472, Training Time: 30 seconds)
Epoch:   9/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.536, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.504, Training Time: 63 seconds)
Epoch:   9/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.636, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.548, Training Time: 99 seconds)
Epoch:   9/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.762, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.602, Training Time: 141 seconds)
Epoch:   9/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.600, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.601, Training Time: 151 seconds)
Epoch:   9/100, Validation loss:  6.936, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.980 to  0.978
Epoch:  10/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.461, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.461, Training Time: 30 seconds)
Epoch:  10/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.503, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.482, Training Time: 63 seconds)
Epoch:  10/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.618, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.528, Training Time: 100 seconds)
Epoch:  10/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.742, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.581, Training Time: 141 seconds)
Epoch:  10/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.571, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.581, Training Time: 151 seconds)
Epoch:  10/100, Validation loss:  6.941, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.978 to  0.975
Training loss improved!
Epoch:  11/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.445, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.445, Training Time: 30 seconds)
Epoch:  11/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.481, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.463, Training Time: 63 seconds)
Epoch:  11/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.606, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.510, Training Time: 100 seconds)
Epoch:  11/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.728, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.565, Training Time: 142 seconds)
Epoch:  11/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.550, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.564, Training Time: 151 seconds)
Epoch:  11/100, Validation loss:  6.870, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.975 to  0.973
Training loss improved!
Validation loss improved!
Epoch:  12/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.411, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.411, Training Time: 30 seconds)
Epoch:  12/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.466, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.438, Training Time: 63 seconds)
Epoch:  12/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.597, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.491, Training Time: 100 seconds)
Epoch:  12/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.714, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.547, Training Time: 141 seconds)
Epoch:  12/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.530, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.546, Training Time: 151 seconds)
Epoch:  12/100, Validation loss:  6.914, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.973 to  0.970
Training loss improved!
Epoch:  13/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.392, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.392, Training Time: 30 seconds)
Epoch:  13/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.452, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.422, Training Time: 64 seconds)
Epoch:  13/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.585, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.477, Training Time: 100 seconds)
Epoch:  13/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.696, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.531, Training Time: 142 seconds)
Epoch:  13/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.507, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.530, Training Time: 152 seconds)
Epoch:  13/100, Validation loss:  6.867, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.970 to  0.968
Training loss improved!
Validation loss improved!
Epoch:  14/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.368, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.368, Training Time: 30 seconds)
Epoch:  14/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.440, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.404, Training Time: 63 seconds)
Epoch:  14/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.571, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.460, Training Time: 99 seconds)
Epoch:  14/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.670, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.512, Training Time: 141 seconds)
Epoch:  14/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.465, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.510, Training Time: 151 seconds)
Epoch:  14/100, Validation loss:  6.844, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.968 to  0.966
Training loss improved!
Validation loss improved!
Epoch:  15/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.353, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.353, Training Time: 30 seconds)
Epoch:  15/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.422, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.387, Training Time: 64 seconds)
Epoch:  15/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.537, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.437, Training Time: 100 seconds)
Epoch:  15/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.643, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.489, Training Time: 142 seconds)
Epoch:  15/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.390, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.484, Training Time: 152 seconds)
Epoch:  15/100, Validation loss:  6.810, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.966 to  0.963
Training loss improved!
Validation loss improved!
Epoch:  16/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.335, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.335, Training Time: 30 seconds)
Epoch:  16/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.393, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.364, Training Time: 63 seconds)
Epoch:  16/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.511, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.413, Training Time: 100 seconds)
Epoch:  16/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.589, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.457, Training Time: 142 seconds)
Epoch:  16/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.333, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.450, Training Time: 152 seconds)
Epoch:  16/100, Validation loss:  6.761, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.963 to  0.961
Training loss improved!
Validation loss improved!
Epoch:  17/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.321, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.321, Training Time: 30 seconds)
Epoch:  17/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.369, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.345, Training Time: 63 seconds)
Epoch:  17/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.491, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.393, Training Time: 100 seconds)
Epoch:  17/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.569, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.437, Training Time: 142 seconds)
Epoch:  17/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.291, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.430, Training Time: 151 seconds)
Epoch:  17/100, Validation loss:  6.746, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.961 to  0.958
Training loss improved!
Validation loss improved!
Epoch:  18/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.288, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.288, Training Time: 30 seconds)
Epoch:  18/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.366, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.327, Training Time: 63 seconds)
Epoch:  18/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.474, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.376, Training Time: 100 seconds)
Epoch:  18/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.551, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.420, Training Time: 142 seconds)
Epoch:  18/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.258, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.411, Training Time: 152 seconds)
Epoch:  18/100, Validation loss:  6.909, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.958 to  0.956
Training loss improved!
Epoch:  19/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.270, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.270, Training Time: 30 seconds)
Epoch:  19/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.338, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.304, Training Time: 63 seconds)
Epoch:  19/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.457, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.355, Training Time: 100 seconds)
Epoch:  19/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.519, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.396, Training Time: 141 seconds)
Epoch:  19/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.218, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.387, Training Time: 151 seconds)
Epoch:  19/100, Validation loss:  6.766, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.956 to  0.954
Training loss improved!
Epoch:  20/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.238, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.238, Training Time: 30 seconds)
Epoch:  20/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.311, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.274, Training Time: 63 seconds)
Epoch:  20/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.438, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.329, Training Time: 100 seconds)
Epoch:  20/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.531, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.380, Training Time: 142 seconds)
Epoch:  20/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.260, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.373, Training Time: 152 seconds)
Epoch:  20/100, Validation loss:  6.783, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.954 to  0.951
Training loss improved!
Epoch:  21/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.203, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.203, Training Time: 30 seconds)
Epoch:  21/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.302, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.252, Training Time: 63 seconds)
Epoch:  21/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.424, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.309, Training Time: 100 seconds)
Epoch:  21/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.457, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.346, Training Time: 141 seconds)
Epoch:  21/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.164, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.337, Training Time: 151 seconds)
Epoch:  21/100, Validation loss:  6.729, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.951 to  0.949
Training loss improved!
Validation loss improved!
Epoch:  22/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.177, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.177, Training Time: 30 seconds)
Epoch:  22/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.271, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.224, Training Time: 63 seconds)
Epoch:  22/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.398, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.282, Training Time: 100 seconds)
Epoch:  22/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.416, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.315, Training Time: 141 seconds)
Epoch:  22/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.084, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.303, Training Time: 151 seconds)
Epoch:  22/100, Validation loss:  6.628, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.949 to  0.946
Training loss improved!
Validation loss improved!
Epoch:  23/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.156, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.156, Training Time: 30 seconds)
Epoch:  23/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.251, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.203, Training Time: 63 seconds)
Epoch:  23/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.373, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.260, Training Time: 100 seconds)
Epoch:  23/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.387, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.292, Training Time: 142 seconds)
Epoch:  23/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.013, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.277, Training Time: 152 seconds)
Epoch:  23/100, Validation loss:  6.631, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.946 to  0.944
Training loss improved!
Epoch:  24/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.123, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.123, Training Time: 30 seconds)
Epoch:  24/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.223, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.173, Training Time: 63 seconds)
Epoch:  24/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.348, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.231, Training Time: 100 seconds)
Epoch:  24/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.349, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.261, Training Time: 141 seconds)
Epoch:  24/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.975, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.246, Training Time: 151 seconds)
Epoch:  24/100, Validation loss:  6.606, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.944 to  0.942
Training loss improved!
Validation loss improved!
Epoch:  25/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.096, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.096, Training Time: 30 seconds)
Epoch:  25/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.185, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.141, Training Time: 63 seconds)
Epoch:  25/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.305, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.195, Training Time: 99 seconds)
Epoch:  25/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.292, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.219, Training Time: 141 seconds)
Epoch:  25/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.899, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.203, Training Time: 151 seconds)
Epoch:  25/100, Validation loss:  6.506, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.942 to  0.939
Training loss improved!
Validation loss improved!
Epoch:  26/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.038, Training Time: 30 seconds), Stats for epoch: (Training Loss:  6.038, Training Time: 30 seconds)
Epoch:  26/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.136, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.087, Training Time: 63 seconds)
Epoch:  26/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.245, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.140, Training Time: 100 seconds)
Epoch:  26/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.231, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.162, Training Time: 142 seconds)
Epoch:  26/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.868, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.147, Training Time: 151 seconds)
Epoch:  26/100, Validation loss:  6.535, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.939 to  0.937
Training loss improved!
Epoch:  27/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.988, Training Time: 30 seconds), Stats for epoch: (Training Loss:  5.988, Training Time: 30 seconds)
Epoch:  27/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.072, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.030, Training Time: 63 seconds)
Epoch:  27/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.192, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.084, Training Time: 99 seconds)
Epoch:  27/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.190, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.111, Training Time: 141 seconds)
Epoch:  27/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.771, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.093, Training Time: 151 seconds)
Epoch:  27/100, Validation loss:  6.340, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.937 to  0.935
Training loss improved!
Validation loss improved!
Epoch:  28/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.892, Training Time: 30 seconds), Stats for epoch: (Training Loss:  5.892, Training Time: 30 seconds)
Epoch:  28/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.994, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.943, Training Time: 63 seconds)
Epoch:  28/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.110, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.999, Training Time: 100 seconds)
Epoch:  28/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.075, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.018, Training Time: 142 seconds)
Epoch:  28/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.671, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.000, Training Time: 152 seconds)
Epoch:  28/100, Validation loss:  6.315, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.935 to  0.932
Training loss improved!
Validation loss improved!
Epoch:  29/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.792, Training Time: 30 seconds), Stats for epoch: (Training Loss:  5.792, Training Time: 30 seconds)
Epoch:  29/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.895, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.844, Training Time: 64 seconds)
Epoch:  29/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.029, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.906, Training Time: 100 seconds)
Epoch:  29/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.975, Training Time: 41 seconds), Stats for epoch: (Training Loss:  5.923, Training Time: 142 seconds)
Epoch:  29/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.507, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.901, Training Time: 152 seconds)
Epoch:  29/100, Validation loss:  6.291, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.932 to  0.930
Training loss improved!
Validation loss improved!
Epoch:  30/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.712, Training Time: 30 seconds), Stats for epoch: (Training Loss:  5.712, Training Time: 30 seconds)
Epoch:  30/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.823, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.767, Training Time: 63 seconds)
Epoch:  30/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.961, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.832, Training Time: 100 seconds)
Epoch:  30/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.930, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.856, Training Time: 142 seconds)
Epoch:  30/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.422, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.834, Training Time: 151 seconds)
Epoch:  30/100, Validation loss:  6.150, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.930 to  0.928
Training loss improved!
Validation loss improved!
Epoch:  31/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.651, Training Time: 30 seconds), Stats for epoch: (Training Loss:  5.651, Training Time: 30 seconds)
Epoch:  31/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.753, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.702, Training Time: 63 seconds)
Epoch:  31/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.883, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.762, Training Time: 100 seconds)
Epoch:  31/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.824, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.777, Training Time: 142 seconds)
Epoch:  31/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.314, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.753, Training Time: 152 seconds)
Epoch:  31/100, Validation loss:  6.169, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.928 to  0.925
Training loss improved!
Epoch:  32/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.530, Training Time: 30 seconds), Stats for epoch: (Training Loss:  5.530, Training Time: 30 seconds)
Epoch:  32/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.675, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.602, Training Time: 63 seconds)
Epoch:  32/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.808, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.671, Training Time: 100 seconds)
Epoch:  32/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.728, Training Time: 41 seconds), Stats for epoch: (Training Loss:  5.685, Training Time: 142 seconds)
Epoch:  32/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.263, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.663, Training Time: 151 seconds)
Epoch:  32/100, Validation loss:  6.125, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.925 to  0.923
Training loss improved!
Validation loss improved!
Epoch:  33/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.496, Training Time: 30 seconds), Stats for epoch: (Training Loss:  5.496, Training Time: 30 seconds)
Epoch:  33/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.611, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.554, Training Time: 63 seconds)
Epoch:  33/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.739, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.615, Training Time: 100 seconds)
Epoch:  33/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.659, Training Time: 41 seconds), Stats for epoch: (Training Loss:  5.626, Training Time: 142 seconds)
Epoch:  33/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.028, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.595, Training Time: 151 seconds)
Epoch:  33/100, Validation loss:  6.045, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.923 to  0.921
Training loss improved!
Validation loss improved!
Epoch:  34/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.390, Training Time: 30 seconds), Stats for epoch: (Training Loss:  5.390, Training Time: 30 seconds)
Epoch:  34/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.543, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.467, Training Time: 64 seconds)
Epoch:  34/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.674, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.536, Training Time: 100 seconds)
Epoch:  34/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.573, Training Time: 41 seconds), Stats for epoch: (Training Loss:  5.545, Training Time: 142 seconds)
Epoch:  34/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.917, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.512, Training Time: 152 seconds)
Epoch:  34/100, Validation loss:  6.017, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.921 to  0.918
Training loss improved!
Validation loss improved!
Epoch:  35/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.312, Training Time: 30 seconds), Stats for epoch: (Training Loss:  5.312, Training Time: 30 seconds)
Epoch:  35/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.474, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.393, Training Time: 63 seconds)
Epoch:  35/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.611, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.466, Training Time: 100 seconds)
Epoch:  35/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.497, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.473, Training Time: 142 seconds)
Epoch:  35/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.792, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.438, Training Time: 152 seconds)
Epoch:  35/100, Validation loss:  5.915, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.918 to  0.916
Training loss improved!
Validation loss improved!
Epoch:  36/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.225, Training Time: 30 seconds), Stats for epoch: (Training Loss:  5.225, Training Time: 30 seconds)
Epoch:  36/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.414, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.319, Training Time: 64 seconds)
Epoch:  36/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.550, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.396, Training Time: 100 seconds)
Epoch:  36/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.436, Training Time: 41 seconds), Stats for epoch: (Training Loss:  5.406, Training Time: 142 seconds)
Epoch:  36/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.685, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.368, Training Time: 152 seconds)
Epoch:  36/100, Validation loss:  5.807, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.916 to  0.914
Training loss improved!
Validation loss improved!
Epoch:  37/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.147, Training Time: 30 seconds), Stats for epoch: (Training Loss:  5.147, Training Time: 30 seconds)
Epoch:  37/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.354, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.250, Training Time: 63 seconds)
Epoch:  37/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.494, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.332, Training Time: 100 seconds)
Epoch:  37/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.368, Training Time: 41 seconds), Stats for epoch: (Training Loss:  5.341, Training Time: 141 seconds)
Epoch:  37/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.600, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.302, Training Time: 151 seconds)
Epoch:  37/100, Validation loss:  5.652, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.914 to  0.912
Training loss improved!
Validation loss improved!
Epoch:  38/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.083, Training Time: 30 seconds), Stats for epoch: (Training Loss:  5.083, Training Time: 30 seconds)
Epoch:  38/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.299, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.191, Training Time: 63 seconds)
Epoch:  38/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.436, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.272, Training Time: 99 seconds)
Epoch:  38/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.276, Training Time: 41 seconds), Stats for epoch: (Training Loss:  5.273, Training Time: 141 seconds)
Epoch:  38/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.508, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.234, Training Time: 151 seconds)
Epoch:  38/100, Validation loss:  5.585, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.912 to  0.909
Training loss improved!
Validation loss improved!
Epoch:  39/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.036, Training Time: 30 seconds), Stats for epoch: (Training Loss:  5.036, Training Time: 30 seconds)
Epoch:  39/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.244, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.140, Training Time: 63 seconds)
Epoch:  39/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.383, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.221, Training Time: 100 seconds)
Epoch:  39/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.232, Training Time: 41 seconds), Stats for epoch: (Training Loss:  5.223, Training Time: 142 seconds)
Epoch:  39/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.422, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.182, Training Time: 151 seconds)
Epoch:  39/100, Validation loss:  5.551, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.909 to  0.907
Training loss improved!
Validation loss improved!
Epoch:  40/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.966, Training Time: 30 seconds), Stats for epoch: (Training Loss:  4.966, Training Time: 30 seconds)
Epoch:  40/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.187, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.076, Training Time: 64 seconds)
Epoch:  40/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.329, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.161, Training Time: 100 seconds)
Epoch:  40/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.171, Training Time: 41 seconds), Stats for epoch: (Training Loss:  5.163, Training Time: 142 seconds)
Epoch:  40/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.562, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.132, Training Time: 151 seconds)
Epoch:  40/100, Validation loss:  5.582, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.907 to  0.905
Training loss improved!
Epoch:  41/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.944, Training Time: 30 seconds), Stats for epoch: (Training Loss:  4.944, Training Time: 30 seconds)
Epoch:  41/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.143, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.043, Training Time: 64 seconds)
Epoch:  41/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.280, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.122, Training Time: 100 seconds)
Epoch:  41/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.124, Training Time: 41 seconds), Stats for epoch: (Training Loss:  5.123, Training Time: 142 seconds)
Epoch:  41/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.241, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.077, Training Time: 152 seconds)
Epoch:  41/100, Validation loss:  5.473, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.905 to  0.902
Training loss improved!
Validation loss improved!
Epoch:  42/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.835, Training Time: 30 seconds), Stats for epoch: (Training Loss:  4.835, Training Time: 30 seconds)
Epoch:  42/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.082, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.959, Training Time: 63 seconds)
Epoch:  42/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.225, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.047, Training Time: 100 seconds)
Epoch:  42/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.063, Training Time: 41 seconds), Stats for epoch: (Training Loss:  5.051, Training Time: 141 seconds)
Epoch:  42/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.164, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.005, Training Time: 151 seconds)
Epoch:  42/100, Validation loss:  5.458, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.902 to  0.900
Training loss improved!
Validation loss improved!
Epoch:  43/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.782, Training Time: 30 seconds), Stats for epoch: (Training Loss:  4.782, Training Time: 30 seconds)
Epoch:  43/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.027, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.904, Training Time: 64 seconds)
Epoch:  43/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.173, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.994, Training Time: 100 seconds)
Epoch:  43/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.021, Training Time: 41 seconds), Stats for epoch: (Training Loss:  5.001, Training Time: 142 seconds)
Epoch:  43/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.084, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.953, Training Time: 152 seconds)
Epoch:  43/100, Validation loss:  5.380, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.900 to  0.898
Training loss improved!
Validation loss improved!
Epoch:  44/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.717, Training Time: 30 seconds), Stats for epoch: (Training Loss:  4.717, Training Time: 30 seconds)
Epoch:  44/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.970, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.844, Training Time: 63 seconds)
Epoch:  44/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.121, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.936, Training Time: 100 seconds)
Epoch:  44/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.973, Training Time: 41 seconds), Stats for epoch: (Training Loss:  4.946, Training Time: 142 seconds)
Epoch:  44/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.022, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.897, Training Time: 152 seconds)
Epoch:  44/100, Validation loss:  5.359, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.898 to  0.896
Training loss improved!
Validation loss improved!
Epoch:  45/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.656, Training Time: 30 seconds), Stats for epoch: (Training Loss:  4.656, Training Time: 30 seconds)
Epoch:  45/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.917, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.786, Training Time: 63 seconds)
Epoch:  45/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.068, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.880, Training Time: 100 seconds)
Epoch:  45/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.933, Training Time: 41 seconds), Stats for epoch: (Training Loss:  4.893, Training Time: 142 seconds)
Epoch:  45/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.027, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.848, Training Time: 152 seconds)
Epoch:  45/100, Validation loss:  5.363, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.896 to  0.893
Training loss improved!
Epoch:  46/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.593, Training Time: 30 seconds), Stats for epoch: (Training Loss:  4.593, Training Time: 30 seconds)
Epoch:  46/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.855, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.724, Training Time: 63 seconds)
Epoch:  46/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.017, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.822, Training Time: 100 seconds)
Epoch:  46/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.883, Training Time: 41 seconds), Stats for epoch: (Training Loss:  4.837, Training Time: 142 seconds)
Epoch:  46/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.971, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.792, Training Time: 151 seconds)
Epoch:  46/100, Validation loss:  5.296, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.893 to  0.891
Training loss improved!
Validation loss improved!
Epoch:  47/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.528, Training Time: 30 seconds), Stats for epoch: (Training Loss:  4.528, Training Time: 30 seconds)
Epoch:  47/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.798, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.663, Training Time: 63 seconds)
Epoch:  47/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.959, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.762, Training Time: 100 seconds)
Epoch:  47/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.844, Training Time: 41 seconds), Stats for epoch: (Training Loss:  4.782, Training Time: 142 seconds)
Epoch:  47/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.865, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.735, Training Time: 152 seconds)
Epoch:  47/100, Validation loss:  5.295, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.891 to  0.889
Training loss improved!
Validation loss improved!
Epoch:  48/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.472, Training Time: 30 seconds), Stats for epoch: (Training Loss:  4.472, Training Time: 30 seconds)
Epoch:  48/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.736, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.604, Training Time: 63 seconds)
Epoch:  48/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.906, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.705, Training Time: 100 seconds)
Epoch:  48/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.801, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.729, Training Time: 142 seconds)
Epoch:  48/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.812, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.681, Training Time: 152 seconds)
Epoch:  48/100, Validation loss:  5.254, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.889 to  0.887
Training loss improved!
Validation loss improved!
Epoch:  49/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.404, Training Time: 30 seconds), Stats for epoch: (Training Loss:  4.404, Training Time: 30 seconds)
Epoch:  49/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.677, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.541, Training Time: 63 seconds)
Epoch:  49/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.852, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.645, Training Time: 100 seconds)
Epoch:  49/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.756, Training Time: 41 seconds), Stats for epoch: (Training Loss:  4.673, Training Time: 142 seconds)
Epoch:  49/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.768, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.625, Training Time: 152 seconds)
Epoch:  49/100, Validation loss:  5.226, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.887 to  0.885
Training loss improved!
Validation loss improved!
Epoch:  50/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.341, Training Time: 30 seconds), Stats for epoch: (Training Loss:  4.341, Training Time: 30 seconds)
Epoch:  50/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.617, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.479, Training Time: 64 seconds)
Epoch:  50/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.794, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.584, Training Time: 100 seconds)
Epoch:  50/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.712, Training Time: 41 seconds), Stats for epoch: (Training Loss:  4.616, Training Time: 142 seconds)
Epoch:  50/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.737, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.570, Training Time: 151 seconds)
Epoch:  50/100, Validation loss:  5.231, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.885 to  0.882
Training loss improved!
Epoch:  51/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.279, Training Time: 30 seconds), Stats for epoch: (Training Loss:  4.279, Training Time: 30 seconds)
Epoch:  51/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.557, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.418, Training Time: 63 seconds)
Epoch:  51/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.742, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.526, Training Time: 100 seconds)
Epoch:  51/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.676, Training Time: 41 seconds), Stats for epoch: (Training Loss:  4.563, Training Time: 141 seconds)
Epoch:  51/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.688, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.518, Training Time: 151 seconds)
Epoch:  51/100, Validation loss:  5.204, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.882 to  0.880
Training loss improved!
Validation loss improved!
Epoch:  52/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.214, Training Time: 30 seconds), Stats for epoch: (Training Loss:  4.214, Training Time: 30 seconds)
Epoch:  52/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.495, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.354, Training Time: 64 seconds)
Epoch:  52/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.684, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.464, Training Time: 100 seconds)
Epoch:  52/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.629, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.505, Training Time: 142 seconds)
Epoch:  52/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.655, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.461, Training Time: 152 seconds)
Epoch:  52/100, Validation loss:  5.174, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.880 to  0.878
Training loss improved!
Validation loss improved!
Epoch:  53/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.150, Training Time: 30 seconds), Stats for epoch: (Training Loss:  4.150, Training Time: 30 seconds)
Epoch:  53/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.432, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.291, Training Time: 63 seconds)
Epoch:  53/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.630, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.404, Training Time: 100 seconds)
Epoch:  53/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.586, Training Time: 41 seconds), Stats for epoch: (Training Loss:  4.449, Training Time: 142 seconds)
Epoch:  53/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.595, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.405, Training Time: 151 seconds)
Epoch:  53/100, Validation loss:  5.165, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.878 to  0.876
Training loss improved!
Validation loss improved!
Epoch:  54/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.085, Training Time: 30 seconds), Stats for epoch: (Training Loss:  4.085, Training Time: 30 seconds)
Epoch:  54/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.371, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.228, Training Time: 64 seconds)
Epoch:  54/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.573, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.343, Training Time: 100 seconds)
Epoch:  54/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.549, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.394, Training Time: 142 seconds)
Epoch:  54/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.549, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.350, Training Time: 152 seconds)
Epoch:  54/100, Validation loss:  5.151, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.876 to  0.874
Training loss improved!
Validation loss improved!
Epoch:  55/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.016, Training Time: 30 seconds), Stats for epoch: (Training Loss:  4.016, Training Time: 30 seconds)
Epoch:  55/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.308, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.162, Training Time: 64 seconds)
Epoch:  55/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.511, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.279, Training Time: 100 seconds)
Epoch:  55/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.501, Training Time: 41 seconds), Stats for epoch: (Training Loss:  4.334, Training Time: 142 seconds)
Epoch:  55/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.513, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.292, Training Time: 152 seconds)
Epoch:  55/100, Validation loss:  5.135, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.874 to  0.871
Training loss improved!
Validation loss improved!
Epoch:  56/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.952, Training Time: 30 seconds), Stats for epoch: (Training Loss:  3.952, Training Time: 30 seconds)
Epoch:  56/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.243, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.098, Training Time: 63 seconds)
Epoch:  56/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.453, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.216, Training Time: 100 seconds)
Epoch:  56/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.458, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.276, Training Time: 142 seconds)
Epoch:  56/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.471, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.234, Training Time: 152 seconds)
Epoch:  56/100, Validation loss:  5.144, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.871 to  0.869
Training loss improved!
Epoch:  57/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.884, Training Time: 30 seconds), Stats for epoch: (Training Loss:  3.884, Training Time: 30 seconds)
Epoch:  57/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.181, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.032, Training Time: 64 seconds)
Epoch:  57/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.393, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.153, Training Time: 100 seconds)
Epoch:  57/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.413, Training Time: 41 seconds), Stats for epoch: (Training Loss:  4.218, Training Time: 142 seconds)
Epoch:  57/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.429, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.177, Training Time: 152 seconds)
Epoch:  57/100, Validation loss:  5.137, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.869 to  0.867
Training loss improved!
Epoch:  58/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.821, Training Time: 30 seconds), Stats for epoch: (Training Loss:  3.821, Training Time: 30 seconds)
Epoch:  58/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.112, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.967, Training Time: 64 seconds)
Epoch:  58/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.334, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.089, Training Time: 100 seconds)
Epoch:  58/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.377, Training Time: 41 seconds), Stats for epoch: (Training Loss:  4.161, Training Time: 142 seconds)
Epoch:  58/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.387, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.121, Training Time: 152 seconds)
Epoch:  58/100, Validation loss:  5.129, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.867 to  0.865
Training loss improved!
Validation loss improved!
Epoch:  59/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.764, Training Time: 30 seconds), Stats for epoch: (Training Loss:  3.764, Training Time: 30 seconds)
Epoch:  59/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.047, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.905, Training Time: 63 seconds)
Epoch:  59/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.273, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.028, Training Time: 100 seconds)
Epoch:  59/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.328, Training Time: 41 seconds), Stats for epoch: (Training Loss:  4.103, Training Time: 142 seconds)
Epoch:  59/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.352, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.064, Training Time: 151 seconds)
Epoch:  59/100, Validation loss:  5.125, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.865 to  0.863
Training loss improved!
Validation loss improved!
Epoch:  60/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.691, Training Time: 30 seconds), Stats for epoch: (Training Loss:  3.691, Training Time: 30 seconds)
Epoch:  60/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.981, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.836, Training Time: 63 seconds)
Epoch:  60/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.209, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.960, Training Time: 100 seconds)
Epoch:  60/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.288, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.042, Training Time: 142 seconds)
Epoch:  60/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.317, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.004, Training Time: 152 seconds)
Epoch:  60/100, Validation loss:  5.133, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.863 to  0.861
Training loss improved!
Epoch:  61/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.618, Training Time: 30 seconds), Stats for epoch: (Training Loss:  3.618, Training Time: 30 seconds)
Epoch:  61/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.917, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.768, Training Time: 64 seconds)
Epoch:  61/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.150, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.895, Training Time: 100 seconds)
Epoch:  61/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.242, Training Time: 41 seconds), Stats for epoch: (Training Loss:  3.982, Training Time: 142 seconds)
Epoch:  61/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.265, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.944, Training Time: 152 seconds)
Epoch:  61/100, Validation loss:  5.111, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.861 to  0.858
Training loss improved!
Validation loss improved!
Epoch:  62/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.564, Training Time: 30 seconds), Stats for epoch: (Training Loss:  3.564, Training Time: 30 seconds)
Epoch:  62/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.848, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.706, Training Time: 64 seconds)
Epoch:  62/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.086, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.833, Training Time: 100 seconds)
Epoch:  62/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.203, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.925, Training Time: 142 seconds)
Epoch:  62/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.229, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.889, Training Time: 152 seconds)
Epoch:  62/100, Validation loss:  5.128, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.858 to  0.856
Training loss improved!
Epoch:  63/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.487, Training Time: 30 seconds), Stats for epoch: (Training Loss:  3.487, Training Time: 30 seconds)
Epoch:  63/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.784, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.635, Training Time: 64 seconds)
Epoch:  63/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.024, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.765, Training Time: 100 seconds)
Epoch:  63/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.153, Training Time: 41 seconds), Stats for epoch: (Training Loss:  3.862, Training Time: 142 seconds)
Epoch:  63/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.182, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.826, Training Time: 152 seconds)
Epoch:  63/100, Validation loss:  5.153, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.856 to  0.854
Training loss improved!
Epoch:  64/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.430, Training Time: 30 seconds), Stats for epoch: (Training Loss:  3.430, Training Time: 30 seconds)
Epoch:  64/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.711, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.571, Training Time: 64 seconds)
Epoch:  64/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.959, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.700, Training Time: 100 seconds)
Epoch:  64/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.112, Training Time: 41 seconds), Stats for epoch: (Training Loss:  3.803, Training Time: 142 seconds)
Epoch:  64/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.139, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.769, Training Time: 152 seconds)
Epoch:  64/100, Validation loss:  5.132, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.854 to  0.852
Training loss improved!
Epoch:  65/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.350, Training Time: 30 seconds), Stats for epoch: (Training Loss:  3.350, Training Time: 30 seconds)
Epoch:  65/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.645, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.497, Training Time: 64 seconds)
Epoch:  65/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.903, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.633, Training Time: 100 seconds)
Epoch:  65/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.062, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.740, Training Time: 142 seconds)
Epoch:  65/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.156, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.709, Training Time: 152 seconds)
Epoch:  65/100, Validation loss:  5.121, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.852 to  0.850
Training loss improved!
Epoch:  66/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.291, Training Time: 30 seconds), Stats for epoch: (Training Loss:  3.291, Training Time: 30 seconds)
Epoch:  66/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.583, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.437, Training Time: 63 seconds)
Epoch:  66/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.832, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.569, Training Time: 100 seconds)
Epoch:  66/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.019, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.681, Training Time: 142 seconds)
Epoch:  66/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.085, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.650, Training Time: 152 seconds)
Epoch:  66/100, Validation loss:  5.141, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.850 to  0.848
Training loss improved!
Epoch:  67/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.230, Training Time: 30 seconds), Stats for epoch: (Training Loss:  3.230, Training Time: 30 seconds)
Epoch:  67/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.516, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.373, Training Time: 64 seconds)
Epoch:  67/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.768, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.505, Training Time: 100 seconds)
Epoch:  67/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.972, Training Time: 41 seconds), Stats for epoch: (Training Loss:  3.621, Training Time: 142 seconds)
Epoch:  67/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.077, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.593, Training Time: 152 seconds)
Epoch:  67/100, Validation loss:  5.193, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.848 to  0.846
Training loss improved!
Epoch:  68/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.174, Training Time: 30 seconds), Stats for epoch: (Training Loss:  3.174, Training Time: 30 seconds)
Epoch:  68/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.440, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.307, Training Time: 63 seconds)
Epoch:  68/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.708, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.441, Training Time: 100 seconds)
Epoch:  68/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.919, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.560, Training Time: 142 seconds)
Epoch:  68/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.990, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.530, Training Time: 152 seconds)
Epoch:  68/100, Validation loss:  5.191, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.846 to  0.843
Training loss improved!
Epoch:  69/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.095, Training Time: 30 seconds), Stats for epoch: (Training Loss:  3.095, Training Time: 30 seconds)
Epoch:  69/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.380, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.238, Training Time: 64 seconds)
Epoch:  69/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.645, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.373, Training Time: 100 seconds)
Epoch:  69/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.877, Training Time: 41 seconds), Stats for epoch: (Training Loss:  3.499, Training Time: 142 seconds)
Epoch:  69/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.946, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.470, Training Time: 152 seconds)
Epoch:  69/100, Validation loss:  5.172, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.843 to  0.841
Training loss improved!
Epoch:  70/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.039, Training Time: 30 seconds), Stats for epoch: (Training Loss:  3.039, Training Time: 30 seconds)
Epoch:  70/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.316, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.178, Training Time: 64 seconds)
Epoch:  70/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.578, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.311, Training Time: 100 seconds)
Epoch:  70/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.832, Training Time: 41 seconds), Stats for epoch: (Training Loss:  3.441, Training Time: 142 seconds)
Epoch:  70/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.913, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.414, Training Time: 152 seconds)
Epoch:  70/100, Validation loss:  5.193, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.841 to  0.839
Training loss improved!
Epoch:  71/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.977, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.977, Training Time: 30 seconds)
Epoch:  71/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.248, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.112, Training Time: 64 seconds)
Epoch:  71/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.510, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.245, Training Time: 100 seconds)
Epoch:  71/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.785, Training Time: 41 seconds), Stats for epoch: (Training Loss:  3.380, Training Time: 142 seconds)
Epoch:  71/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.866, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.353, Training Time: 152 seconds)
Epoch:  71/100, Validation loss:  5.198, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.839 to  0.837
Training loss improved!
Epoch:  72/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.916, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.916, Training Time: 30 seconds)
Epoch:  72/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.180, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.048, Training Time: 63 seconds)
Epoch:  72/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.451, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.183, Training Time: 100 seconds)
Epoch:  72/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.745, Training Time: 41 seconds), Stats for epoch: (Training Loss:  3.323, Training Time: 142 seconds)
Epoch:  72/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.847, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.298, Training Time: 152 seconds)
Epoch:  72/100, Validation loss:  5.238, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.837 to  0.835
Training loss improved!
Epoch:  73/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.864, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.864, Training Time: 30 seconds)
Epoch:  73/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.113, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.988, Training Time: 64 seconds)
Epoch:  73/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.391, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.122, Training Time: 100 seconds)
Epoch:  73/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.697, Training Time: 41 seconds), Stats for epoch: (Training Loss:  3.266, Training Time: 142 seconds)
Epoch:  73/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.777, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.241, Training Time: 152 seconds)
Epoch:  73/100, Validation loss:  5.203, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.835 to  0.833
Training loss improved!
Epoch:  74/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.794, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.794, Training Time: 30 seconds)
Epoch:  74/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.049, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.922, Training Time: 63 seconds)
Epoch:  74/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.322, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.055, Training Time: 100 seconds)
Epoch:  74/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.644, Training Time: 41 seconds), Stats for epoch: (Training Loss:  3.202, Training Time: 142 seconds)
Epoch:  74/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.743, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.178, Training Time: 152 seconds)
Epoch:  74/100, Validation loss:  5.259, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.833 to  0.831
Training loss improved!
Epoch:  75/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.728, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.728, Training Time: 30 seconds)
Epoch:  75/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.986, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.857, Training Time: 63 seconds)
Epoch:  75/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.255, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.990, Training Time: 100 seconds)
Epoch:  75/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.601, Training Time: 41 seconds), Stats for epoch: (Training Loss:  3.143, Training Time: 142 seconds)
Epoch:  75/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.720, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.121, Training Time: 152 seconds)
Epoch:  75/100, Validation loss:  5.260, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.831 to  0.829
Training loss improved!
Epoch:  76/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.673, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.673, Training Time: 30 seconds)
Epoch:  76/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.923, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.798, Training Time: 63 seconds)
Epoch:  76/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.192, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.929, Training Time: 100 seconds)
Epoch:  76/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.546, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.083, Training Time: 142 seconds)
Epoch:  76/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.686, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.063, Training Time: 152 seconds)
Epoch:  76/100, Validation loss:  5.261, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.829 to  0.827
Training loss improved!
Epoch:  77/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.614, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.614, Training Time: 30 seconds)
Epoch:  77/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.856, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.735, Training Time: 64 seconds)
Epoch:  77/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.133, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.868, Training Time: 100 seconds)
Epoch:  77/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.492, Training Time: 41 seconds), Stats for epoch: (Training Loss:  3.024, Training Time: 142 seconds)
Epoch:  77/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.626, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.003, Training Time: 152 seconds)
Epoch:  77/100, Validation loss:  5.339, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.827 to  0.825
Training loss improved!
Epoch:  78/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.567, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.567, Training Time: 30 seconds)
Epoch:  78/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.803, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.685, Training Time: 64 seconds)
Epoch:  78/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.072, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.814, Training Time: 100 seconds)
Epoch:  78/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.444, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.972, Training Time: 142 seconds)
Epoch:  78/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.586, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.951, Training Time: 152 seconds)
Epoch:  78/100, Validation loss:  5.334, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.825 to  0.823
Training loss improved!
Backup to models/csv/20201107_070848_backup_2_951 complete!
Epoch:  79/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.508, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.508, Training Time: 30 seconds)
Epoch:  79/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.738, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.623, Training Time: 64 seconds)
Epoch:  79/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.008, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.751, Training Time: 100 seconds)
Epoch:  79/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.400, Training Time: 41 seconds), Stats for epoch: (Training Loss:  2.913, Training Time: 142 seconds)
Epoch:  79/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.558, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.895, Training Time: 152 seconds)
Epoch:  79/100, Validation loss:  5.335, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.823 to  0.821
Training loss improved!
Epoch:  80/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.446, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.446, Training Time: 30 seconds)
Epoch:  80/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.679, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.562, Training Time: 64 seconds)
Epoch:  80/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.947, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.691, Training Time: 100 seconds)
Epoch:  80/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.357, Training Time: 41 seconds), Stats for epoch: (Training Loss:  2.857, Training Time: 142 seconds)
Epoch:  80/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.504, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.839, Training Time: 152 seconds)
Epoch:  80/100, Validation loss:  5.365, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.821 to  0.819
Training loss improved!
Epoch:  81/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.405, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.405, Training Time: 30 seconds)
Epoch:  81/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.614, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.509, Training Time: 64 seconds)
Epoch:  81/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.875, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.631, Training Time: 100 seconds)
Epoch:  81/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.316, Training Time: 41 seconds), Stats for epoch: (Training Loss:  2.802, Training Time: 142 seconds)
Epoch:  81/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.462, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.785, Training Time: 152 seconds)
Epoch:  81/100, Validation loss:  5.356, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.819 to  0.816
Training loss improved!
Epoch:  82/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.357, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.357, Training Time: 30 seconds)
Epoch:  82/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.557, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.457, Training Time: 63 seconds)
Epoch:  82/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.834, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.582, Training Time: 100 seconds)
Epoch:  82/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.255, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.750, Training Time: 142 seconds)
Epoch:  82/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.431, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.734, Training Time: 152 seconds)
Epoch:  82/100, Validation loss:  5.406, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.816 to  0.814
Training loss improved!
Epoch:  83/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.306, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.306, Training Time: 30 seconds)
Epoch:  83/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.498, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.402, Training Time: 64 seconds)
Epoch:  83/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.760, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.521, Training Time: 100 seconds)
Epoch:  83/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.204, Training Time: 41 seconds), Stats for epoch: (Training Loss:  2.692, Training Time: 142 seconds)
Epoch:  83/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.385, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.676, Training Time: 152 seconds)
Epoch:  83/100, Validation loss:  5.482, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.814 to  0.812
Training loss improved!
Epoch:  84/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.245, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.245, Training Time: 30 seconds)
Epoch:  84/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.448, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.347, Training Time: 64 seconds)
Epoch:  84/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.704, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.466, Training Time: 100 seconds)
Epoch:  84/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.150, Training Time: 41 seconds), Stats for epoch: (Training Loss:  2.637, Training Time: 142 seconds)
Epoch:  84/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.346, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.622, Training Time: 152 seconds)
Epoch:  84/100, Validation loss:  5.459, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.812 to  0.810
Training loss improved!
Epoch:  85/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.200, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.200, Training Time: 30 seconds)
Epoch:  85/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.385, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.293, Training Time: 64 seconds)
Epoch:  85/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.653, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.413, Training Time: 100 seconds)
Epoch:  85/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.108, Training Time: 41 seconds), Stats for epoch: (Training Loss:  2.586, Training Time: 142 seconds)
Epoch:  85/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.311, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.572, Training Time: 152 seconds)
Epoch:  85/100, Validation loss:  5.482, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.810 to  0.808
Training loss improved!
Epoch:  86/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.153, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.153, Training Time: 30 seconds)
Epoch:  86/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.338, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.246, Training Time: 64 seconds)
Epoch:  86/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.594, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.362, Training Time: 100 seconds)
Epoch:  86/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.047, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.533, Training Time: 142 seconds)
Epoch:  86/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.263, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.519, Training Time: 152 seconds)
Epoch:  86/100, Validation loss:  5.516, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.808 to  0.806
Training loss improved!
Epoch:  87/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.116, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.116, Training Time: 30 seconds)
Epoch:  87/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.283, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.199, Training Time: 64 seconds)
Epoch:  87/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.538, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.312, Training Time: 100 seconds)
Epoch:  87/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.014, Training Time: 41 seconds), Stats for epoch: (Training Loss:  2.487, Training Time: 142 seconds)
Epoch:  87/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.234, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.474, Training Time: 152 seconds)
Epoch:  87/100, Validation loss:  5.549, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.806 to  0.804
Training loss improved!
Backup to models/csv/20201107_070848_backup_2_474 complete!
Epoch:  88/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.073, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.073, Training Time: 30 seconds)
Epoch:  88/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.229, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.151, Training Time: 63 seconds)
Epoch:  88/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.478, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.260, Training Time: 100 seconds)
Epoch:  88/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.965, Training Time: 41 seconds), Stats for epoch: (Training Loss:  2.436, Training Time: 142 seconds)
Epoch:  88/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.195, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.424, Training Time: 151 seconds)
Epoch:  88/100, Validation loss:  5.601, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.804 to  0.802
Training loss improved!
Epoch:  89/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.024, Training Time: 30 seconds), Stats for epoch: (Training Loss:  2.024, Training Time: 30 seconds)
Epoch:  89/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.174, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.099, Training Time: 64 seconds)
Epoch:  89/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.425, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.208, Training Time: 100 seconds)
Epoch:  89/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.909, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.383, Training Time: 142 seconds)
Epoch:  89/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.151, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.371, Training Time: 152 seconds)
Epoch:  89/100, Validation loss:  5.618, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.802 to  0.800
Training loss improved!
Epoch:  90/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.982, Training Time: 30 seconds), Stats for epoch: (Training Loss:  1.982, Training Time: 30 seconds)
Epoch:  90/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.128, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.055, Training Time: 63 seconds)
Epoch:  90/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.369, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.160, Training Time: 100 seconds)
Epoch:  90/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.862, Training Time: 41 seconds), Stats for epoch: (Training Loss:  2.335, Training Time: 142 seconds)
Epoch:  90/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.116, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.324, Training Time: 152 seconds)
Epoch:  90/100, Validation loss:  5.608, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.800 to  0.798
Training loss improved!
Epoch:  91/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.939, Training Time: 30 seconds), Stats for epoch: (Training Loss:  1.939, Training Time: 30 seconds)
Epoch:  91/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.078, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.008, Training Time: 63 seconds)
Epoch:  91/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.306, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.108, Training Time: 100 seconds)
Epoch:  91/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.812, Training Time: 41 seconds), Stats for epoch: (Training Loss:  2.284, Training Time: 141 seconds)
Epoch:  91/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.090, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.274, Training Time: 151 seconds)
Epoch:  91/100, Validation loss:  5.635, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.798 to  0.796
Training loss improved!
Epoch:  92/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.899, Training Time: 30 seconds), Stats for epoch: (Training Loss:  1.899, Training Time: 30 seconds)
Epoch:  92/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.031, Training Time: 33 seconds), Stats for epoch: (Training Loss:  1.965, Training Time: 63 seconds)
Epoch:  92/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.262, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.064, Training Time: 100 seconds)
Epoch:  92/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.762, Training Time: 41 seconds), Stats for epoch: (Training Loss:  2.239, Training Time: 142 seconds)
Epoch:  92/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.057, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.229, Training Time: 152 seconds)
Epoch:  92/100, Validation loss:  5.670, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.796 to  0.794
Training loss improved!
Epoch:  93/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.877, Training Time: 30 seconds), Stats for epoch: (Training Loss:  1.877, Training Time: 30 seconds)
Epoch:  93/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  1.987, Training Time: 33 seconds), Stats for epoch: (Training Loss:  1.932, Training Time: 63 seconds)
Epoch:  93/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.212, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.025, Training Time: 100 seconds)
Epoch:  93/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.727, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.201, Training Time: 142 seconds)
Epoch:  93/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.000, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.190, Training Time: 152 seconds)
Epoch:  93/100, Validation loss:  5.693, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.794 to  0.792
Training loss improved!
Epoch:  94/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.831, Training Time: 30 seconds), Stats for epoch: (Training Loss:  1.831, Training Time: 30 seconds)
Epoch:  94/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  1.941, Training Time: 33 seconds), Stats for epoch: (Training Loss:  1.886, Training Time: 64 seconds)
Epoch:  94/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.163, Training Time: 36 seconds), Stats for epoch: (Training Loss:  1.978, Training Time: 100 seconds)
Epoch:  94/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.670, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.151, Training Time: 142 seconds)
Epoch:  94/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  1.975, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.142, Training Time: 152 seconds)
Epoch:  94/100, Validation loss:  5.691, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.792 to  0.790
Training loss improved!
Epoch:  95/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.786, Training Time: 30 seconds), Stats for epoch: (Training Loss:  1.786, Training Time: 30 seconds)
Epoch:  95/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  1.892, Training Time: 33 seconds), Stats for epoch: (Training Loss:  1.839, Training Time: 64 seconds)
Epoch:  95/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.104, Training Time: 36 seconds), Stats for epoch: (Training Loss:  1.927, Training Time: 100 seconds)
Epoch:  95/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.614, Training Time: 41 seconds), Stats for epoch: (Training Loss:  2.099, Training Time: 142 seconds)
Epoch:  95/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  1.945, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.091, Training Time: 152 seconds)
Epoch:  95/100, Validation loss:  5.733, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.790 to  0.788
Training loss improved!
Epoch:  96/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.747, Training Time: 30 seconds), Stats for epoch: (Training Loss:  1.747, Training Time: 30 seconds)
Epoch:  96/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  1.846, Training Time: 33 seconds), Stats for epoch: (Training Loss:  1.797, Training Time: 64 seconds)
Epoch:  96/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.060, Training Time: 36 seconds), Stats for epoch: (Training Loss:  1.884, Training Time: 100 seconds)
Epoch:  96/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.575, Training Time: 41 seconds), Stats for epoch: (Training Loss:  2.057, Training Time: 142 seconds)
Epoch:  96/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  1.886, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.048, Training Time: 152 seconds)
Epoch:  96/100, Validation loss:  5.754, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.788 to  0.786
Training loss improved!
Epoch:  97/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.717, Training Time: 30 seconds), Stats for epoch: (Training Loss:  1.717, Training Time: 30 seconds)
Epoch:  97/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  1.799, Training Time: 33 seconds), Stats for epoch: (Training Loss:  1.758, Training Time: 64 seconds)
Epoch:  97/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.009, Training Time: 36 seconds), Stats for epoch: (Training Loss:  1.842, Training Time: 100 seconds)
Epoch:  97/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.524, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.012, Training Time: 142 seconds)
Epoch:  97/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  1.879, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.005, Training Time: 152 seconds)
Epoch:  97/100, Validation loss:  5.827, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.786 to  0.784
Training loss improved!
Epoch:  98/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.685, Training Time: 30 seconds), Stats for epoch: (Training Loss:  1.685, Training Time: 30 seconds)
Epoch:  98/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  1.765, Training Time: 33 seconds), Stats for epoch: (Training Loss:  1.725, Training Time: 64 seconds)
Epoch:  98/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  1.962, Training Time: 36 seconds), Stats for epoch: (Training Loss:  1.804, Training Time: 100 seconds)
Epoch:  98/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.485, Training Time: 41 seconds), Stats for epoch: (Training Loss:  1.974, Training Time: 142 seconds)
Epoch:  98/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  1.824, Training Time: 9 seconds), Stats for epoch: (Training Loss:  1.966, Training Time: 152 seconds)
Epoch:  98/100, Validation loss:  5.821, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.784 to  0.782
Training loss improved!
Backup to models/csv/20201107_070848_backup_1_966 complete!
Epoch:  99/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.646, Training Time: 30 seconds), Stats for epoch: (Training Loss:  1.646, Training Time: 30 seconds)
Epoch:  99/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  1.723, Training Time: 33 seconds), Stats for epoch: (Training Loss:  1.684, Training Time: 63 seconds)
Epoch:  99/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  1.917, Training Time: 36 seconds), Stats for epoch: (Training Loss:  1.762, Training Time: 100 seconds)
Epoch:  99/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.447, Training Time: 42 seconds), Stats for epoch: (Training Loss:  1.933, Training Time: 142 seconds)
Epoch:  99/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  1.783, Training Time: 9 seconds), Stats for epoch: (Training Loss:  1.925, Training Time: 152 seconds)
Epoch:  99/100, Validation loss:  5.820, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.782 to  0.781
Training loss improved!
Epoch: 100/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.624, Training Time: 30 seconds), Stats for epoch: (Training Loss:  1.624, Training Time: 30 seconds)
Epoch: 100/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  1.679, Training Time: 33 seconds), Stats for epoch: (Training Loss:  1.651, Training Time: 64 seconds)
Epoch: 100/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  1.875, Training Time: 36 seconds), Stats for epoch: (Training Loss:  1.726, Training Time: 100 seconds)
Epoch: 100/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.394, Training Time: 41 seconds), Stats for epoch: (Training Loss:  1.893, Training Time: 142 seconds)
Epoch: 100/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  1.750, Training Time: 9 seconds), Stats for epoch: (Training Loss:  1.885, Training Time: 152 seconds)
Epoch: 100/100, Validation loss:  5.849, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.781 to  0.779
Training loss improved!
Training Complete!
