
Reading dataset 'csv'...

Final shared vocab size: 24919

Splitting 49871 samples into training & validation sets (20.0% used for validation)...
Training set: 39897 samples. Validation set: 9974 samples.
Sorting training & validation sets to increase training efficiency...
Initializing model...


Creating checkpoint batch files...
Initializing training...
Epochs: 100
Batch Size: 128
Optimizer: sgd
Epoch:   1/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  8.317, Training Time: 32 seconds), Stats for epoch: (Training Loss:  8.317, Training Time: 32 seconds)
Epoch:   1/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  7.018, Training Time: 32 seconds), Stats for epoch: (Training Loss:  7.668, Training Time: 65 seconds)
Epoch:   1/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.971, Training Time: 39 seconds), Stats for epoch: (Training Loss:  7.435, Training Time: 104 seconds)
Epoch:   1/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.971, Training Time: 5 seconds), Stats for epoch: (Training Loss:  7.417, Training Time: 110 seconds)
Epoch:   1/100, Validation loss:  6.835, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  1.000 to  0.998
Training loss improved!
Validation loss improved!
Epoch:   2/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.805, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.805, Training Time: 29 seconds)
Epoch:   2/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.710, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.758, Training Time: 62 seconds)
Epoch:   2/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.785, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.767, Training Time: 101 seconds)
Epoch:   2/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.808, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.768, Training Time: 107 seconds)
Epoch:   2/100, Validation loss:  6.714, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.998 to  0.995
Training loss improved!
Validation loss improved!
Epoch:   3/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.651, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.651, Training Time: 29 seconds)
Epoch:   3/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.611, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.631, Training Time: 62 seconds)
Epoch:   3/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.720, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.661, Training Time: 101 seconds)
Epoch:   3/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.741, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.664, Training Time: 107 seconds)
Epoch:   3/100, Validation loss:  6.721, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.995 to  0.993
Training loss improved!
Epoch:   4/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.575, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.575, Training Time: 29 seconds)
Epoch:   4/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.570, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.572, Training Time: 62 seconds)
Epoch:   4/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.681, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.609, Training Time: 101 seconds)
Epoch:   4/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.690, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.612, Training Time: 107 seconds)
Epoch:   4/100, Validation loss:  6.658, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.993 to  0.990
Training loss improved!
Validation loss improved!
Epoch:   5/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.518, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.518, Training Time: 29 seconds)
Epoch:   5/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.539, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.529, Training Time: 62 seconds)
Epoch:   5/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.652, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.570, Training Time: 101 seconds)
Epoch:   5/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.652, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.573, Training Time: 107 seconds)
Epoch:   5/100, Validation loss:  6.627, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.990 to  0.988
Training loss improved!
Validation loss improved!
Epoch:   6/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.488, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.488, Training Time: 29 seconds)
Epoch:   6/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.524, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.506, Training Time: 62 seconds)
Epoch:   6/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.628, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.547, Training Time: 101 seconds)
Epoch:   6/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.609, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.549, Training Time: 107 seconds)
Epoch:   6/100, Validation loss:  6.633, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.988 to  0.985
Training loss improved!
Epoch:   7/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.461, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.461, Training Time: 29 seconds)
Epoch:   7/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.503, Training Time: 32 seconds), Stats for epoch: (Training Loss:  6.482, Training Time: 62 seconds)
Epoch:   7/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.610, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.525, Training Time: 101 seconds)
Epoch:   7/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.588, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.527, Training Time: 107 seconds)
Epoch:   7/100, Validation loss:  6.603, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.985 to  0.983
Training loss improved!
Validation loss improved!
Epoch:   8/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.416, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.416, Training Time: 29 seconds)
Epoch:   8/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.489, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.453, Training Time: 62 seconds)
Epoch:   8/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.593, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.500, Training Time: 101 seconds)
Epoch:   8/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.560, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.502, Training Time: 107 seconds)
Epoch:   8/100, Validation loss:  6.610, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.983 to  0.980
Training loss improved!
Epoch:   9/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.429, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.429, Training Time: 29 seconds)
Epoch:   9/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.478, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.453, Training Time: 62 seconds)
Epoch:   9/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.577, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.495, Training Time: 101 seconds)
Epoch:   9/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.529, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.496, Training Time: 107 seconds)
Epoch:   9/100, Validation loss:  6.660, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.980 to  0.978
Training loss improved!
Epoch:  10/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.408, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.408, Training Time: 29 seconds)
Epoch:  10/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.464, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.436, Training Time: 62 seconds)
Epoch:  10/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.562, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.478, Training Time: 101 seconds)
Epoch:  10/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.502, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.479, Training Time: 107 seconds)
Epoch:  10/100, Validation loss:  6.593, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.978 to  0.975
Training loss improved!
Validation loss improved!
Epoch:  11/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.380, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.380, Training Time: 29 seconds)
Epoch:  11/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.445, Training Time: 32 seconds), Stats for epoch: (Training Loss:  6.413, Training Time: 62 seconds)
Epoch:  11/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.546, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.457, Training Time: 101 seconds)
Epoch:  11/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.479, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.458, Training Time: 107 seconds)
Epoch:  11/100, Validation loss:  6.644, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.975 to  0.973
Training loss improved!
Epoch:  12/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.365, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.365, Training Time: 29 seconds)
Epoch:  12/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.436, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.400, Training Time: 62 seconds)
Epoch:  12/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.536, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.445, Training Time: 102 seconds)
Epoch:  12/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.446, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.445, Training Time: 107 seconds)
Epoch:  12/100, Validation loss:  6.626, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.973 to  0.970
Training loss improved!
Epoch:  13/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.341, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.341, Training Time: 29 seconds)
Epoch:  13/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.426, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.383, Training Time: 62 seconds)
Epoch:  13/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.520, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.429, Training Time: 102 seconds)
Epoch:  13/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.428, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.429, Training Time: 107 seconds)
Epoch:  13/100, Validation loss:  6.629, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.970 to  0.968
Training loss improved!
Epoch:  14/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.332, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.332, Training Time: 29 seconds)
Epoch:  14/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.411, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.372, Training Time: 62 seconds)
Epoch:  14/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.500, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.414, Training Time: 102 seconds)
Epoch:  14/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.363, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.412, Training Time: 107 seconds)
Epoch:  14/100, Validation loss:  6.562, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.968 to  0.966
Training loss improved!
Validation loss improved!
Epoch:  15/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.310, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.310, Training Time: 29 seconds)
Epoch:  15/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.393, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.352, Training Time: 62 seconds)
Epoch:  15/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.474, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.393, Training Time: 102 seconds)
Epoch:  15/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.279, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.388, Training Time: 107 seconds)
Epoch:  15/100, Validation loss:  6.532, Batch Validation Time: 9 seconds
Learning rate decay: adjusting from  0.966 to  0.963
Training loss improved!
Validation loss improved!
Epoch:  16/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.272, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.272, Training Time: 29 seconds)
Epoch:  16/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.376, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.324, Training Time: 62 seconds)
Epoch:  16/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.425, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.358, Training Time: 102 seconds)
Epoch:  16/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.223, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.352, Training Time: 107 seconds)
Epoch:  16/100, Validation loss:  6.512, Batch Validation Time: 9 seconds
Learning rate decay: adjusting from  0.963 to  0.961
Training loss improved!
Validation loss improved!
Epoch:  17/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.257, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.257, Training Time: 29 seconds)
Epoch:  17/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.356, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.307, Training Time: 62 seconds)
Epoch:  17/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.379, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.331, Training Time: 102 seconds)
Epoch:  17/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.206, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.326, Training Time: 107 seconds)
Epoch:  17/100, Validation loss:  6.506, Batch Validation Time: 9 seconds
Learning rate decay: adjusting from  0.961 to  0.958
Training loss improved!
Validation loss improved!
Epoch:  18/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.238, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.238, Training Time: 29 seconds)
Epoch:  18/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.342, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.290, Training Time: 62 seconds)
Epoch:  18/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.369, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.316, Training Time: 101 seconds)
Epoch:  18/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.173, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.311, Training Time: 107 seconds)
Epoch:  18/100, Validation loss:  6.496, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.958 to  0.956
Training loss improved!
Validation loss improved!
Epoch:  19/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.200, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.200, Training Time: 29 seconds)
Epoch:  19/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.330, Training Time: 32 seconds), Stats for epoch: (Training Loss:  6.265, Training Time: 62 seconds)
Epoch:  19/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.328, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.286, Training Time: 101 seconds)
Epoch:  19/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.113, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.279, Training Time: 107 seconds)
Epoch:  19/100, Validation loss:  6.498, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.956 to  0.954
Training loss improved!
Epoch:  20/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.207, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.207, Training Time: 29 seconds)
Epoch:  20/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.318, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.262, Training Time: 62 seconds)
Epoch:  20/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.294, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.273, Training Time: 101 seconds)
Epoch:  20/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.131, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.267, Training Time: 107 seconds)
Epoch:  20/100, Validation loss:  6.480, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.954 to  0.951
Training loss improved!
Validation loss improved!
Epoch:  21/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.170, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.170, Training Time: 29 seconds)
Epoch:  21/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.308, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.239, Training Time: 62 seconds)
Epoch:  21/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.286, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.255, Training Time: 101 seconds)
Epoch:  21/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.108, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.249, Training Time: 107 seconds)
Epoch:  21/100, Validation loss:  6.553, Batch Validation Time: 9 seconds
Learning rate decay: adjusting from  0.951 to  0.949
Training loss improved!
Epoch:  22/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.157, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.157, Training Time: 29 seconds)
Epoch:  22/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.300, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.228, Training Time: 62 seconds)
Epoch:  22/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.275, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.244, Training Time: 102 seconds)
Epoch:  22/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.068, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.237, Training Time: 107 seconds)
Epoch:  22/100, Validation loss:  6.514, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.949 to  0.946
Training loss improved!
Epoch:  23/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.137, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.137, Training Time: 29 seconds)
Epoch:  23/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.288, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.213, Training Time: 62 seconds)
Epoch:  23/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.225, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.217, Training Time: 102 seconds)
Epoch:  23/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.030, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.209, Training Time: 107 seconds)
Epoch:  23/100, Validation loss:  6.472, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.946 to  0.944
Training loss improved!
Validation loss improved!
Epoch:  24/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.156, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.156, Training Time: 29 seconds)
Epoch:  24/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.277, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.217, Training Time: 62 seconds)
Epoch:  24/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.224, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.219, Training Time: 101 seconds)
Epoch:  24/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  6.010, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.211, Training Time: 107 seconds)
Epoch:  24/100, Validation loss:  6.463, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.944 to  0.942
Validation loss improved!
Epoch:  25/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.133, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.133, Training Time: 29 seconds)
Epoch:  25/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.273, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.203, Training Time: 62 seconds)
Epoch:  25/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.188, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.198, Training Time: 102 seconds)
Epoch:  25/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  5.963, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.189, Training Time: 107 seconds)
Epoch:  25/100, Validation loss:  6.422, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.942 to  0.939
Training loss improved!
Validation loss improved!
Epoch:  26/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.098, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.098, Training Time: 29 seconds)
Epoch:  26/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.257, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.178, Training Time: 62 seconds)
Epoch:  26/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.158, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.171, Training Time: 102 seconds)
Epoch:  26/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  5.878, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.160, Training Time: 107 seconds)
Epoch:  26/100, Validation loss:  6.411, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.939 to  0.937
Training loss improved!
Validation loss improved!
Epoch:  27/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.075, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.075, Training Time: 29 seconds)
Epoch:  27/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.243, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.159, Training Time: 62 seconds)
Epoch:  27/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.156, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.158, Training Time: 101 seconds)
Epoch:  27/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  5.868, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.147, Training Time: 107 seconds)
Epoch:  27/100, Validation loss:  6.409, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.937 to  0.935
Training loss improved!
Validation loss improved!
Epoch:  28/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.122, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.122, Training Time: 29 seconds)
Epoch:  28/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.253, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.187, Training Time: 62 seconds)
Epoch:  28/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.139, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.171, Training Time: 102 seconds)
Epoch:  28/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  5.851, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.159, Training Time: 107 seconds)
Epoch:  28/100, Validation loss:  6.392, Batch Validation Time: 9 seconds
Learning rate decay: adjusting from  0.935 to  0.932
Validation loss improved!
Epoch:  29/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.156, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.156, Training Time: 29 seconds)
Epoch:  29/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.242, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.199, Training Time: 62 seconds)
Epoch:  29/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.144, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.181, Training Time: 102 seconds)
Epoch:  29/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  5.886, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.169, Training Time: 107 seconds)
Epoch:  29/100, Validation loss:  6.435, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.932 to  0.930
Epoch:  30/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.059, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.059, Training Time: 29 seconds)
Epoch:  30/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.224, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.142, Training Time: 62 seconds)
Epoch:  30/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.083, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.122, Training Time: 102 seconds)
Epoch:  30/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  5.854, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.112, Training Time: 107 seconds)
Epoch:  30/100, Validation loss:  6.406, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.930 to  0.928
Training loss improved!
Epoch:  31/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.035, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.035, Training Time: 29 seconds)
Epoch:  31/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.207, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.121, Training Time: 62 seconds)
Epoch:  31/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.052, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.098, Training Time: 102 seconds)
Epoch:  31/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  5.713, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.083, Training Time: 107 seconds)
Epoch:  31/100, Validation loss:  6.373, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.928 to  0.925
Training loss improved!
Validation loss improved!
Epoch:  32/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  6.011, Training Time: 29 seconds), Stats for epoch: (Training Loss:  6.011, Training Time: 29 seconds)
Epoch:  32/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.196, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.104, Training Time: 62 seconds)
Epoch:  32/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.013, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.073, Training Time: 102 seconds)
Epoch:  32/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  5.687, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.059, Training Time: 107 seconds)
Epoch:  32/100, Validation loss:  6.321, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.925 to  0.923
Training loss improved!
Validation loss improved!
Epoch:  33/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  5.993, Training Time: 29 seconds), Stats for epoch: (Training Loss:  5.993, Training Time: 29 seconds)
Epoch:  33/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.174, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.083, Training Time: 62 seconds)
Epoch:  33/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  6.003, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.056, Training Time: 102 seconds)
Epoch:  33/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  5.616, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.040, Training Time: 107 seconds)
Epoch:  33/100, Validation loss:  6.369, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.923 to  0.921
Training loss improved!
Epoch:  34/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  5.956, Training Time: 29 seconds), Stats for epoch: (Training Loss:  5.956, Training Time: 29 seconds)
Epoch:  34/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.150, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.053, Training Time: 62 seconds)
Epoch:  34/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  5.956, Training Time: 39 seconds), Stats for epoch: (Training Loss:  6.021, Training Time: 102 seconds)
Epoch:  34/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  5.577, Training Time: 5 seconds), Stats for epoch: (Training Loss:  6.004, Training Time: 107 seconds)
Epoch:  34/100, Validation loss:  6.272, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.921 to  0.918
Training loss improved!
Validation loss improved!
Epoch:  35/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  5.926, Training Time: 29 seconds), Stats for epoch: (Training Loss:  5.926, Training Time: 29 seconds)
Epoch:  35/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.131, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.029, Training Time: 62 seconds)
Epoch:  35/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  5.931, Training Time: 39 seconds), Stats for epoch: (Training Loss:  5.996, Training Time: 102 seconds)
Epoch:  35/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  5.534, Training Time: 5 seconds), Stats for epoch: (Training Loss:  5.978, Training Time: 107 seconds)
Epoch:  35/100, Validation loss:  6.261, Batch Validation Time: 9 seconds
Learning rate decay: adjusting from  0.918 to  0.916
Training loss improved!
Validation loss improved!
Epoch:  36/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  5.902, Training Time: 29 seconds), Stats for epoch: (Training Loss:  5.902, Training Time: 29 seconds)
Epoch:  36/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.105, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.004, Training Time: 62 seconds)
Epoch:  36/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  5.898, Training Time: 39 seconds), Stats for epoch: (Training Loss:  5.969, Training Time: 102 seconds)
Epoch:  36/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  5.490, Training Time: 5 seconds), Stats for epoch: (Training Loss:  5.950, Training Time: 107 seconds)
Epoch:  36/100, Validation loss:  6.237, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.916 to  0.914
Training loss improved!
Validation loss improved!
Epoch:  37/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  5.863, Training Time: 29 seconds), Stats for epoch: (Training Loss:  5.863, Training Time: 29 seconds)
Epoch:  37/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  6.058, Training Time: 32 seconds), Stats for epoch: (Training Loss:  5.960, Training Time: 62 seconds)
Epoch:  37/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  5.822, Training Time: 39 seconds), Stats for epoch: (Training Loss:  5.914, Training Time: 101 seconds)
Epoch:  37/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  5.466, Training Time: 5 seconds), Stats for epoch: (Training Loss:  5.897, Training Time: 107 seconds)
Epoch:  37/100, Validation loss:  6.159, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.914 to  0.912
Training loss improved!
Validation loss improved!
Epoch:  38/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  5.777, Training Time: 29 seconds), Stats for epoch: (Training Loss:  5.777, Training Time: 29 seconds)
Epoch:  38/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  5.979, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.878, Training Time: 62 seconds)
Epoch:  38/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  5.767, Training Time: 39 seconds), Stats for epoch: (Training Loss:  5.841, Training Time: 102 seconds)
Epoch:  38/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  5.304, Training Time: 5 seconds), Stats for epoch: (Training Loss:  5.820, Training Time: 107 seconds)
Epoch:  38/100, Validation loss:  6.095, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.912 to  0.909
Training loss improved!
Validation loss improved!
Epoch:  39/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  5.748, Training Time: 29 seconds), Stats for epoch: (Training Loss:  5.748, Training Time: 29 seconds)
Epoch:  39/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  5.915, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.831, Training Time: 62 seconds)
Epoch:  39/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  5.677, Training Time: 39 seconds), Stats for epoch: (Training Loss:  5.780, Training Time: 102 seconds)
Epoch:  39/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  5.276, Training Time: 5 seconds), Stats for epoch: (Training Loss:  5.760, Training Time: 107 seconds)
Epoch:  39/100, Validation loss:  6.016, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.909 to  0.907
Training loss improved!
Validation loss improved!
Epoch:  40/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  5.657, Training Time: 29 seconds), Stats for epoch: (Training Loss:  5.657, Training Time: 29 seconds)
Epoch:  40/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  5.836, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.746, Training Time: 62 seconds)
Epoch:  40/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  5.598, Training Time: 39 seconds), Stats for epoch: (Training Loss:  5.697, Training Time: 101 seconds)
Epoch:  40/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  5.150, Training Time: 5 seconds), Stats for epoch: (Training Loss:  5.676, Training Time: 107 seconds)
Epoch:  40/100, Validation loss:  5.965, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.907 to  0.905
Training loss improved!
Validation loss improved!
Epoch:  41/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  5.545, Training Time: 29 seconds), Stats for epoch: (Training Loss:  5.545, Training Time: 29 seconds)
Epoch:  41/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  5.772, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.659, Training Time: 62 seconds)
Epoch:  41/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  5.524, Training Time: 39 seconds), Stats for epoch: (Training Loss:  5.614, Training Time: 102 seconds)
Epoch:  41/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  5.064, Training Time: 5 seconds), Stats for epoch: (Training Loss:  5.592, Training Time: 107 seconds)
Epoch:  41/100, Validation loss:  5.905, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.905 to  0.902
Training loss improved!
Validation loss improved!
Epoch:  42/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  5.548, Training Time: 29 seconds), Stats for epoch: (Training Loss:  5.548, Training Time: 29 seconds)
Epoch:  42/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  5.725, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.636, Training Time: 62 seconds)
Epoch:  42/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  5.454, Training Time: 39 seconds), Stats for epoch: (Training Loss:  5.576, Training Time: 102 seconds)
Epoch:  42/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  4.914, Training Time: 5 seconds), Stats for epoch: (Training Loss:  5.550, Training Time: 107 seconds)
Epoch:  42/100, Validation loss:  5.954, Batch Validation Time: 9 seconds
Learning rate decay: adjusting from  0.902 to  0.900
Training loss improved!
Epoch:  43/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  5.446, Training Time: 29 seconds), Stats for epoch: (Training Loss:  5.446, Training Time: 29 seconds)
Epoch:  43/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  5.666, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.556, Training Time: 62 seconds)
Epoch:  43/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  5.377, Training Time: 39 seconds), Stats for epoch: (Training Loss:  5.496, Training Time: 102 seconds)
Epoch:  43/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  4.823, Training Time: 5 seconds), Stats for epoch: (Training Loss:  5.470, Training Time: 107 seconds)
Epoch:  43/100, Validation loss:  5.805, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.900 to  0.898
Training loss improved!
Validation loss improved!
Epoch:  44/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  5.378, Training Time: 29 seconds), Stats for epoch: (Training Loss:  5.378, Training Time: 29 seconds)
Epoch:  44/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  5.615, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.497, Training Time: 62 seconds)
Epoch:  44/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  5.303, Training Time: 39 seconds), Stats for epoch: (Training Loss:  5.432, Training Time: 102 seconds)
Epoch:  44/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  4.734, Training Time: 5 seconds), Stats for epoch: (Training Loss:  5.405, Training Time: 107 seconds)
Epoch:  44/100, Validation loss:  5.748, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.898 to  0.896
Training loss improved!
Validation loss improved!
Epoch:  45/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  5.288, Training Time: 29 seconds), Stats for epoch: (Training Loss:  5.288, Training Time: 29 seconds)
Epoch:  45/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  5.558, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.423, Training Time: 62 seconds)
Epoch:  45/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  5.233, Training Time: 39 seconds), Stats for epoch: (Training Loss:  5.360, Training Time: 102 seconds)
Epoch:  45/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  4.631, Training Time: 5 seconds), Stats for epoch: (Training Loss:  5.332, Training Time: 107 seconds)
Epoch:  45/100, Validation loss:  5.730, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.896 to  0.893
Training loss improved!
Validation loss improved!
Epoch:  46/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  5.228, Training Time: 29 seconds), Stats for epoch: (Training Loss:  5.228, Training Time: 29 seconds)
Epoch:  46/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  5.505, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.367, Training Time: 62 seconds)
Epoch:  46/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  5.157, Training Time: 39 seconds), Stats for epoch: (Training Loss:  5.297, Training Time: 102 seconds)
Epoch:  46/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  4.669, Training Time: 5 seconds), Stats for epoch: (Training Loss:  5.273, Training Time: 107 seconds)
Epoch:  46/100, Validation loss:  5.699, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.893 to  0.891
Training loss improved!
Validation loss improved!
Epoch:  47/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  5.167, Training Time: 29 seconds), Stats for epoch: (Training Loss:  5.167, Training Time: 29 seconds)
Epoch:  47/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  5.451, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.309, Training Time: 62 seconds)
Epoch:  47/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  5.101, Training Time: 39 seconds), Stats for epoch: (Training Loss:  5.239, Training Time: 102 seconds)
Epoch:  47/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  4.377, Training Time: 5 seconds), Stats for epoch: (Training Loss:  5.206, Training Time: 107 seconds)
Epoch:  47/100, Validation loss:  5.596, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.891 to  0.889
Training loss improved!
Validation loss improved!
Epoch:  48/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  5.102, Training Time: 29 seconds), Stats for epoch: (Training Loss:  5.102, Training Time: 29 seconds)
Epoch:  48/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  5.387, Training Time: 32 seconds), Stats for epoch: (Training Loss:  5.245, Training Time: 62 seconds)
Epoch:  48/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  5.006, Training Time: 39 seconds), Stats for epoch: (Training Loss:  5.165, Training Time: 102 seconds)
Epoch:  48/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  4.465, Training Time: 5 seconds), Stats for epoch: (Training Loss:  5.138, Training Time: 107 seconds)
Epoch:  48/100, Validation loss:  5.551, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.889 to  0.887
Training loss improved!
Validation loss improved!
Epoch:  49/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  5.036, Training Time: 29 seconds), Stats for epoch: (Training Loss:  5.036, Training Time: 29 seconds)
Epoch:  49/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  5.325, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.181, Training Time: 62 seconds)
Epoch:  49/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.956, Training Time: 39 seconds), Stats for epoch: (Training Loss:  5.106, Training Time: 102 seconds)
Epoch:  49/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  4.297, Training Time: 5 seconds), Stats for epoch: (Training Loss:  5.075, Training Time: 107 seconds)
Epoch:  49/100, Validation loss:  5.517, Batch Validation Time: 9 seconds
Learning rate decay: adjusting from  0.887 to  0.885
Training loss improved!
Validation loss improved!
Epoch:  50/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  4.956, Training Time: 29 seconds), Stats for epoch: (Training Loss:  4.956, Training Time: 29 seconds)
Epoch:  50/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  5.264, Training Time: 32 seconds), Stats for epoch: (Training Loss:  5.110, Training Time: 62 seconds)
Epoch:  50/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.897, Training Time: 39 seconds), Stats for epoch: (Training Loss:  5.039, Training Time: 102 seconds)
Epoch:  50/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  4.282, Training Time: 5 seconds), Stats for epoch: (Training Loss:  5.010, Training Time: 107 seconds)
Epoch:  50/100, Validation loss:  5.449, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.885 to  0.882
Training loss improved!
Validation loss improved!
Epoch:  51/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  4.880, Training Time: 29 seconds), Stats for epoch: (Training Loss:  4.880, Training Time: 29 seconds)
Epoch:  51/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  5.205, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.042, Training Time: 62 seconds)
Epoch:  51/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.817, Training Time: 39 seconds), Stats for epoch: (Training Loss:  4.967, Training Time: 102 seconds)
Epoch:  51/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  4.037, Training Time: 5 seconds), Stats for epoch: (Training Loss:  4.931, Training Time: 107 seconds)
Epoch:  51/100, Validation loss:  5.384, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.882 to  0.880
Training loss improved!
Validation loss improved!
Epoch:  52/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  4.814, Training Time: 29 seconds), Stats for epoch: (Training Loss:  4.814, Training Time: 29 seconds)
Epoch:  52/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  5.149, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.982, Training Time: 62 seconds)
Epoch:  52/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.775, Training Time: 39 seconds), Stats for epoch: (Training Loss:  4.913, Training Time: 102 seconds)
Epoch:  52/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  4.084, Training Time: 5 seconds), Stats for epoch: (Training Loss:  4.881, Training Time: 107 seconds)
Epoch:  52/100, Validation loss:  5.345, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.880 to  0.878
Training loss improved!
Validation loss improved!
Epoch:  53/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  4.750, Training Time: 29 seconds), Stats for epoch: (Training Loss:  4.750, Training Time: 29 seconds)
Epoch:  53/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  5.090, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.920, Training Time: 62 seconds)
Epoch:  53/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.704, Training Time: 39 seconds), Stats for epoch: (Training Loss:  4.848, Training Time: 102 seconds)
Epoch:  53/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.923, Training Time: 5 seconds), Stats for epoch: (Training Loss:  4.812, Training Time: 107 seconds)
Epoch:  53/100, Validation loss:  5.329, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.878 to  0.876
Training loss improved!
Validation loss improved!
Epoch:  54/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  4.682, Training Time: 29 seconds), Stats for epoch: (Training Loss:  4.682, Training Time: 29 seconds)
Epoch:  54/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  5.030, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.856, Training Time: 62 seconds)
Epoch:  54/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.679, Training Time: 39 seconds), Stats for epoch: (Training Loss:  4.797, Training Time: 102 seconds)
Epoch:  54/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.928, Training Time: 5 seconds), Stats for epoch: (Training Loss:  4.764, Training Time: 107 seconds)
Epoch:  54/100, Validation loss:  5.480, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.876 to  0.874
Training loss improved!
Epoch:  55/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  4.657, Training Time: 29 seconds), Stats for epoch: (Training Loss:  4.657, Training Time: 29 seconds)
Epoch:  55/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  4.985, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.821, Training Time: 62 seconds)
Epoch:  55/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.608, Training Time: 39 seconds), Stats for epoch: (Training Loss:  4.750, Training Time: 102 seconds)
Epoch:  55/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.804, Training Time: 5 seconds), Stats for epoch: (Training Loss:  4.714, Training Time: 107 seconds)
Epoch:  55/100, Validation loss:  5.264, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.874 to  0.871
Training loss improved!
Validation loss improved!
Epoch:  56/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  4.563, Training Time: 29 seconds), Stats for epoch: (Training Loss:  4.563, Training Time: 29 seconds)
Epoch:  56/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  4.916, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.740, Training Time: 62 seconds)
Epoch:  56/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.548, Training Time: 39 seconds), Stats for epoch: (Training Loss:  4.676, Training Time: 102 seconds)
Epoch:  56/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.765, Training Time: 5 seconds), Stats for epoch: (Training Loss:  4.641, Training Time: 107 seconds)
Epoch:  56/100, Validation loss:  5.223, Batch Validation Time: 9 seconds
Learning rate decay: adjusting from  0.871 to  0.869
Training loss improved!
Validation loss improved!
Epoch:  57/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  4.493, Training Time: 29 seconds), Stats for epoch: (Training Loss:  4.493, Training Time: 29 seconds)
Epoch:  57/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  4.856, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.674, Training Time: 62 seconds)
Epoch:  57/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.494, Training Time: 39 seconds), Stats for epoch: (Training Loss:  4.614, Training Time: 102 seconds)
Epoch:  57/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.697, Training Time: 5 seconds), Stats for epoch: (Training Loss:  4.579, Training Time: 107 seconds)
Epoch:  57/100, Validation loss:  5.198, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.869 to  0.867
Training loss improved!
Validation loss improved!
Epoch:  58/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  4.422, Training Time: 29 seconds), Stats for epoch: (Training Loss:  4.422, Training Time: 29 seconds)
Epoch:  58/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  4.798, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.610, Training Time: 62 seconds)
Epoch:  58/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.443, Training Time: 39 seconds), Stats for epoch: (Training Loss:  4.554, Training Time: 102 seconds)
Epoch:  58/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.627, Training Time: 5 seconds), Stats for epoch: (Training Loss:  4.519, Training Time: 107 seconds)
Epoch:  58/100, Validation loss:  5.149, Batch Validation Time: 9 seconds
Learning rate decay: adjusting from  0.867 to  0.865
Training loss improved!
Validation loss improved!
Epoch:  59/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  4.361, Training Time: 29 seconds), Stats for epoch: (Training Loss:  4.361, Training Time: 29 seconds)
Epoch:  59/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  4.734, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.547, Training Time: 62 seconds)
Epoch:  59/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.392, Training Time: 39 seconds), Stats for epoch: (Training Loss:  4.496, Training Time: 102 seconds)
Epoch:  59/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.561, Training Time: 5 seconds), Stats for epoch: (Training Loss:  4.460, Training Time: 107 seconds)
Epoch:  59/100, Validation loss:  5.126, Batch Validation Time: 9 seconds
Learning rate decay: adjusting from  0.865 to  0.863
Training loss improved!
Validation loss improved!
Epoch:  60/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  4.295, Training Time: 29 seconds), Stats for epoch: (Training Loss:  4.295, Training Time: 29 seconds)
Epoch:  60/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  4.672, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.484, Training Time: 62 seconds)
Epoch:  60/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.333, Training Time: 39 seconds), Stats for epoch: (Training Loss:  4.434, Training Time: 102 seconds)
Epoch:  60/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.507, Training Time: 5 seconds), Stats for epoch: (Training Loss:  4.398, Training Time: 107 seconds)
Epoch:  60/100, Validation loss:  5.112, Batch Validation Time: 9 seconds
Learning rate decay: adjusting from  0.863 to  0.861
Training loss improved!
Validation loss improved!
Epoch:  61/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  4.233, Training Time: 29 seconds), Stats for epoch: (Training Loss:  4.233, Training Time: 29 seconds)
Epoch:  61/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  4.616, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.424, Training Time: 62 seconds)
Epoch:  61/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.287, Training Time: 39 seconds), Stats for epoch: (Training Loss:  4.379, Training Time: 102 seconds)
Epoch:  61/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.463, Training Time: 5 seconds), Stats for epoch: (Training Loss:  4.343, Training Time: 107 seconds)
Epoch:  61/100, Validation loss:  5.101, Batch Validation Time: 9 seconds
Learning rate decay: adjusting from  0.861 to  0.858
Training loss improved!
Validation loss improved!
Epoch:  62/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  4.171, Training Time: 29 seconds), Stats for epoch: (Training Loss:  4.171, Training Time: 29 seconds)
Epoch:  62/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  4.551, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.361, Training Time: 62 seconds)
Epoch:  62/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.253, Training Time: 39 seconds), Stats for epoch: (Training Loss:  4.325, Training Time: 102 seconds)
Epoch:  62/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.438, Training Time: 5 seconds), Stats for epoch: (Training Loss:  4.291, Training Time: 107 seconds)
Epoch:  62/100, Validation loss:  5.151, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.858 to  0.856
Training loss improved!
Epoch:  63/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  4.101, Training Time: 29 seconds), Stats for epoch: (Training Loss:  4.101, Training Time: 29 seconds)
Epoch:  63/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  4.489, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.295, Training Time: 62 seconds)
Epoch:  63/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.195, Training Time: 39 seconds), Stats for epoch: (Training Loss:  4.262, Training Time: 102 seconds)
Epoch:  63/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.373, Training Time: 5 seconds), Stats for epoch: (Training Loss:  4.228, Training Time: 107 seconds)
Epoch:  63/100, Validation loss:  5.054, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.856 to  0.854
Training loss improved!
Validation loss improved!
Epoch:  64/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  4.039, Training Time: 29 seconds), Stats for epoch: (Training Loss:  4.039, Training Time: 29 seconds)
Epoch:  64/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  4.426, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.233, Training Time: 62 seconds)
Epoch:  64/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.140, Training Time: 39 seconds), Stats for epoch: (Training Loss:  4.202, Training Time: 102 seconds)
Epoch:  64/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.335, Training Time: 5 seconds), Stats for epoch: (Training Loss:  4.169, Training Time: 107 seconds)
Epoch:  64/100, Validation loss:  5.043, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.854 to  0.852
Training loss improved!
Validation loss improved!
Epoch:  65/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  3.964, Training Time: 29 seconds), Stats for epoch: (Training Loss:  3.964, Training Time: 29 seconds)
Epoch:  65/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  4.357, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.160, Training Time: 62 seconds)
Epoch:  65/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.084, Training Time: 39 seconds), Stats for epoch: (Training Loss:  4.135, Training Time: 102 seconds)
Epoch:  65/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.286, Training Time: 5 seconds), Stats for epoch: (Training Loss:  4.102, Training Time: 107 seconds)
Epoch:  65/100, Validation loss:  5.051, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.852 to  0.850
Training loss improved!
Epoch:  66/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  3.914, Training Time: 29 seconds), Stats for epoch: (Training Loss:  3.914, Training Time: 29 seconds)
Epoch:  66/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  4.293, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.104, Training Time: 62 seconds)
Epoch:  66/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  4.040, Training Time: 39 seconds), Stats for epoch: (Training Loss:  4.083, Training Time: 102 seconds)
Epoch:  66/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.230, Training Time: 5 seconds), Stats for epoch: (Training Loss:  4.050, Training Time: 107 seconds)
Epoch:  66/100, Validation loss:  5.059, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.850 to  0.848
Training loss improved!
Epoch:  67/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  3.838, Training Time: 29 seconds), Stats for epoch: (Training Loss:  3.838, Training Time: 29 seconds)
Epoch:  67/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  4.233, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.036, Training Time: 62 seconds)
Epoch:  67/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  3.981, Training Time: 39 seconds), Stats for epoch: (Training Loss:  4.018, Training Time: 102 seconds)
Epoch:  67/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.209, Training Time: 5 seconds), Stats for epoch: (Training Loss:  3.987, Training Time: 107 seconds)
Epoch:  67/100, Validation loss:  5.034, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.848 to  0.846
Training loss improved!
Validation loss improved!
Epoch:  68/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  3.766, Training Time: 29 seconds), Stats for epoch: (Training Loss:  3.766, Training Time: 29 seconds)
Epoch:  68/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  4.157, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.962, Training Time: 62 seconds)
Epoch:  68/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  3.929, Training Time: 39 seconds), Stats for epoch: (Training Loss:  3.951, Training Time: 102 seconds)
Epoch:  68/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.152, Training Time: 5 seconds), Stats for epoch: (Training Loss:  3.920, Training Time: 107 seconds)
Epoch:  68/100, Validation loss:  5.052, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.846 to  0.843
Training loss improved!
Epoch:  69/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  3.693, Training Time: 29 seconds), Stats for epoch: (Training Loss:  3.693, Training Time: 29 seconds)
Epoch:  69/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  4.092, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.892, Training Time: 62 seconds)
Epoch:  69/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  3.873, Training Time: 39 seconds), Stats for epoch: (Training Loss:  3.886, Training Time: 102 seconds)
Epoch:  69/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.100, Training Time: 5 seconds), Stats for epoch: (Training Loss:  3.856, Training Time: 107 seconds)
Epoch:  69/100, Validation loss:  5.053, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.843 to  0.841
Training loss improved!
Epoch:  70/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  3.616, Training Time: 29 seconds), Stats for epoch: (Training Loss:  3.616, Training Time: 29 seconds)
Epoch:  70/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  4.022, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.819, Training Time: 62 seconds)
Epoch:  70/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  3.837, Training Time: 39 seconds), Stats for epoch: (Training Loss:  3.825, Training Time: 102 seconds)
Epoch:  70/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.069, Training Time: 5 seconds), Stats for epoch: (Training Loss:  3.796, Training Time: 107 seconds)
Epoch:  70/100, Validation loss:  5.059, Batch Validation Time: 9 seconds
Learning rate decay: adjusting from  0.841 to  0.839
Training loss improved!
Epoch:  71/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  3.553, Training Time: 29 seconds), Stats for epoch: (Training Loss:  3.553, Training Time: 29 seconds)
Epoch:  71/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  3.956, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.755, Training Time: 62 seconds)
Epoch:  71/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  3.764, Training Time: 39 seconds), Stats for epoch: (Training Loss:  3.758, Training Time: 102 seconds)
Epoch:  71/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  3.021, Training Time: 5 seconds), Stats for epoch: (Training Loss:  3.730, Training Time: 107 seconds)
Epoch:  71/100, Validation loss:  5.001, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.839 to  0.837
Training loss improved!
Validation loss improved!
Epoch:  72/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  3.492, Training Time: 29 seconds), Stats for epoch: (Training Loss:  3.492, Training Time: 29 seconds)
Epoch:  72/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  3.873, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.683, Training Time: 62 seconds)
Epoch:  72/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  3.696, Training Time: 39 seconds), Stats for epoch: (Training Loss:  3.687, Training Time: 102 seconds)
Epoch:  72/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.989, Training Time: 5 seconds), Stats for epoch: (Training Loss:  3.660, Training Time: 107 seconds)
Epoch:  72/100, Validation loss:  5.089, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.837 to  0.835
Training loss improved!
Epoch:  73/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  3.421, Training Time: 29 seconds), Stats for epoch: (Training Loss:  3.421, Training Time: 29 seconds)
Epoch:  73/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  3.808, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.615, Training Time: 62 seconds)
Epoch:  73/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  3.650, Training Time: 39 seconds), Stats for epoch: (Training Loss:  3.626, Training Time: 102 seconds)
Epoch:  73/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.957, Training Time: 5 seconds), Stats for epoch: (Training Loss:  3.601, Training Time: 107 seconds)
Epoch:  73/100, Validation loss:  5.008, Batch Validation Time: 9 seconds
Learning rate decay: adjusting from  0.835 to  0.833
Training loss improved!
Epoch:  74/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  3.346, Training Time: 29 seconds), Stats for epoch: (Training Loss:  3.346, Training Time: 29 seconds)
Epoch:  74/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  3.725, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.535, Training Time: 62 seconds)
Epoch:  74/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  3.590, Training Time: 39 seconds), Stats for epoch: (Training Loss:  3.554, Training Time: 102 seconds)
Epoch:  74/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.928, Training Time: 5 seconds), Stats for epoch: (Training Loss:  3.530, Training Time: 107 seconds)
Epoch:  74/100, Validation loss:  5.059, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.833 to  0.831
Training loss improved!
Epoch:  75/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  3.279, Training Time: 29 seconds), Stats for epoch: (Training Loss:  3.279, Training Time: 29 seconds)
Epoch:  75/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  3.652, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.465, Training Time: 62 seconds)
Epoch:  75/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  3.527, Training Time: 39 seconds), Stats for epoch: (Training Loss:  3.486, Training Time: 102 seconds)
Epoch:  75/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.837, Training Time: 5 seconds), Stats for epoch: (Training Loss:  3.461, Training Time: 107 seconds)
Epoch:  75/100, Validation loss:  5.084, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.831 to  0.829
Training loss improved!
Epoch:  76/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  3.208, Training Time: 29 seconds), Stats for epoch: (Training Loss:  3.208, Training Time: 29 seconds)
Epoch:  76/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  3.578, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.393, Training Time: 62 seconds)
Epoch:  76/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  3.474, Training Time: 39 seconds), Stats for epoch: (Training Loss:  3.420, Training Time: 102 seconds)
Epoch:  76/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.808, Training Time: 5 seconds), Stats for epoch: (Training Loss:  3.396, Training Time: 107 seconds)
Epoch:  76/100, Validation loss:  5.113, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.829 to  0.827
Training loss improved!
Epoch:  77/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  3.138, Training Time: 29 seconds), Stats for epoch: (Training Loss:  3.138, Training Time: 29 seconds)
Epoch:  77/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  3.501, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.319, Training Time: 62 seconds)
Epoch:  77/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  3.418, Training Time: 39 seconds), Stats for epoch: (Training Loss:  3.352, Training Time: 102 seconds)
Epoch:  77/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.744, Training Time: 5 seconds), Stats for epoch: (Training Loss:  3.329, Training Time: 107 seconds)
Epoch:  77/100, Validation loss:  5.072, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.827 to  0.825
Training loss improved!
Epoch:  78/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  3.071, Training Time: 29 seconds), Stats for epoch: (Training Loss:  3.071, Training Time: 29 seconds)
Epoch:  78/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  3.421, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.246, Training Time: 62 seconds)
Epoch:  78/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  3.345, Training Time: 39 seconds), Stats for epoch: (Training Loss:  3.279, Training Time: 102 seconds)
Epoch:  78/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.725, Training Time: 5 seconds), Stats for epoch: (Training Loss:  3.258, Training Time: 107 seconds)
Epoch:  78/100, Validation loss:  5.079, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.825 to  0.823
Training loss improved!
Epoch:  79/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  2.989, Training Time: 29 seconds), Stats for epoch: (Training Loss:  2.989, Training Time: 29 seconds)
Epoch:  79/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  3.350, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.170, Training Time: 62 seconds)
Epoch:  79/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  3.283, Training Time: 39 seconds), Stats for epoch: (Training Loss:  3.207, Training Time: 102 seconds)
Epoch:  79/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.660, Training Time: 5 seconds), Stats for epoch: (Training Loss:  3.186, Training Time: 107 seconds)
Epoch:  79/100, Validation loss:  5.110, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.823 to  0.821
Training loss improved!
Epoch:  80/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  2.928, Training Time: 29 seconds), Stats for epoch: (Training Loss:  2.928, Training Time: 29 seconds)
Epoch:  80/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  3.280, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.104, Training Time: 62 seconds)
Epoch:  80/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  3.228, Training Time: 39 seconds), Stats for epoch: (Training Loss:  3.146, Training Time: 102 seconds)
Epoch:  80/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.656, Training Time: 5 seconds), Stats for epoch: (Training Loss:  3.127, Training Time: 107 seconds)
Epoch:  80/100, Validation loss:  5.126, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.821 to  0.819
Training loss improved!
Epoch:  81/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  2.855, Training Time: 29 seconds), Stats for epoch: (Training Loss:  2.855, Training Time: 29 seconds)
Epoch:  81/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  3.194, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.025, Training Time: 62 seconds)
Epoch:  81/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  3.160, Training Time: 39 seconds), Stats for epoch: (Training Loss:  3.070, Training Time: 102 seconds)
Epoch:  81/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.583, Training Time: 5 seconds), Stats for epoch: (Training Loss:  3.051, Training Time: 107 seconds)
Epoch:  81/100, Validation loss:  5.147, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.819 to  0.816
Training loss improved!
Epoch:  82/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  2.774, Training Time: 29 seconds), Stats for epoch: (Training Loss:  2.774, Training Time: 29 seconds)
Epoch:  82/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  3.130, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.952, Training Time: 63 seconds)
Epoch:  82/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  3.089, Training Time: 39 seconds), Stats for epoch: (Training Loss:  2.998, Training Time: 102 seconds)
Epoch:  82/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.562, Training Time: 5 seconds), Stats for epoch: (Training Loss:  2.981, Training Time: 107 seconds)
Epoch:  82/100, Validation loss:  5.157, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.816 to  0.814
Training loss improved!
Backup to models/csv/20201025_212051_backup_2_981 complete!
Epoch:  83/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  2.716, Training Time: 29 seconds), Stats for epoch: (Training Loss:  2.716, Training Time: 29 seconds)
Epoch:  83/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  3.047, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.881, Training Time: 62 seconds)
Epoch:  83/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  3.031, Training Time: 39 seconds), Stats for epoch: (Training Loss:  2.931, Training Time: 102 seconds)
Epoch:  83/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.482, Training Time: 5 seconds), Stats for epoch: (Training Loss:  2.914, Training Time: 107 seconds)
Epoch:  83/100, Validation loss:  5.193, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.814 to  0.812
Training loss improved!
Epoch:  84/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  2.659, Training Time: 29 seconds), Stats for epoch: (Training Loss:  2.659, Training Time: 29 seconds)
Epoch:  84/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  2.957, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.808, Training Time: 62 seconds)
Epoch:  84/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  2.968, Training Time: 39 seconds), Stats for epoch: (Training Loss:  2.861, Training Time: 102 seconds)
Epoch:  84/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.418, Training Time: 5 seconds), Stats for epoch: (Training Loss:  2.844, Training Time: 107 seconds)
Epoch:  84/100, Validation loss:  5.184, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.812 to  0.810
Training loss improved!
Epoch:  85/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  2.583, Training Time: 29 seconds), Stats for epoch: (Training Loss:  2.583, Training Time: 29 seconds)
Epoch:  85/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  2.887, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.735, Training Time: 62 seconds)
Epoch:  85/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  2.903, Training Time: 39 seconds), Stats for epoch: (Training Loss:  2.791, Training Time: 102 seconds)
Epoch:  85/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.380, Training Time: 5 seconds), Stats for epoch: (Training Loss:  2.775, Training Time: 107 seconds)
Epoch:  85/100, Validation loss:  5.227, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.810 to  0.808
Training loss improved!
Epoch:  86/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  2.508, Training Time: 29 seconds), Stats for epoch: (Training Loss:  2.508, Training Time: 29 seconds)
Epoch:  86/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  2.806, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.657, Training Time: 62 seconds)
Epoch:  86/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  2.827, Training Time: 39 seconds), Stats for epoch: (Training Loss:  2.714, Training Time: 102 seconds)
Epoch:  86/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.328, Training Time: 5 seconds), Stats for epoch: (Training Loss:  2.699, Training Time: 107 seconds)
Epoch:  86/100, Validation loss:  5.225, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.808 to  0.806
Training loss improved!
Epoch:  87/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  2.456, Training Time: 29 seconds), Stats for epoch: (Training Loss:  2.456, Training Time: 29 seconds)
Epoch:  87/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  2.734, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.595, Training Time: 62 seconds)
Epoch:  87/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  2.755, Training Time: 39 seconds), Stats for epoch: (Training Loss:  2.648, Training Time: 102 seconds)
Epoch:  87/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.327, Training Time: 5 seconds), Stats for epoch: (Training Loss:  2.636, Training Time: 107 seconds)
Epoch:  87/100, Validation loss:  5.282, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.806 to  0.804
Training loss improved!
Epoch:  88/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  2.398, Training Time: 29 seconds), Stats for epoch: (Training Loss:  2.398, Training Time: 29 seconds)
Epoch:  88/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  2.654, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.526, Training Time: 62 seconds)
Epoch:  88/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  2.706, Training Time: 39 seconds), Stats for epoch: (Training Loss:  2.586, Training Time: 102 seconds)
Epoch:  88/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.220, Training Time: 5 seconds), Stats for epoch: (Training Loss:  2.572, Training Time: 107 seconds)
Epoch:  88/100, Validation loss:  5.294, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.804 to  0.802
Training loss improved!
Epoch:  89/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  2.323, Training Time: 29 seconds), Stats for epoch: (Training Loss:  2.323, Training Time: 29 seconds)
Epoch:  89/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  2.580, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.452, Training Time: 62 seconds)
Epoch:  89/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  2.645, Training Time: 39 seconds), Stats for epoch: (Training Loss:  2.516, Training Time: 102 seconds)
Epoch:  89/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.176, Training Time: 5 seconds), Stats for epoch: (Training Loss:  2.503, Training Time: 107 seconds)
Epoch:  89/100, Validation loss:  5.330, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.802 to  0.800
Training loss improved!
Epoch:  90/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  2.272, Training Time: 29 seconds), Stats for epoch: (Training Loss:  2.272, Training Time: 29 seconds)
Epoch:  90/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  2.495, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.384, Training Time: 62 seconds)
Epoch:  90/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  2.567, Training Time: 39 seconds), Stats for epoch: (Training Loss:  2.445, Training Time: 102 seconds)
Epoch:  90/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.159, Training Time: 5 seconds), Stats for epoch: (Training Loss:  2.434, Training Time: 107 seconds)
Epoch:  90/100, Validation loss:  5.325, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.800 to  0.798
Training loss improved!
Backup to models/csv/20201025_212051_backup_2_434 complete!
Epoch:  91/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  2.211, Training Time: 29 seconds), Stats for epoch: (Training Loss:  2.211, Training Time: 29 seconds)
Epoch:  91/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  2.426, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.319, Training Time: 63 seconds)
Epoch:  91/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  2.496, Training Time: 39 seconds), Stats for epoch: (Training Loss:  2.378, Training Time: 102 seconds)
Epoch:  91/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.103, Training Time: 5 seconds), Stats for epoch: (Training Loss:  2.367, Training Time: 107 seconds)
Epoch:  91/100, Validation loss:  5.374, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.798 to  0.796
Training loss improved!
Epoch:  92/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  2.147, Training Time: 29 seconds), Stats for epoch: (Training Loss:  2.147, Training Time: 29 seconds)
Epoch:  92/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  2.355, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.251, Training Time: 62 seconds)
Epoch:  92/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  2.432, Training Time: 39 seconds), Stats for epoch: (Training Loss:  2.311, Training Time: 102 seconds)
Epoch:  92/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.068, Training Time: 5 seconds), Stats for epoch: (Training Loss:  2.302, Training Time: 107 seconds)
Epoch:  92/100, Validation loss:  5.436, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.796 to  0.794
Training loss improved!
Epoch:  93/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  2.095, Training Time: 29 seconds), Stats for epoch: (Training Loss:  2.095, Training Time: 29 seconds)
Epoch:  93/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  2.284, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.190, Training Time: 62 seconds)
Epoch:  93/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  2.373, Training Time: 39 seconds), Stats for epoch: (Training Loss:  2.251, Training Time: 102 seconds)
Epoch:  93/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  2.013, Training Time: 5 seconds), Stats for epoch: (Training Loss:  2.242, Training Time: 107 seconds)
Epoch:  93/100, Validation loss:  5.419, Batch Validation Time: 9 seconds
Learning rate decay: adjusting from  0.794 to  0.792
Training loss improved!
Epoch:  94/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  2.037, Training Time: 29 seconds), Stats for epoch: (Training Loss:  2.037, Training Time: 29 seconds)
Epoch:  94/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  2.226, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.131, Training Time: 62 seconds)
Epoch:  94/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  2.311, Training Time: 39 seconds), Stats for epoch: (Training Loss:  2.191, Training Time: 102 seconds)
Epoch:  94/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  1.954, Training Time: 5 seconds), Stats for epoch: (Training Loss:  2.182, Training Time: 107 seconds)
Epoch:  94/100, Validation loss:  5.460, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.792 to  0.790
Training loss improved!
Epoch:  95/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  1.994, Training Time: 29 seconds), Stats for epoch: (Training Loss:  1.994, Training Time: 29 seconds)
Epoch:  95/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  2.152, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.073, Training Time: 62 seconds)
Epoch:  95/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  2.239, Training Time: 39 seconds), Stats for epoch: (Training Loss:  2.128, Training Time: 102 seconds)
Epoch:  95/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  1.888, Training Time: 5 seconds), Stats for epoch: (Training Loss:  2.119, Training Time: 107 seconds)
Epoch:  95/100, Validation loss:  5.468, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.790 to  0.788
Training loss improved!
Epoch:  96/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  1.950, Training Time: 29 seconds), Stats for epoch: (Training Loss:  1.950, Training Time: 29 seconds)
Epoch:  96/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  2.208, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.079, Training Time: 62 seconds)
Epoch:  96/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  2.234, Training Time: 39 seconds), Stats for epoch: (Training Loss:  2.131, Training Time: 102 seconds)
Epoch:  96/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  1.926, Training Time: 5 seconds), Stats for epoch: (Training Loss:  2.123, Training Time: 107 seconds)
Epoch:  96/100, Validation loss:  5.542, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.788 to  0.786
Epoch:  97/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  1.981, Training Time: 29 seconds), Stats for epoch: (Training Loss:  1.981, Training Time: 29 seconds)
Epoch:  97/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  2.059, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.020, Training Time: 62 seconds)
Epoch:  97/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  2.117, Training Time: 39 seconds), Stats for epoch: (Training Loss:  2.052, Training Time: 102 seconds)
Epoch:  97/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  1.798, Training Time: 5 seconds), Stats for epoch: (Training Loss:  2.042, Training Time: 107 seconds)
Epoch:  97/100, Validation loss:  5.512, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.786 to  0.784
Training loss improved!
Epoch:  98/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  1.856, Training Time: 29 seconds), Stats for epoch: (Training Loss:  1.856, Training Time: 29 seconds)
Epoch:  98/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  1.964, Training Time: 33 seconds), Stats for epoch: (Training Loss:  1.910, Training Time: 62 seconds)
Epoch:  98/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  2.052, Training Time: 39 seconds), Stats for epoch: (Training Loss:  1.958, Training Time: 102 seconds)
Epoch:  98/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  1.756, Training Time: 5 seconds), Stats for epoch: (Training Loss:  1.950, Training Time: 107 seconds)
Epoch:  98/100, Validation loss:  5.580, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.784 to  0.782
Training loss improved!
Backup to models/csv/20201025_212051_backup_1_950 complete!
Epoch:  99/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  1.794, Training Time: 29 seconds), Stats for epoch: (Training Loss:  1.794, Training Time: 29 seconds)
Epoch:  99/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  1.892, Training Time: 33 seconds), Stats for epoch: (Training Loss:  1.843, Training Time: 62 seconds)
Epoch:  99/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  1.990, Training Time: 39 seconds), Stats for epoch: (Training Loss:  1.892, Training Time: 102 seconds)
Epoch:  99/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  1.689, Training Time: 5 seconds), Stats for epoch: (Training Loss:  1.884, Training Time: 107 seconds)
Epoch:  99/100, Validation loss:  5.598, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.782 to  0.781
Training loss improved!
Epoch: 100/100, Batch:  100/312, Stats for last 100 batches: (Training Loss:  1.736, Training Time: 29 seconds), Stats for epoch: (Training Loss:  1.736, Training Time: 29 seconds)
Epoch: 100/100, Batch:  200/312, Stats for last 100 batches: (Training Loss:  1.830, Training Time: 33 seconds), Stats for epoch: (Training Loss:  1.783, Training Time: 62 seconds)
Epoch: 100/100, Batch:  300/312, Stats for last 100 batches: (Training Loss:  1.936, Training Time: 39 seconds), Stats for epoch: (Training Loss:  1.834, Training Time: 102 seconds)
Epoch: 100/100, Batch:  312/312, Stats for last 12 batches: (Training Loss:  1.663, Training Time: 5 seconds), Stats for epoch: (Training Loss:  1.827, Training Time: 107 seconds)
Epoch: 100/100, Validation loss:  5.640, Batch Validation Time: 8 seconds
Learning rate decay: adjusting from  0.781 to  0.779
Training loss improved!
Training Complete!
