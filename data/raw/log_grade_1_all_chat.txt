
Reading dataset 'csv'...

Final shared vocab size: 33274

Splitting 67395 samples into training & validation sets (20.0% used for validation)...
Training set: 53916 samples. Validation set: 13479 samples.
Sorting training & validation sets to increase training efficiency...
Initializing model...


Creating checkpoint batch files...
Initializing training...
Epochs: 100
Batch Size: 128
Optimizer: sgd
Epoch:   1/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  8.237, Training Time: 33 seconds), Stats for epoch: (Training Loss:  8.237, Training Time: 33 seconds)
Epoch:   1/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  7.111, Training Time: 33 seconds), Stats for epoch: (Training Loss:  7.674, Training Time: 67 seconds)
Epoch:   1/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  7.095, Training Time: 36 seconds), Stats for epoch: (Training Loss:  7.481, Training Time: 104 seconds)
Epoch:   1/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  7.106, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.388, Training Time: 146 seconds)
Epoch:   1/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  7.041, Training Time: 10 seconds), Stats for epoch: (Training Loss:  7.369, Training Time: 156 seconds)
Epoch:   1/100, Validation loss:  7.293, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  1.000 to  0.998
Training loss improved!
Validation loss improved!
Epoch:   2/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.861, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.861, Training Time: 31 seconds)
Epoch:   2/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.759, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.810, Training Time: 64 seconds)
Epoch:   2/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.836, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.819, Training Time: 101 seconds)
Epoch:   2/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.946, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.850, Training Time: 143 seconds)
Epoch:   2/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.879, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.852, Training Time: 153 seconds)
Epoch:   2/100, Validation loss:  7.123, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.998 to  0.995
Training loss improved!
Validation loss improved!
Epoch:   3/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.680, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.680, Training Time: 31 seconds)
Epoch:   3/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.663, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.672, Training Time: 64 seconds)
Epoch:   3/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.761, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.702, Training Time: 101 seconds)
Epoch:   3/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.883, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.747, Training Time: 143 seconds)
Epoch:   3/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.792, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.749, Training Time: 153 seconds)
Epoch:   3/100, Validation loss:  7.055, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.995 to  0.993
Training loss improved!
Validation loss improved!
Epoch:   4/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.613, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.613, Training Time: 31 seconds)
Epoch:   4/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.617, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.615, Training Time: 65 seconds)
Epoch:   4/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.724, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.651, Training Time: 101 seconds)
Epoch:   4/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.844, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.700, Training Time: 144 seconds)
Epoch:   4/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.746, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.702, Training Time: 154 seconds)
Epoch:   4/100, Validation loss:  6.911, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.993 to  0.990
Training loss improved!
Validation loss improved!
Epoch:   5/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.610, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.610, Training Time: 31 seconds)
Epoch:   5/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.572, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.591, Training Time: 65 seconds)
Epoch:   5/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.695, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.626, Training Time: 102 seconds)
Epoch:   5/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.820, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.674, Training Time: 144 seconds)
Epoch:   5/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.717, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.676, Training Time: 154 seconds)
Epoch:   5/100, Validation loss:  6.913, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.990 to  0.988
Training loss improved!
Epoch:   6/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.530, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.530, Training Time: 31 seconds)
Epoch:   6/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.554, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.542, Training Time: 65 seconds)
Epoch:   6/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.672, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.585, Training Time: 101 seconds)
Epoch:   6/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.800, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.639, Training Time: 144 seconds)
Epoch:   6/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.681, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.641, Training Time: 154 seconds)
Epoch:   6/100, Validation loss:  6.943, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.988 to  0.985
Training loss improved!
Epoch:   7/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.515, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.515, Training Time: 31 seconds)
Epoch:   7/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.537, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.526, Training Time: 64 seconds)
Epoch:   7/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.655, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.569, Training Time: 101 seconds)
Epoch:   7/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.783, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.622, Training Time: 144 seconds)
Epoch:   7/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.653, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.624, Training Time: 154 seconds)
Epoch:   7/100, Validation loss:  6.940, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.985 to  0.983
Training loss improved!
Epoch:   8/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.481, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.481, Training Time: 31 seconds)
Epoch:   8/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.522, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.502, Training Time: 64 seconds)
Epoch:   8/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.640, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.548, Training Time: 101 seconds)
Epoch:   8/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.769, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.603, Training Time: 144 seconds)
Epoch:   8/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.635, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.605, Training Time: 154 seconds)
Epoch:   8/100, Validation loss:  6.896, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.983 to  0.980
Training loss improved!
Validation loss improved!
Epoch:   9/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.473, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.473, Training Time: 31 seconds)
Epoch:   9/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.511, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.492, Training Time: 65 seconds)
Epoch:   9/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.633, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.539, Training Time: 101 seconds)
Epoch:   9/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.757, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.593, Training Time: 144 seconds)
Epoch:   9/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.609, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.594, Training Time: 154 seconds)
Epoch:   9/100, Validation loss:  6.912, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.980 to  0.978
Training loss improved!
Epoch:  10/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.448, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.448, Training Time: 31 seconds)
Epoch:  10/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.495, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.471, Training Time: 64 seconds)
Epoch:  10/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.618, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.520, Training Time: 101 seconds)
Epoch:  10/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.745, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.577, Training Time: 144 seconds)
Epoch:  10/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.590, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.577, Training Time: 154 seconds)
Epoch:  10/100, Validation loss:  6.882, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.978 to  0.975
Training loss improved!
Validation loss improved!
Epoch:  11/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.418, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.418, Training Time: 31 seconds)
Epoch:  11/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.483, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.450, Training Time: 64 seconds)
Epoch:  11/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.608, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.503, Training Time: 101 seconds)
Epoch:  11/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.730, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.560, Training Time: 143 seconds)
Epoch:  11/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.571, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.560, Training Time: 153 seconds)
Epoch:  11/100, Validation loss:  6.893, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.975 to  0.973
Training loss improved!
Epoch:  12/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.397, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.397, Training Time: 31 seconds)
Epoch:  12/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.476, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.436, Training Time: 64 seconds)
Epoch:  12/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.601, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.491, Training Time: 101 seconds)
Epoch:  12/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.725, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.550, Training Time: 143 seconds)
Epoch:  12/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.553, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.550, Training Time: 153 seconds)
Epoch:  12/100, Validation loss:  6.843, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.973 to  0.970
Training loss improved!
Validation loss improved!
Epoch:  13/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.401, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.401, Training Time: 31 seconds)
Epoch:  13/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.461, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.431, Training Time: 64 seconds)
Epoch:  13/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.590, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.484, Training Time: 101 seconds)
Epoch:  13/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.708, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.540, Training Time: 143 seconds)
Epoch:  13/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.530, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.539, Training Time: 153 seconds)
Epoch:  13/100, Validation loss:  6.875, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.970 to  0.968
Training loss improved!
Epoch:  14/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.370, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.370, Training Time: 31 seconds)
Epoch:  14/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.444, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.407, Training Time: 64 seconds)
Epoch:  14/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.579, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.464, Training Time: 101 seconds)
Epoch:  14/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.685, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.519, Training Time: 143 seconds)
Epoch:  14/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.484, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.517, Training Time: 153 seconds)
Epoch:  14/100, Validation loss:  6.813, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.968 to  0.966
Training loss improved!
Validation loss improved!
Epoch:  15/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.355, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.355, Training Time: 31 seconds)
Epoch:  15/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.424, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.389, Training Time: 64 seconds)
Epoch:  15/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.540, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.440, Training Time: 101 seconds)
Epoch:  15/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.657, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.494, Training Time: 143 seconds)
Epoch:  15/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.438, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.491, Training Time: 153 seconds)
Epoch:  15/100, Validation loss:  6.830, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.966 to  0.963
Training loss improved!
Epoch:  16/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.342, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.342, Training Time: 31 seconds)
Epoch:  16/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.399, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.370, Training Time: 64 seconds)
Epoch:  16/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.513, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.418, Training Time: 101 seconds)
Epoch:  16/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.606, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.465, Training Time: 143 seconds)
Epoch:  16/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.390, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.461, Training Time: 153 seconds)
Epoch:  16/100, Validation loss:  6.760, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.963 to  0.961
Training loss improved!
Validation loss improved!
Epoch:  17/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.308, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.308, Training Time: 31 seconds)
Epoch:  17/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.380, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.344, Training Time: 64 seconds)
Epoch:  17/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.494, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.394, Training Time: 101 seconds)
Epoch:  17/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.589, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.443, Training Time: 143 seconds)
Epoch:  17/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.328, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.437, Training Time: 153 seconds)
Epoch:  17/100, Validation loss:  7.455, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.961 to  0.958
Training loss improved!
Epoch:  18/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.289, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.289, Training Time: 31 seconds)
Epoch:  18/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.355, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.322, Training Time: 64 seconds)
Epoch:  18/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.474, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.373, Training Time: 101 seconds)
Epoch:  18/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.543, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.415, Training Time: 143 seconds)
Epoch:  18/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.278, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.408, Training Time: 153 seconds)
Epoch:  18/100, Validation loss:  6.742, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.958 to  0.956
Training loss improved!
Validation loss improved!
Epoch:  19/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.263, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.263, Training Time: 31 seconds)
Epoch:  19/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.329, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.296, Training Time: 64 seconds)
Epoch:  19/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.458, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.350, Training Time: 101 seconds)
Epoch:  19/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.520, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.393, Training Time: 143 seconds)
Epoch:  19/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.258, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.386, Training Time: 153 seconds)
Epoch:  19/100, Validation loss:  6.667, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.956 to  0.954
Training loss improved!
Validation loss improved!
Epoch:  20/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.240, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.240, Training Time: 31 seconds)
Epoch:  20/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.312, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.276, Training Time: 64 seconds)
Epoch:  20/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.442, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.331, Training Time: 101 seconds)
Epoch:  20/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.522, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.379, Training Time: 143 seconds)
Epoch:  20/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.239, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.372, Training Time: 153 seconds)
Epoch:  20/100, Validation loss:  6.673, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.954 to  0.951
Training loss improved!
Epoch:  21/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.213, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.213, Training Time: 31 seconds)
Epoch:  21/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.299, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.256, Training Time: 64 seconds)
Epoch:  21/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.426, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.312, Training Time: 101 seconds)
Epoch:  21/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.473, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.353, Training Time: 143 seconds)
Epoch:  21/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.234, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.346, Training Time: 153 seconds)
Epoch:  21/100, Validation loss:  6.670, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.951 to  0.949
Training loss improved!
Epoch:  22/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.183, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.183, Training Time: 31 seconds)
Epoch:  22/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.283, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.233, Training Time: 65 seconds)
Epoch:  22/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.410, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.292, Training Time: 101 seconds)
Epoch:  22/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.451, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.332, Training Time: 144 seconds)
Epoch:  22/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.177, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.324, Training Time: 154 seconds)
Epoch:  22/100, Validation loss:  6.693, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.949 to  0.946
Training loss improved!
Epoch:  23/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.151, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.151, Training Time: 31 seconds)
Epoch:  23/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.269, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.210, Training Time: 64 seconds)
Epoch:  23/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.390, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.270, Training Time: 101 seconds)
Epoch:  23/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.439, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.312, Training Time: 143 seconds)
Epoch:  23/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.157, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.304, Training Time: 153 seconds)
Epoch:  23/100, Validation loss:  6.617, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.946 to  0.944
Training loss improved!
Validation loss improved!
Epoch:  24/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.132, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.132, Training Time: 31 seconds)
Epoch:  24/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.255, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.194, Training Time: 64 seconds)
Epoch:  24/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.377, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.255, Training Time: 101 seconds)
Epoch:  24/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.405, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.292, Training Time: 144 seconds)
Epoch:  24/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.130, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.284, Training Time: 154 seconds)
Epoch:  24/100, Validation loss:  6.569, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.944 to  0.942
Training loss improved!
Validation loss improved!
Epoch:  25/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.103, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.103, Training Time: 31 seconds)
Epoch:  25/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.240, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.172, Training Time: 64 seconds)
Epoch:  25/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.360, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.234, Training Time: 101 seconds)
Epoch:  25/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.375, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.270, Training Time: 143 seconds)
Epoch:  25/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.079, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.260, Training Time: 153 seconds)
Epoch:  25/100, Validation loss:  6.569, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.942 to  0.939
Training loss improved!
Validation loss improved!
Epoch:  26/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.079, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.079, Training Time: 31 seconds)
Epoch:  26/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.224, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.151, Training Time: 64 seconds)
Epoch:  26/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.340, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.214, Training Time: 101 seconds)
Epoch:  26/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.364, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.252, Training Time: 143 seconds)
Epoch:  26/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.053, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.242, Training Time: 153 seconds)
Epoch:  26/100, Validation loss:  6.586, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.939 to  0.937
Training loss improved!
Epoch:  27/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.063, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.063, Training Time: 31 seconds)
Epoch:  27/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.215, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.139, Training Time: 64 seconds)
Epoch:  27/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.324, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.201, Training Time: 101 seconds)
Epoch:  27/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.335, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.234, Training Time: 143 seconds)
Epoch:  27/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  6.033, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.224, Training Time: 154 seconds)
Epoch:  27/100, Validation loss:  6.674, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.937 to  0.935
Training loss improved!
Epoch:  28/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.055, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.055, Training Time: 31 seconds)
Epoch:  28/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.197, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.126, Training Time: 64 seconds)
Epoch:  28/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.309, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.187, Training Time: 101 seconds)
Epoch:  28/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.301, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.216, Training Time: 143 seconds)
Epoch:  28/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.965, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.203, Training Time: 153 seconds)
Epoch:  28/100, Validation loss:  6.799, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.935 to  0.932
Training loss improved!
Epoch:  29/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.015, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.015, Training Time: 31 seconds)
Epoch:  29/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.184, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.100, Training Time: 64 seconds)
Epoch:  29/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.296, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.165, Training Time: 101 seconds)
Epoch:  29/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.280, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.194, Training Time: 143 seconds)
Epoch:  29/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.938, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.180, Training Time: 154 seconds)
Epoch:  29/100, Validation loss:  6.643, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.932 to  0.930
Training loss improved!
Epoch:  30/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  6.024, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.024, Training Time: 31 seconds)
Epoch:  30/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.173, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.098, Training Time: 64 seconds)
Epoch:  30/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.283, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.160, Training Time: 101 seconds)
Epoch:  30/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.258, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.184, Training Time: 143 seconds)
Epoch:  30/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.948, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.172, Training Time: 153 seconds)
Epoch:  30/100, Validation loss:  6.482, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.930 to  0.928
Training loss improved!
Validation loss improved!
Epoch:  31/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.995, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.995, Training Time: 31 seconds)
Epoch:  31/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.152, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.074, Training Time: 64 seconds)
Epoch:  31/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.254, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.134, Training Time: 101 seconds)
Epoch:  31/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.239, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.160, Training Time: 143 seconds)
Epoch:  31/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.875, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.145, Training Time: 153 seconds)
Epoch:  31/100, Validation loss:  6.482, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.928 to  0.925
Training loss improved!
Validation loss improved!
Epoch:  32/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.971, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.971, Training Time: 31 seconds)
Epoch:  32/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.138, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.055, Training Time: 64 seconds)
Epoch:  32/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.238, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.116, Training Time: 101 seconds)
Epoch:  32/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.222, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.142, Training Time: 143 seconds)
Epoch:  32/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.809, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.125, Training Time: 153 seconds)
Epoch:  32/100, Validation loss:  6.567, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.925 to  0.923
Training loss improved!
Epoch:  33/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.936, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.936, Training Time: 31 seconds)
Epoch:  33/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.121, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.028, Training Time: 64 seconds)
Epoch:  33/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.209, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.089, Training Time: 101 seconds)
Epoch:  33/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.179, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.111, Training Time: 143 seconds)
Epoch:  33/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.829, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.096, Training Time: 153 seconds)
Epoch:  33/100, Validation loss:  6.503, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.923 to  0.921
Training loss improved!
Epoch:  34/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.990, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.990, Training Time: 31 seconds)
Epoch:  34/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.102, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.046, Training Time: 64 seconds)
Epoch:  34/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.184, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.092, Training Time: 101 seconds)
Epoch:  34/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.149, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.106, Training Time: 143 seconds)
Epoch:  34/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.743, Training Time: 10 seconds), Stats for epoch: (Training Loss:  6.087, Training Time: 153 seconds)
Epoch:  34/100, Validation loss:  6.493, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.921 to  0.918
Training loss improved!
Epoch:  35/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.929, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.929, Training Time: 31 seconds)
Epoch:  35/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.073, Training Time: 33 seconds), Stats for epoch: (Training Loss:  6.001, Training Time: 64 seconds)
Epoch:  35/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.158, Training Time: 36 seconds), Stats for epoch: (Training Loss:  6.053, Training Time: 101 seconds)
Epoch:  35/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.108, Training Time: 41 seconds), Stats for epoch: (Training Loss:  6.067, Training Time: 143 seconds)
Epoch:  35/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.718, Training Time: 9 seconds), Stats for epoch: (Training Loss:  6.049, Training Time: 153 seconds)
Epoch:  35/100, Validation loss:  6.371, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.918 to  0.916
Training loss improved!
Validation loss improved!
Epoch:  36/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.846, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.846, Training Time: 31 seconds)
Epoch:  36/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  6.029, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.938, Training Time: 64 seconds)
Epoch:  36/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.118, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.998, Training Time: 101 seconds)
Epoch:  36/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.049, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.011, Training Time: 143 seconds)
Epoch:  36/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.614, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.990, Training Time: 153 seconds)
Epoch:  36/100, Validation loss:  6.356, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.916 to  0.914
Training loss improved!
Validation loss improved!
Epoch:  37/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.790, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.790, Training Time: 31 seconds)
Epoch:  37/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.983, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.887, Training Time: 64 seconds)
Epoch:  37/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  6.050, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.941, Training Time: 101 seconds)
Epoch:  37/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  6.014, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.959, Training Time: 143 seconds)
Epoch:  37/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.551, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.938, Training Time: 153 seconds)
Epoch:  37/100, Validation loss:  6.257, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.914 to  0.912
Training loss improved!
Validation loss improved!
Epoch:  38/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.717, Training Time: 30 seconds), Stats for epoch: (Training Loss:  5.717, Training Time: 30 seconds)
Epoch:  38/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.892, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.804, Training Time: 64 seconds)
Epoch:  38/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.956, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.855, Training Time: 100 seconds)
Epoch:  38/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.886, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.863, Training Time: 142 seconds)
Epoch:  38/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.435, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.840, Training Time: 152 seconds)
Epoch:  38/100, Validation loss:  6.206, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.912 to  0.909
Training loss improved!
Validation loss improved!
Epoch:  39/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.592, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.592, Training Time: 31 seconds)
Epoch:  39/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.785, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.689, Training Time: 64 seconds)
Epoch:  39/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.857, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.745, Training Time: 101 seconds)
Epoch:  39/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.791, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.756, Training Time: 143 seconds)
Epoch:  39/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.289, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.732, Training Time: 153 seconds)
Epoch:  39/100, Validation loss:  6.111, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.909 to  0.907
Training loss improved!
Validation loss improved!
Epoch:  40/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.490, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.490, Training Time: 31 seconds)
Epoch:  40/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.693, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.592, Training Time: 64 seconds)
Epoch:  40/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.773, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.652, Training Time: 101 seconds)
Epoch:  40/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.698, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.664, Training Time: 143 seconds)
Epoch:  40/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.217, Training Time: 10 seconds), Stats for epoch: (Training Loss:  5.640, Training Time: 153 seconds)
Epoch:  40/100, Validation loss:  6.125, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.907 to  0.905
Training loss improved!
Epoch:  41/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.416, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.416, Training Time: 31 seconds)
Epoch:  41/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.613, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.514, Training Time: 64 seconds)
Epoch:  41/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.698, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.576, Training Time: 101 seconds)
Epoch:  41/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.605, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.583, Training Time: 143 seconds)
Epoch:  41/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.058, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.556, Training Time: 153 seconds)
Epoch:  41/100, Validation loss:  5.956, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.905 to  0.902
Training loss improved!
Validation loss improved!
Epoch:  42/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.324, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.324, Training Time: 31 seconds)
Epoch:  42/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.538, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.431, Training Time: 64 seconds)
Epoch:  42/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.626, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.496, Training Time: 101 seconds)
Epoch:  42/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.526, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.503, Training Time: 144 seconds)
Epoch:  42/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.892, Training Time: 10 seconds), Stats for epoch: (Training Loss:  5.472, Training Time: 154 seconds)
Epoch:  42/100, Validation loss:  5.922, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.902 to  0.900
Training loss improved!
Validation loss improved!
Epoch:  43/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.236, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.236, Training Time: 31 seconds)
Epoch:  43/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.470, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.353, Training Time: 64 seconds)
Epoch:  43/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.564, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.423, Training Time: 101 seconds)
Epoch:  43/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.481, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.438, Training Time: 144 seconds)
Epoch:  43/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.809, Training Time: 10 seconds), Stats for epoch: (Training Loss:  5.405, Training Time: 154 seconds)
Epoch:  43/100, Validation loss:  5.816, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.900 to  0.898
Training loss improved!
Validation loss improved!
Epoch:  44/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.173, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.173, Training Time: 31 seconds)
Epoch:  44/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.402, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.288, Training Time: 64 seconds)
Epoch:  44/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.502, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.359, Training Time: 101 seconds)
Epoch:  44/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.379, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.364, Training Time: 143 seconds)
Epoch:  44/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.676, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.328, Training Time: 153 seconds)
Epoch:  44/100, Validation loss:  5.770, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.898 to  0.896
Training loss improved!
Validation loss improved!
Epoch:  45/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.090, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.090, Training Time: 31 seconds)
Epoch:  45/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.335, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.213, Training Time: 64 seconds)
Epoch:  45/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.436, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.287, Training Time: 101 seconds)
Epoch:  45/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.359, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.305, Training Time: 143 seconds)
Epoch:  45/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  5.087, Training Time: 9 seconds), Stats for epoch: (Training Loss:  5.294, Training Time: 153 seconds)
Epoch:  45/100, Validation loss:  5.758, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.896 to  0.893
Training loss improved!
Validation loss improved!
Epoch:  46/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  5.058, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.058, Training Time: 31 seconds)
Epoch:  46/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.283, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.171, Training Time: 64 seconds)
Epoch:  46/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.389, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.243, Training Time: 101 seconds)
Epoch:  46/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.270, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.250, Training Time: 143 seconds)
Epoch:  46/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.528, Training Time: 10 seconds), Stats for epoch: (Training Loss:  5.213, Training Time: 153 seconds)
Epoch:  46/100, Validation loss:  5.710, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.893 to  0.891
Training loss improved!
Validation loss improved!
Epoch:  47/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.953, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.953, Training Time: 31 seconds)
Epoch:  47/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.223, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.088, Training Time: 64 seconds)
Epoch:  47/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.329, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.168, Training Time: 101 seconds)
Epoch:  47/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.191, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.174, Training Time: 144 seconds)
Epoch:  47/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.337, Training Time: 10 seconds), Stats for epoch: (Training Loss:  5.130, Training Time: 154 seconds)
Epoch:  47/100, Validation loss:  5.613, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.891 to  0.889
Training loss improved!
Validation loss improved!
Epoch:  48/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.890, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.890, Training Time: 31 seconds)
Epoch:  48/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.160, Training Time: 33 seconds), Stats for epoch: (Training Loss:  5.025, Training Time: 64 seconds)
Epoch:  48/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.276, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.108, Training Time: 101 seconds)
Epoch:  48/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.137, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.115, Training Time: 144 seconds)
Epoch:  48/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.265, Training Time: 10 seconds), Stats for epoch: (Training Loss:  5.071, Training Time: 154 seconds)
Epoch:  48/100, Validation loss:  5.591, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.889 to  0.887
Training loss improved!
Validation loss improved!
Epoch:  49/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.823, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.823, Training Time: 31 seconds)
Epoch:  49/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.097, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.960, Training Time: 64 seconds)
Epoch:  49/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.222, Training Time: 36 seconds), Stats for epoch: (Training Loss:  5.047, Training Time: 101 seconds)
Epoch:  49/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.081, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.056, Training Time: 143 seconds)
Epoch:  49/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.187, Training Time: 10 seconds), Stats for epoch: (Training Loss:  5.010, Training Time: 153 seconds)
Epoch:  49/100, Validation loss:  5.556, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.887 to  0.885
Training loss improved!
Validation loss improved!
Epoch:  50/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.754, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.754, Training Time: 31 seconds)
Epoch:  50/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  5.037, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.895, Training Time: 64 seconds)
Epoch:  50/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.161, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.984, Training Time: 101 seconds)
Epoch:  50/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  5.027, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.995, Training Time: 144 seconds)
Epoch:  50/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.112, Training Time: 10 seconds), Stats for epoch: (Training Loss:  4.949, Training Time: 154 seconds)
Epoch:  50/100, Validation loss:  5.504, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.885 to  0.882
Training loss improved!
Validation loss improved!
Epoch:  51/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.684, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.684, Training Time: 31 seconds)
Epoch:  51/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.979, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.831, Training Time: 64 seconds)
Epoch:  51/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.104, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.922, Training Time: 101 seconds)
Epoch:  51/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.985, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.938, Training Time: 143 seconds)
Epoch:  51/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.065, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.893, Training Time: 153 seconds)
Epoch:  51/100, Validation loss:  5.492, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.882 to  0.880
Training loss improved!
Validation loss improved!
Epoch:  52/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.615, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.615, Training Time: 31 seconds)
Epoch:  52/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.913, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.764, Training Time: 64 seconds)
Epoch:  52/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  5.048, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.859, Training Time: 101 seconds)
Epoch:  52/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.930, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.877, Training Time: 143 seconds)
Epoch:  52/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.996, Training Time: 10 seconds), Stats for epoch: (Training Loss:  4.831, Training Time: 153 seconds)
Epoch:  52/100, Validation loss:  5.442, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.880 to  0.878
Training loss improved!
Validation loss improved!
Epoch:  53/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.552, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.552, Training Time: 31 seconds)
Epoch:  53/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.851, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.702, Training Time: 64 seconds)
Epoch:  53/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.991, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.798, Training Time: 101 seconds)
Epoch:  53/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.881, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.819, Training Time: 143 seconds)
Epoch:  53/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  4.021, Training Time: 10 seconds), Stats for epoch: (Training Loss:  4.777, Training Time: 153 seconds)
Epoch:  53/100, Validation loss:  5.733, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.878 to  0.876
Training loss improved!
Epoch:  54/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.486, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.486, Training Time: 31 seconds)
Epoch:  54/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.789, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.638, Training Time: 64 seconds)
Epoch:  54/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.936, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.737, Training Time: 101 seconds)
Epoch:  54/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.834, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.761, Training Time: 144 seconds)
Epoch:  54/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.889, Training Time: 10 seconds), Stats for epoch: (Training Loss:  4.716, Training Time: 154 seconds)
Epoch:  54/100, Validation loss:  5.368, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.876 to  0.874
Training loss improved!
Validation loss improved!
Epoch:  55/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.417, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.417, Training Time: 31 seconds)
Epoch:  55/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.724, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.570, Training Time: 64 seconds)
Epoch:  55/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.878, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.673, Training Time: 101 seconds)
Epoch:  55/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.785, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.701, Training Time: 144 seconds)
Epoch:  55/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.834, Training Time: 10 seconds), Stats for epoch: (Training Loss:  4.656, Training Time: 154 seconds)
Epoch:  55/100, Validation loss:  5.331, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.874 to  0.871
Training loss improved!
Validation loss improved!
Epoch:  56/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.353, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.353, Training Time: 31 seconds)
Epoch:  56/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.660, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.507, Training Time: 64 seconds)
Epoch:  56/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.818, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.611, Training Time: 101 seconds)
Epoch:  56/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.740, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.643, Training Time: 143 seconds)
Epoch:  56/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.784, Training Time: 10 seconds), Stats for epoch: (Training Loss:  4.598, Training Time: 153 seconds)
Epoch:  56/100, Validation loss:  5.319, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.871 to  0.869
Training loss improved!
Validation loss improved!
Epoch:  57/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.279, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.279, Training Time: 31 seconds)
Epoch:  57/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.603, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.441, Training Time: 64 seconds)
Epoch:  57/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.763, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.548, Training Time: 101 seconds)
Epoch:  57/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.696, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.585, Training Time: 144 seconds)
Epoch:  57/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.750, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.542, Training Time: 154 seconds)
Epoch:  57/100, Validation loss:  5.287, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.869 to  0.867
Training loss improved!
Validation loss improved!
Epoch:  58/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.227, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.227, Training Time: 31 seconds)
Epoch:  58/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.535, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.381, Training Time: 64 seconds)
Epoch:  58/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.707, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.490, Training Time: 101 seconds)
Epoch:  58/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.653, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.531, Training Time: 143 seconds)
Epoch:  58/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.694, Training Time: 10 seconds), Stats for epoch: (Training Loss:  4.487, Training Time: 153 seconds)
Epoch:  58/100, Validation loss:  5.254, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.867 to  0.865
Training loss improved!
Validation loss improved!
Epoch:  59/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.147, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.147, Training Time: 31 seconds)
Epoch:  59/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.473, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.310, Training Time: 64 seconds)
Epoch:  59/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.650, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.424, Training Time: 101 seconds)
Epoch:  59/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.608, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.470, Training Time: 143 seconds)
Epoch:  59/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.654, Training Time: 10 seconds), Stats for epoch: (Training Loss:  4.427, Training Time: 153 seconds)
Epoch:  59/100, Validation loss:  5.257, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.865 to  0.863
Training loss improved!
Epoch:  60/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.086, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.086, Training Time: 31 seconds)
Epoch:  60/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.412, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.249, Training Time: 64 seconds)
Epoch:  60/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.591, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.363, Training Time: 101 seconds)
Epoch:  60/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.565, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.413, Training Time: 144 seconds)
Epoch:  60/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.616, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.372, Training Time: 154 seconds)
Epoch:  60/100, Validation loss:  5.231, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.863 to  0.861
Training loss improved!
Validation loss improved!
Epoch:  61/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  4.027, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.027, Training Time: 31 seconds)
Epoch:  61/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.350, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.188, Training Time: 64 seconds)
Epoch:  61/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.532, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.303, Training Time: 101 seconds)
Epoch:  61/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.533, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.361, Training Time: 143 seconds)
Epoch:  61/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.574, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.320, Training Time: 153 seconds)
Epoch:  61/100, Validation loss:  5.242, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.861 to  0.858
Training loss improved!
Epoch:  62/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.953, Training Time: 31 seconds), Stats for epoch: (Training Loss:  3.953, Training Time: 31 seconds)
Epoch:  62/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.278, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.115, Training Time: 64 seconds)
Epoch:  62/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.475, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.235, Training Time: 101 seconds)
Epoch:  62/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.481, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.297, Training Time: 143 seconds)
Epoch:  62/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.524, Training Time: 10 seconds), Stats for epoch: (Training Loss:  4.256, Training Time: 153 seconds)
Epoch:  62/100, Validation loss:  5.217, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.858 to  0.856
Training loss improved!
Validation loss improved!
Epoch:  63/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.887, Training Time: 31 seconds), Stats for epoch: (Training Loss:  3.887, Training Time: 31 seconds)
Epoch:  63/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.221, Training Time: 33 seconds), Stats for epoch: (Training Loss:  4.054, Training Time: 65 seconds)
Epoch:  63/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.415, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.175, Training Time: 101 seconds)
Epoch:  63/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.435, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.240, Training Time: 144 seconds)
Epoch:  63/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.481, Training Time: 9 seconds), Stats for epoch: (Training Loss:  4.200, Training Time: 154 seconds)
Epoch:  63/100, Validation loss:  5.210, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.856 to  0.854
Training loss improved!
Validation loss improved!
Epoch:  64/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.821, Training Time: 31 seconds), Stats for epoch: (Training Loss:  3.821, Training Time: 31 seconds)
Epoch:  64/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.155, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.988, Training Time: 64 seconds)
Epoch:  64/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.360, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.112, Training Time: 101 seconds)
Epoch:  64/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.398, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.184, Training Time: 144 seconds)
Epoch:  64/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.455, Training Time: 10 seconds), Stats for epoch: (Training Loss:  4.146, Training Time: 154 seconds)
Epoch:  64/100, Validation loss:  5.213, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.854 to  0.852
Training loss improved!
Epoch:  65/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.754, Training Time: 31 seconds), Stats for epoch: (Training Loss:  3.754, Training Time: 31 seconds)
Epoch:  65/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.084, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.919, Training Time: 64 seconds)
Epoch:  65/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.297, Training Time: 36 seconds), Stats for epoch: (Training Loss:  4.045, Training Time: 101 seconds)
Epoch:  65/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.355, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.122, Training Time: 143 seconds)
Epoch:  65/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.410, Training Time: 10 seconds), Stats for epoch: (Training Loss:  4.085, Training Time: 154 seconds)
Epoch:  65/100, Validation loss:  5.196, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.852 to  0.850
Training loss improved!
Validation loss improved!
Epoch:  66/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.680, Training Time: 31 seconds), Stats for epoch: (Training Loss:  3.680, Training Time: 31 seconds)
Epoch:  66/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  4.015, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.848, Training Time: 64 seconds)
Epoch:  66/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.234, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.977, Training Time: 101 seconds)
Epoch:  66/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.309, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.060, Training Time: 143 seconds)
Epoch:  66/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.365, Training Time: 10 seconds), Stats for epoch: (Training Loss:  4.023, Training Time: 153 seconds)
Epoch:  66/100, Validation loss:  5.201, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.850 to  0.848
Training loss improved!
Epoch:  67/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.620, Training Time: 31 seconds), Stats for epoch: (Training Loss:  3.620, Training Time: 31 seconds)
Epoch:  67/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.947, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.784, Training Time: 64 seconds)
Epoch:  67/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.173, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.914, Training Time: 101 seconds)
Epoch:  67/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.278, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.005, Training Time: 143 seconds)
Epoch:  67/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.409, Training Time: 10 seconds), Stats for epoch: (Training Loss:  3.973, Training Time: 153 seconds)
Epoch:  67/100, Validation loss:  5.256, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.848 to  0.846
Training loss improved!
Epoch:  68/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.562, Training Time: 31 seconds), Stats for epoch: (Training Loss:  3.562, Training Time: 31 seconds)
Epoch:  68/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.885, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.724, Training Time: 64 seconds)
Epoch:  68/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.117, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.855, Training Time: 101 seconds)
Epoch:  68/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.228, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.948, Training Time: 143 seconds)
Epoch:  68/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.293, Training Time: 10 seconds), Stats for epoch: (Training Loss:  3.914, Training Time: 154 seconds)
Epoch:  68/100, Validation loss:  5.179, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.846 to  0.843
Training loss improved!
Validation loss improved!
Epoch:  69/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.484, Training Time: 31 seconds), Stats for epoch: (Training Loss:  3.484, Training Time: 31 seconds)
Epoch:  69/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.814, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.649, Training Time: 64 seconds)
Epoch:  69/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  4.056, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.785, Training Time: 101 seconds)
Epoch:  69/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.177, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.883, Training Time: 144 seconds)
Epoch:  69/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.250, Training Time: 10 seconds), Stats for epoch: (Training Loss:  3.850, Training Time: 154 seconds)
Epoch:  69/100, Validation loss:  5.184, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.843 to  0.841
Training loss improved!
Epoch:  70/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.411, Training Time: 31 seconds), Stats for epoch: (Training Loss:  3.411, Training Time: 31 seconds)
Epoch:  70/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.746, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.579, Training Time: 64 seconds)
Epoch:  70/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.990, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.716, Training Time: 101 seconds)
Epoch:  70/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.132, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.820, Training Time: 144 seconds)
Epoch:  70/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.210, Training Time: 10 seconds), Stats for epoch: (Training Loss:  3.788, Training Time: 154 seconds)
Epoch:  70/100, Validation loss:  5.193, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.841 to  0.839
Training loss improved!
Epoch:  71/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.339, Training Time: 31 seconds), Stats for epoch: (Training Loss:  3.339, Training Time: 31 seconds)
Epoch:  71/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.678, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.508, Training Time: 64 seconds)
Epoch:  71/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.917, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.645, Training Time: 101 seconds)
Epoch:  71/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.081, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.754, Training Time: 144 seconds)
Epoch:  71/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.165, Training Time: 10 seconds), Stats for epoch: (Training Loss:  3.723, Training Time: 154 seconds)
Epoch:  71/100, Validation loss:  5.233, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.839 to  0.837
Training loss improved!
Epoch:  72/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.265, Training Time: 31 seconds), Stats for epoch: (Training Loss:  3.265, Training Time: 31 seconds)
Epoch:  72/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.607, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.436, Training Time: 64 seconds)
Epoch:  72/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.856, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.576, Training Time: 101 seconds)
Epoch:  72/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.043, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.693, Training Time: 144 seconds)
Epoch:  72/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.127, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.663, Training Time: 154 seconds)
Epoch:  72/100, Validation loss:  5.244, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.837 to  0.835
Training loss improved!
Epoch:  73/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.207, Training Time: 31 seconds), Stats for epoch: (Training Loss:  3.207, Training Time: 31 seconds)
Epoch:  73/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.530, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.368, Training Time: 64 seconds)
Epoch:  73/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.795, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.511, Training Time: 101 seconds)
Epoch:  73/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.993, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.631, Training Time: 144 seconds)
Epoch:  73/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.091, Training Time: 10 seconds), Stats for epoch: (Training Loss:  3.603, Training Time: 154 seconds)
Epoch:  73/100, Validation loss:  5.234, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.835 to  0.833
Training loss improved!
Epoch:  74/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.136, Training Time: 31 seconds), Stats for epoch: (Training Loss:  3.136, Training Time: 31 seconds)
Epoch:  74/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.462, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.299, Training Time: 64 seconds)
Epoch:  74/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.727, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.442, Training Time: 101 seconds)
Epoch:  74/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  4.062, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.597, Training Time: 143 seconds)
Epoch:  74/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.220, Training Time: 10 seconds), Stats for epoch: (Training Loss:  3.577, Training Time: 153 seconds)
Epoch:  74/100, Validation loss:  5.317, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.833 to  0.831
Training loss improved!
Epoch:  75/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.085, Training Time: 31 seconds), Stats for epoch: (Training Loss:  3.085, Training Time: 31 seconds)
Epoch:  75/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.395, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.240, Training Time: 64 seconds)
Epoch:  75/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.658, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.379, Training Time: 101 seconds)
Epoch:  75/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.900, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.510, Training Time: 144 seconds)
Epoch:  75/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  3.021, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.484, Training Time: 154 seconds)
Epoch:  75/100, Validation loss:  5.263, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.831 to  0.829
Training loss improved!
Epoch:  76/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  3.012, Training Time: 31 seconds), Stats for epoch: (Training Loss:  3.012, Training Time: 31 seconds)
Epoch:  76/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.329, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.171, Training Time: 64 seconds)
Epoch:  76/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.590, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.310, Training Time: 101 seconds)
Epoch:  76/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.845, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.444, Training Time: 143 seconds)
Epoch:  76/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.972, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.419, Training Time: 153 seconds)
Epoch:  76/100, Validation loss:  5.273, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.829 to  0.827
Training loss improved!
Epoch:  77/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.932, Training Time: 31 seconds), Stats for epoch: (Training Loss:  2.932, Training Time: 31 seconds)
Epoch:  77/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.252, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.092, Training Time: 64 seconds)
Epoch:  77/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.522, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.235, Training Time: 101 seconds)
Epoch:  77/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.810, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.379, Training Time: 143 seconds)
Epoch:  77/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.943, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.356, Training Time: 153 seconds)
Epoch:  77/100, Validation loss:  5.314, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.827 to  0.825
Training loss improved!
Epoch:  78/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.869, Training Time: 31 seconds), Stats for epoch: (Training Loss:  2.869, Training Time: 31 seconds)
Epoch:  78/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.183, Training Time: 33 seconds), Stats for epoch: (Training Loss:  3.026, Training Time: 64 seconds)
Epoch:  78/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.456, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.169, Training Time: 101 seconds)
Epoch:  78/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.756, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.316, Training Time: 144 seconds)
Epoch:  78/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.890, Training Time: 9 seconds), Stats for epoch: (Training Loss:  3.294, Training Time: 153 seconds)
Epoch:  78/100, Validation loss:  5.309, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.825 to  0.823
Training loss improved!
Epoch:  79/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.797, Training Time: 31 seconds), Stats for epoch: (Training Loss:  2.797, Training Time: 31 seconds)
Epoch:  79/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.115, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.956, Training Time: 64 seconds)
Epoch:  79/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.387, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.100, Training Time: 101 seconds)
Epoch:  79/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.697, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.249, Training Time: 143 seconds)
Epoch:  79/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.844, Training Time: 10 seconds), Stats for epoch: (Training Loss:  3.228, Training Time: 153 seconds)
Epoch:  79/100, Validation loss:  5.354, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.823 to  0.821
Training loss improved!
Epoch:  80/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.739, Training Time: 31 seconds), Stats for epoch: (Training Loss:  2.739, Training Time: 31 seconds)
Epoch:  80/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  3.045, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.892, Training Time: 64 seconds)
Epoch:  80/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.315, Training Time: 36 seconds), Stats for epoch: (Training Loss:  3.033, Training Time: 101 seconds)
Epoch:  80/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.657, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.189, Training Time: 144 seconds)
Epoch:  80/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.821, Training Time: 10 seconds), Stats for epoch: (Training Loss:  3.170, Training Time: 154 seconds)
Epoch:  80/100, Validation loss:  5.338, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.821 to  0.819
Training loss improved!
Epoch:  81/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.675, Training Time: 31 seconds), Stats for epoch: (Training Loss:  2.675, Training Time: 31 seconds)
Epoch:  81/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.970, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.822, Training Time: 64 seconds)
Epoch:  81/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.257, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.967, Training Time: 101 seconds)
Epoch:  81/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.611, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.128, Training Time: 143 seconds)
Epoch:  81/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.795, Training Time: 10 seconds), Stats for epoch: (Training Loss:  3.111, Training Time: 153 seconds)
Epoch:  81/100, Validation loss:  5.358, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.819 to  0.816
Training loss improved!
Epoch:  82/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.612, Training Time: 31 seconds), Stats for epoch: (Training Loss:  2.612, Training Time: 31 seconds)
Epoch:  82/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.903, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.757, Training Time: 64 seconds)
Epoch:  82/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.181, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.899, Training Time: 101 seconds)
Epoch:  82/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.553, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.062, Training Time: 144 seconds)
Epoch:  82/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.745, Training Time: 10 seconds), Stats for epoch: (Training Loss:  3.046, Training Time: 154 seconds)
Epoch:  82/100, Validation loss:  5.399, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.816 to  0.814
Training loss improved!
Epoch:  83/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.555, Training Time: 31 seconds), Stats for epoch: (Training Loss:  2.555, Training Time: 31 seconds)
Epoch:  83/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.829, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.692, Training Time: 64 seconds)
Epoch:  83/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.111, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.832, Training Time: 101 seconds)
Epoch:  83/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.511, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.002, Training Time: 144 seconds)
Epoch:  83/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.678, Training Time: 10 seconds), Stats for epoch: (Training Loss:  2.985, Training Time: 154 seconds)
Epoch:  83/100, Validation loss:  5.398, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.814 to  0.812
Training loss improved!
Backup to models/csv/20201103_161327_backup_2_985 complete!
Epoch:  84/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.487, Training Time: 31 seconds), Stats for epoch: (Training Loss:  2.487, Training Time: 31 seconds)
Epoch:  84/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.769, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.628, Training Time: 64 seconds)
Epoch:  84/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  3.043, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.766, Training Time: 101 seconds)
Epoch:  84/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.461, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.940, Training Time: 144 seconds)
Epoch:  84/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.643, Training Time: 10 seconds), Stats for epoch: (Training Loss:  2.925, Training Time: 154 seconds)
Epoch:  84/100, Validation loss:  5.453, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.812 to  0.810
Training loss improved!
Epoch:  85/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.433, Training Time: 31 seconds), Stats for epoch: (Training Loss:  2.433, Training Time: 31 seconds)
Epoch:  85/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.701, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.567, Training Time: 64 seconds)
Epoch:  85/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.982, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.705, Training Time: 101 seconds)
Epoch:  85/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.398, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.878, Training Time: 143 seconds)
Epoch:  85/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.612, Training Time: 10 seconds), Stats for epoch: (Training Loss:  2.865, Training Time: 153 seconds)
Epoch:  85/100, Validation loss:  5.478, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.810 to  0.808
Training loss improved!
Epoch:  86/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.373, Training Time: 31 seconds), Stats for epoch: (Training Loss:  2.373, Training Time: 31 seconds)
Epoch:  86/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.638, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.506, Training Time: 64 seconds)
Epoch:  86/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.925, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.645, Training Time: 101 seconds)
Epoch:  86/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.350, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.822, Training Time: 144 seconds)
Epoch:  86/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.570, Training Time: 10 seconds), Stats for epoch: (Training Loss:  2.808, Training Time: 154 seconds)
Epoch:  86/100, Validation loss:  5.505, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.808 to  0.806
Training loss improved!
Epoch:  87/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.321, Training Time: 31 seconds), Stats for epoch: (Training Loss:  2.321, Training Time: 31 seconds)
Epoch:  87/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.573, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.447, Training Time: 64 seconds)
Epoch:  87/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.855, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.583, Training Time: 101 seconds)
Epoch:  87/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.298, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.762, Training Time: 144 seconds)
Epoch:  87/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.539, Training Time: 10 seconds), Stats for epoch: (Training Loss:  2.750, Training Time: 154 seconds)
Epoch:  87/100, Validation loss:  5.485, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.806 to  0.804
Training loss improved!
Epoch:  88/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.264, Training Time: 31 seconds), Stats for epoch: (Training Loss:  2.264, Training Time: 31 seconds)
Epoch:  88/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.515, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.389, Training Time: 64 seconds)
Epoch:  88/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.793, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.524, Training Time: 101 seconds)
Epoch:  88/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.251, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.706, Training Time: 144 seconds)
Epoch:  88/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.550, Training Time: 10 seconds), Stats for epoch: (Training Loss:  2.697, Training Time: 154 seconds)
Epoch:  88/100, Validation loss:  5.766, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.804 to  0.802
Training loss improved!
Epoch:  89/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.235, Training Time: 31 seconds), Stats for epoch: (Training Loss:  2.235, Training Time: 31 seconds)
Epoch:  89/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.457, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.346, Training Time: 64 seconds)
Epoch:  89/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.728, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.473, Training Time: 101 seconds)
Epoch:  89/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.210, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.658, Training Time: 144 seconds)
Epoch:  89/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.457, Training Time: 10 seconds), Stats for epoch: (Training Loss:  2.647, Training Time: 154 seconds)
Epoch:  89/100, Validation loss:  5.572, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.802 to  0.800
Training loss improved!
Epoch:  90/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.160, Training Time: 31 seconds), Stats for epoch: (Training Loss:  2.160, Training Time: 31 seconds)
Epoch:  90/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.386, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.273, Training Time: 64 seconds)
Epoch:  90/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.656, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.401, Training Time: 101 seconds)
Epoch:  90/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.148, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.588, Training Time: 144 seconds)
Epoch:  90/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.400, Training Time: 10 seconds), Stats for epoch: (Training Loss:  2.578, Training Time: 154 seconds)
Epoch:  90/100, Validation loss:  5.569, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.800 to  0.798
Training loss improved!
Epoch:  91/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.106, Training Time: 31 seconds), Stats for epoch: (Training Loss:  2.106, Training Time: 31 seconds)
Epoch:  91/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.322, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.214, Training Time: 64 seconds)
Epoch:  91/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.588, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.339, Training Time: 101 seconds)
Epoch:  91/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.105, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.530, Training Time: 144 seconds)
Epoch:  91/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.372, Training Time: 10 seconds), Stats for epoch: (Training Loss:  2.522, Training Time: 154 seconds)
Epoch:  91/100, Validation loss:  5.595, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.798 to  0.796
Training loss improved!
Epoch:  92/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.063, Training Time: 31 seconds), Stats for epoch: (Training Loss:  2.063, Training Time: 31 seconds)
Epoch:  92/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.266, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.165, Training Time: 64 seconds)
Epoch:  92/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.529, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.286, Training Time: 101 seconds)
Epoch:  92/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  3.046, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.476, Training Time: 144 seconds)
Epoch:  92/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.313, Training Time: 10 seconds), Stats for epoch: (Training Loss:  2.468, Training Time: 154 seconds)
Epoch:  92/100, Validation loss:  5.635, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.796 to  0.794
Training loss improved!
Backup to models/csv/20201103_161327_backup_2_468 complete!
Epoch:  93/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  2.007, Training Time: 31 seconds), Stats for epoch: (Training Loss:  2.007, Training Time: 31 seconds)
Epoch:  93/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.215, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.111, Training Time: 64 seconds)
Epoch:  93/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.475, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.232, Training Time: 101 seconds)
Epoch:  93/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.991, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.422, Training Time: 144 seconds)
Epoch:  93/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.287, Training Time: 10 seconds), Stats for epoch: (Training Loss:  2.415, Training Time: 154 seconds)
Epoch:  93/100, Validation loss:  5.676, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.794 to  0.792
Training loss improved!
Epoch:  94/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.968, Training Time: 31 seconds), Stats for epoch: (Training Loss:  1.968, Training Time: 31 seconds)
Epoch:  94/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.150, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.059, Training Time: 64 seconds)
Epoch:  94/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.402, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.173, Training Time: 101 seconds)
Epoch:  94/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.932, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.363, Training Time: 144 seconds)
Epoch:  94/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.266, Training Time: 10 seconds), Stats for epoch: (Training Loss:  2.358, Training Time: 154 seconds)
Epoch:  94/100, Validation loss:  5.678, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.792 to  0.790
Training loss improved!
Epoch:  95/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.917, Training Time: 31 seconds), Stats for epoch: (Training Loss:  1.917, Training Time: 31 seconds)
Epoch:  95/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.093, Training Time: 33 seconds), Stats for epoch: (Training Loss:  2.005, Training Time: 64 seconds)
Epoch:  95/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.350, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.120, Training Time: 101 seconds)
Epoch:  95/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.892, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.313, Training Time: 143 seconds)
Epoch:  95/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.188, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.306, Training Time: 153 seconds)
Epoch:  95/100, Validation loss:  5.704, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.790 to  0.788
Training loss improved!
Epoch:  96/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.874, Training Time: 31 seconds), Stats for epoch: (Training Loss:  1.874, Training Time: 31 seconds)
Epoch:  96/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  2.037, Training Time: 33 seconds), Stats for epoch: (Training Loss:  1.955, Training Time: 64 seconds)
Epoch:  96/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.303, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.071, Training Time: 101 seconds)
Epoch:  96/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.841, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.263, Training Time: 144 seconds)
Epoch:  96/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.173, Training Time: 10 seconds), Stats for epoch: (Training Loss:  2.259, Training Time: 154 seconds)
Epoch:  96/100, Validation loss:  5.739, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.788 to  0.786
Training loss improved!
Epoch:  97/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.832, Training Time: 31 seconds), Stats for epoch: (Training Loss:  1.832, Training Time: 31 seconds)
Epoch:  97/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  1.995, Training Time: 33 seconds), Stats for epoch: (Training Loss:  1.913, Training Time: 64 seconds)
Epoch:  97/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.243, Training Time: 36 seconds), Stats for epoch: (Training Loss:  2.023, Training Time: 101 seconds)
Epoch:  97/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.793, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.216, Training Time: 144 seconds)
Epoch:  97/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.137, Training Time: 10 seconds), Stats for epoch: (Training Loss:  2.212, Training Time: 154 seconds)
Epoch:  97/100, Validation loss:  5.758, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.786 to  0.784
Training loss improved!
Epoch:  98/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.797, Training Time: 31 seconds), Stats for epoch: (Training Loss:  1.797, Training Time: 31 seconds)
Epoch:  98/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  1.934, Training Time: 33 seconds), Stats for epoch: (Training Loss:  1.865, Training Time: 64 seconds)
Epoch:  98/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.187, Training Time: 36 seconds), Stats for epoch: (Training Loss:  1.973, Training Time: 101 seconds)
Epoch:  98/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.754, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.168, Training Time: 144 seconds)
Epoch:  98/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.068, Training Time: 10 seconds), Stats for epoch: (Training Loss:  2.163, Training Time: 154 seconds)
Epoch:  98/100, Validation loss:  5.787, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.784 to  0.782
Training loss improved!
Epoch:  99/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.757, Training Time: 31 seconds), Stats for epoch: (Training Loss:  1.757, Training Time: 31 seconds)
Epoch:  99/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  1.883, Training Time: 33 seconds), Stats for epoch: (Training Loss:  1.820, Training Time: 64 seconds)
Epoch:  99/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.134, Training Time: 36 seconds), Stats for epoch: (Training Loss:  1.925, Training Time: 101 seconds)
Epoch:  99/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.679, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.113, Training Time: 143 seconds)
Epoch:  99/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.035, Training Time: 9 seconds), Stats for epoch: (Training Loss:  2.109, Training Time: 153 seconds)
Epoch:  99/100, Validation loss:  5.801, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.782 to  0.781
Training loss improved!
Epoch: 100/100, Batch:  100/422, Stats for last 100 batches: (Training Loss:  1.718, Training Time: 31 seconds), Stats for epoch: (Training Loss:  1.718, Training Time: 31 seconds)
Epoch: 100/100, Batch:  200/422, Stats for last 100 batches: (Training Loss:  1.836, Training Time: 33 seconds), Stats for epoch: (Training Loss:  1.777, Training Time: 65 seconds)
Epoch: 100/100, Batch:  300/422, Stats for last 100 batches: (Training Loss:  2.075, Training Time: 36 seconds), Stats for epoch: (Training Loss:  1.876, Training Time: 101 seconds)
Epoch: 100/100, Batch:  400/422, Stats for last 100 batches: (Training Loss:  2.632, Training Time: 42 seconds), Stats for epoch: (Training Loss:  2.065, Training Time: 144 seconds)
Epoch: 100/100, Batch:  422/422, Stats for last 22 batches: (Training Loss:  2.010, Training Time: 10 seconds), Stats for epoch: (Training Loss:  2.062, Training Time: 154 seconds)
Epoch: 100/100, Validation loss:  5.853, Batch Validation Time: 12 seconds
Learning rate decay: adjusting from  0.781 to  0.779
Training loss improved!
Training Complete!
