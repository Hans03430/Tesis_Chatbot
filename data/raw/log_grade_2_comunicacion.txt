
Reading dataset 'csv'...

Final shared vocab size: 68880

Splitting 98264 samples into training & validation sets (20.0% used for validation)...
Training set: 78612 samples. Validation set: 19652 samples.
Sorting training & validation sets to increase training efficiency...
Initializing model...


Creating checkpoint batch files...
Initializing training...
Epochs: 100
Batch Size: 128
Optimizer: sgd
Epoch:   1/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  9.073, Training Time: 45 seconds), Stats for epoch: (Training Loss:  9.073, Training Time: 45 seconds)
Epoch:   1/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.730, Training Time: 45 seconds), Stats for epoch: (Training Loss:  8.401, Training Time: 91 seconds)
Epoch:   1/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.549, Training Time: 48 seconds), Stats for epoch: (Training Loss:  8.117, Training Time: 139 seconds)
Epoch:   1/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.453, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.951, Training Time: 190 seconds)
Epoch:   1/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.393, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.840, Training Time: 245 seconds)
Epoch:   1/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.350, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.758, Training Time: 303 seconds)
Epoch:   1/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.327, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.747, Training Time: 311 seconds)
Epoch:   1/100, Validation loss:  7.303, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  1.000 to  0.998
Training loss improved!
Validation loss improved!
Epoch:   2/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.374, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.374, Training Time: 42 seconds)
Epoch:   2/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.259, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.317, Training Time: 88 seconds)
Epoch:   2/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.233, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.289, Training Time: 136 seconds)
Epoch:   2/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.223, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.272, Training Time: 187 seconds)
Epoch:   2/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.213, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.260, Training Time: 242 seconds)
Epoch:   2/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.192, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.249, Training Time: 300 seconds)
Epoch:   2/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.181, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.247, Training Time: 308 seconds)
Epoch:   2/100, Validation loss:  7.213, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.998 to  0.995
Training loss improved!
Validation loss improved!
Epoch:   3/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.253, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.253, Training Time: 42 seconds)
Epoch:   3/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.158, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.206, Training Time: 88 seconds)
Epoch:   3/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.146, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.186, Training Time: 136 seconds)
Epoch:   3/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.151, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.177, Training Time: 187 seconds)
Epoch:   3/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.143, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.170, Training Time: 242 seconds)
Epoch:   3/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.137, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.165, Training Time: 300 seconds)
Epoch:   3/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.147, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.164, Training Time: 308 seconds)
Epoch:   3/100, Validation loss:  7.153, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.995 to  0.993
Training loss improved!
Validation loss improved!
Epoch:   4/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.150, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.150, Training Time: 42 seconds)
Epoch:   4/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.096, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.123, Training Time: 87 seconds)
Epoch:   4/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.102, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.116, Training Time: 136 seconds)
Epoch:   4/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.115, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.116, Training Time: 187 seconds)
Epoch:   4/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.107, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.114, Training Time: 242 seconds)
Epoch:   4/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.098, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.111, Training Time: 300 seconds)
Epoch:   4/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.115, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.111, Training Time: 308 seconds)
Epoch:   4/100, Validation loss:  7.130, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.993 to  0.990
Training loss improved!
Validation loss improved!
Epoch:   5/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.130, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.130, Training Time: 42 seconds)
Epoch:   5/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.066, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.098, Training Time: 88 seconds)
Epoch:   5/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.073, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.090, Training Time: 136 seconds)
Epoch:   5/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.088, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.089, Training Time: 187 seconds)
Epoch:   5/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.087, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.089, Training Time: 242 seconds)
Epoch:   5/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.069, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.085, Training Time: 300 seconds)
Epoch:   5/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.110, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.086, Training Time: 309 seconds)
Epoch:   5/100, Validation loss:  7.117, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.990 to  0.988
Training loss improved!
Validation loss improved!
Epoch:   6/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.100, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.100, Training Time: 42 seconds)
Epoch:   6/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.045, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.072, Training Time: 88 seconds)
Epoch:   6/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.053, Training Time: 47 seconds), Stats for epoch: (Training Loss:  7.066, Training Time: 136 seconds)
Epoch:   6/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.072, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.067, Training Time: 187 seconds)
Epoch:   6/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.069, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.068, Training Time: 242 seconds)
Epoch:   6/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.060, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.066, Training Time: 300 seconds)
Epoch:   6/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.072, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.066, Training Time: 308 seconds)
Epoch:   6/100, Validation loss:  7.128, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.988 to  0.985
Training loss improved!
Epoch:   7/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.074, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.074, Training Time: 42 seconds)
Epoch:   7/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.029, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.052, Training Time: 88 seconds)
Epoch:   7/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.041, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.048, Training Time: 136 seconds)
Epoch:   7/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.059, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.051, Training Time: 187 seconds)
Epoch:   7/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.055, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.052, Training Time: 242 seconds)
Epoch:   7/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.048, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.051, Training Time: 300 seconds)
Epoch:   7/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.064, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.051, Training Time: 309 seconds)
Epoch:   7/100, Validation loss:  7.111, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.985 to  0.983
Training loss improved!
Validation loss improved!
Epoch:   8/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.054, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.054, Training Time: 42 seconds)
Epoch:   8/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.017, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.036, Training Time: 88 seconds)
Epoch:   8/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.026, Training Time: 47 seconds), Stats for epoch: (Training Loss:  7.032, Training Time: 136 seconds)
Epoch:   8/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.049, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.037, Training Time: 187 seconds)
Epoch:   8/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.044, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.038, Training Time: 242 seconds)
Epoch:   8/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.037, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.038, Training Time: 300 seconds)
Epoch:   8/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.065, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.038, Training Time: 309 seconds)
Epoch:   8/100, Validation loss:  7.101, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.983 to  0.980
Training loss improved!
Validation loss improved!
Epoch:   9/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.032, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.032, Training Time: 42 seconds)
Epoch:   9/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.009, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.021, Training Time: 88 seconds)
Epoch:   9/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.017, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.020, Training Time: 136 seconds)
Epoch:   9/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.039, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.024, Training Time: 187 seconds)
Epoch:   9/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.036, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.027, Training Time: 242 seconds)
Epoch:   9/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.031, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.027, Training Time: 300 seconds)
Epoch:   9/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.049, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.028, Training Time: 308 seconds)
Epoch:   9/100, Validation loss:  7.098, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.980 to  0.978
Training loss improved!
Validation loss improved!
Epoch:  10/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.023, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.023, Training Time: 42 seconds)
Epoch:  10/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.998, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.010, Training Time: 88 seconds)
Epoch:  10/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.008, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.010, Training Time: 136 seconds)
Epoch:  10/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.032, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.015, Training Time: 187 seconds)
Epoch:  10/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.027, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.018, Training Time: 242 seconds)
Epoch:  10/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.026, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.019, Training Time: 300 seconds)
Epoch:  10/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.045, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.020, Training Time: 309 seconds)
Epoch:  10/100, Validation loss:  7.096, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.978 to  0.975
Training loss improved!
Validation loss improved!
Epoch:  11/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.013, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.013, Training Time: 42 seconds)
Epoch:  11/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.987, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.000, Training Time: 88 seconds)
Epoch:  11/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.004, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.001, Training Time: 136 seconds)
Epoch:  11/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.026, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.007, Training Time: 187 seconds)
Epoch:  11/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.025, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.011, Training Time: 242 seconds)
Epoch:  11/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.019, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.012, Training Time: 300 seconds)
Epoch:  11/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.041, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.013, Training Time: 309 seconds)
Epoch:  11/100, Validation loss:  7.096, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.975 to  0.973
Training loss improved!
Validation loss improved!
Epoch:  12/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.007, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.007, Training Time: 42 seconds)
Epoch:  12/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.984, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.996, Training Time: 88 seconds)
Epoch:  12/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.996, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.996, Training Time: 136 seconds)
Epoch:  12/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.019, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.002, Training Time: 187 seconds)
Epoch:  12/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.018, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.005, Training Time: 242 seconds)
Epoch:  12/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.015, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.006, Training Time: 300 seconds)
Epoch:  12/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.045, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.007, Training Time: 308 seconds)
Epoch:  12/100, Validation loss:  7.076, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.973 to  0.970
Training loss improved!
Validation loss improved!
Epoch:  13/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.989, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.989, Training Time: 42 seconds)
Epoch:  13/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.975, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.982, Training Time: 88 seconds)
Epoch:  13/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.987, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.984, Training Time: 136 seconds)
Epoch:  13/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.012, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.991, Training Time: 187 seconds)
Epoch:  13/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.009, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.994, Training Time: 242 seconds)
Epoch:  13/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.021, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.999, Training Time: 300 seconds)
Epoch:  13/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.009, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.999, Training Time: 309 seconds)
Epoch:  13/100, Validation loss:  7.094, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.970 to  0.968
Training loss improved!
Epoch:  14/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.988, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.988, Training Time: 42 seconds)
Epoch:  14/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.971, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.980, Training Time: 88 seconds)
Epoch:  14/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.982, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.980, Training Time: 136 seconds)
Epoch:  14/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.004, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.986, Training Time: 187 seconds)
Epoch:  14/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.002, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.989, Training Time: 242 seconds)
Epoch:  14/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.017, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.994, Training Time: 300 seconds)
Epoch:  14/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.015, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.994, Training Time: 309 seconds)
Epoch:  14/100, Validation loss:  7.088, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.968 to  0.966
Training loss improved!
Epoch:  15/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.003, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.003, Training Time: 42 seconds)
Epoch:  15/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.975, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.989, Training Time: 88 seconds)
Epoch:  15/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.986, Training Time: 47 seconds), Stats for epoch: (Training Loss:  6.988, Training Time: 136 seconds)
Epoch:  15/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.011, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.994, Training Time: 187 seconds)
Epoch:  15/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.011, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.997, Training Time: 242 seconds)
Epoch:  15/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.007, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.999, Training Time: 300 seconds)
Epoch:  15/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.011, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.999, Training Time: 308 seconds)
Epoch:  15/100, Validation loss:  7.085, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.966 to  0.963
Epoch:  16/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.983, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.983, Training Time: 42 seconds)
Epoch:  16/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.966, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.975, Training Time: 88 seconds)
Epoch:  16/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.980, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.976, Training Time: 136 seconds)
Epoch:  16/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.003, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.983, Training Time: 187 seconds)
Epoch:  16/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.008, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.988, Training Time: 242 seconds)
Epoch:  16/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.002, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.990, Training Time: 300 seconds)
Epoch:  16/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.019, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.991, Training Time: 308 seconds)
Epoch:  16/100, Validation loss:  7.094, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.963 to  0.961
Training loss improved!
Epoch:  17/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.975, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.975, Training Time: 42 seconds)
Epoch:  17/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.960, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.967, Training Time: 88 seconds)
Epoch:  17/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.976, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.970, Training Time: 136 seconds)
Epoch:  17/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.997, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.977, Training Time: 187 seconds)
Epoch:  17/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.995, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.980, Training Time: 242 seconds)
Epoch:  17/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.010, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.985, Training Time: 300 seconds)
Epoch:  17/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.004, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.986, Training Time: 309 seconds)
Epoch:  17/100, Validation loss:  7.082, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.961 to  0.958
Training loss improved!
Epoch:  18/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.968, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.968, Training Time: 42 seconds)
Epoch:  18/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.955, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.962, Training Time: 88 seconds)
Epoch:  18/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.969, Training Time: 47 seconds), Stats for epoch: (Training Loss:  6.964, Training Time: 136 seconds)
Epoch:  18/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.993, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.971, Training Time: 187 seconds)
Epoch:  18/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.992, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.975, Training Time: 242 seconds)
Epoch:  18/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.981, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.976, Training Time: 300 seconds)
Epoch:  18/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.973, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.976, Training Time: 308 seconds)
Epoch:  18/100, Validation loss:  7.114, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.958 to  0.956
Training loss improved!
Epoch:  19/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.960, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.960, Training Time: 42 seconds)
Epoch:  19/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.955, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.958, Training Time: 88 seconds)
Epoch:  19/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.966, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.960, Training Time: 136 seconds)
Epoch:  19/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.991, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.968, Training Time: 187 seconds)
Epoch:  19/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.989, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.972, Training Time: 242 seconds)
Epoch:  19/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.979, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.973, Training Time: 300 seconds)
Epoch:  19/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.969, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.973, Training Time: 309 seconds)
Epoch:  19/100, Validation loss:  7.117, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.956 to  0.954
Training loss improved!
Epoch:  20/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.956, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.956, Training Time: 42 seconds)
Epoch:  20/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.952, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.954, Training Time: 88 seconds)
Epoch:  20/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.965, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.958, Training Time: 136 seconds)
Epoch:  20/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.988, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.965, Training Time: 187 seconds)
Epoch:  20/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.987, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.970, Training Time: 242 seconds)
Epoch:  20/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.976, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.971, Training Time: 300 seconds)
Epoch:  20/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.966, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.971, Training Time: 309 seconds)
Epoch:  20/100, Validation loss:  7.125, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.954 to  0.951
Training loss improved!
Epoch:  21/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.949, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.949, Training Time: 42 seconds)
Epoch:  21/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.948, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.948, Training Time: 88 seconds)
Epoch:  21/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.964, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.954, Training Time: 136 seconds)
Epoch:  21/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.986, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.962, Training Time: 187 seconds)
Epoch:  21/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.985, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.966, Training Time: 242 seconds)
Epoch:  21/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.973, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.968, Training Time: 300 seconds)
Epoch:  21/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.962, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.967, Training Time: 309 seconds)
Epoch:  21/100, Validation loss:  7.115, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.951 to  0.949
Training loss improved!
Epoch:  22/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.940, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.940, Training Time: 42 seconds)
Epoch:  22/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.940, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.940, Training Time: 88 seconds)
Epoch:  22/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.964, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.948, Training Time: 136 seconds)
Epoch:  22/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.984, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.957, Training Time: 187 seconds)
Epoch:  22/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.983, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.962, Training Time: 242 seconds)
Epoch:  22/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.971, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.964, Training Time: 300 seconds)
Epoch:  22/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.957, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.963, Training Time: 309 seconds)
Epoch:  22/100, Validation loss:  7.109, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.949 to  0.946
Training loss improved!
Epoch:  23/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.935, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.935, Training Time: 42 seconds)
Epoch:  23/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.937, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.936, Training Time: 88 seconds)
Epoch:  23/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.956, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.943, Training Time: 136 seconds)
Epoch:  23/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.982, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.952, Training Time: 187 seconds)
Epoch:  23/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.981, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.958, Training Time: 242 seconds)
Epoch:  23/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.967, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.960, Training Time: 300 seconds)
Epoch:  23/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.951, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.959, Training Time: 309 seconds)
Epoch:  23/100, Validation loss:  7.113, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.946 to  0.944
Training loss improved!
Epoch:  24/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.925, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.925, Training Time: 42 seconds)
Epoch:  24/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.933, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.929, Training Time: 88 seconds)
Epoch:  24/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.961, Training Time: 47 seconds), Stats for epoch: (Training Loss:  6.940, Training Time: 136 seconds)
Epoch:  24/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.980, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.950, Training Time: 187 seconds)
Epoch:  24/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.979, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.956, Training Time: 242 seconds)
Epoch:  24/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.965, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.957, Training Time: 300 seconds)
Epoch:  24/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.944, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.957, Training Time: 309 seconds)
Epoch:  24/100, Validation loss:  7.122, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.944 to  0.942
Training loss improved!
Epoch:  25/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.920, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.920, Training Time: 42 seconds)
Epoch:  25/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.947, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.934, Training Time: 88 seconds)
Epoch:  25/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.961, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.943, Training Time: 136 seconds)
Epoch:  25/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.980, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.952, Training Time: 187 seconds)
Epoch:  25/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.978, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.957, Training Time: 242 seconds)
Epoch:  25/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.962, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.958, Training Time: 300 seconds)
Epoch:  25/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.937, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.958, Training Time: 308 seconds)
Epoch:  25/100, Validation loss:  7.136, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.942 to  0.939
Epoch:  26/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.915, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.915, Training Time: 42 seconds)
Epoch:  26/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.940, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.928, Training Time: 88 seconds)
Epoch:  26/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.949, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.935, Training Time: 136 seconds)
Epoch:  26/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.976, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.945, Training Time: 187 seconds)
Epoch:  26/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.975, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.951, Training Time: 242 seconds)
Epoch:  26/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.959, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.952, Training Time: 300 seconds)
Epoch:  26/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.930, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.952, Training Time: 309 seconds)
Epoch:  26/100, Validation loss:  7.169, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.939 to  0.937
Training loss improved!
Epoch:  27/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.916, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.916, Training Time: 42 seconds)
Epoch:  27/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.925, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.921, Training Time: 88 seconds)
Epoch:  27/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.951, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.931, Training Time: 136 seconds)
Epoch:  27/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.973, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.941, Training Time: 187 seconds)
Epoch:  27/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.974, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.948, Training Time: 242 seconds)
Epoch:  27/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.956, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.949, Training Time: 301 seconds)
Epoch:  27/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.924, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.949, Training Time: 309 seconds)
Epoch:  27/100, Validation loss:  7.175, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.937 to  0.935
Training loss improved!
Epoch:  28/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.904, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.904, Training Time: 42 seconds)
Epoch:  28/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.923, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.913, Training Time: 88 seconds)
Epoch:  28/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.944, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.924, Training Time: 136 seconds)
Epoch:  28/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.971, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.936, Training Time: 187 seconds)
Epoch:  28/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.984, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.945, Training Time: 242 seconds)
Epoch:  28/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.977, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.950, Training Time: 300 seconds)
Epoch:  28/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.941, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.950, Training Time: 309 seconds)
Epoch:  28/100, Validation loss:  7.115, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.935 to  0.932
Epoch:  29/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.909, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.909, Training Time: 42 seconds)
Epoch:  29/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.925, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.917, Training Time: 88 seconds)
Epoch:  29/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.946, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.927, Training Time: 136 seconds)
Epoch:  29/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.970, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.937, Training Time: 187 seconds)
Epoch:  29/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.970, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.944, Training Time: 242 seconds)
Epoch:  29/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.952, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.945, Training Time: 300 seconds)
Epoch:  29/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.912, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.944, Training Time: 309 seconds)
Epoch:  29/100, Validation loss:  7.145, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.932 to  0.930
Training loss improved!
Epoch:  30/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.898, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.898, Training Time: 42 seconds)
Epoch:  30/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.917, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.908, Training Time: 88 seconds)
Epoch:  30/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.950, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.922, Training Time: 136 seconds)
Epoch:  30/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.967, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.933, Training Time: 187 seconds)
Epoch:  30/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.966, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.940, Training Time: 242 seconds)
Epoch:  30/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.954, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.942, Training Time: 300 seconds)
Epoch:  30/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.947, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.942, Training Time: 309 seconds)
Epoch:  30/100, Validation loss:  7.124, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.930 to  0.928
Training loss improved!
Epoch:  31/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.899, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.899, Training Time: 42 seconds)
Epoch:  31/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.914, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.907, Training Time: 88 seconds)
Epoch:  31/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.940, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.918, Training Time: 136 seconds)
Epoch:  31/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.964, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.930, Training Time: 187 seconds)
Epoch:  31/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.964, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.936, Training Time: 242 seconds)
Epoch:  31/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.971, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.942, Training Time: 300 seconds)
Epoch:  31/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.923, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.942, Training Time: 309 seconds)
Epoch:  31/100, Validation loss:  7.137, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.928 to  0.925
Training loss improved!
Epoch:  32/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.895, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.895, Training Time: 42 seconds)
Epoch:  32/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.912, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.903, Training Time: 88 seconds)
Epoch:  32/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.943, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.917, Training Time: 136 seconds)
Epoch:  32/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.968, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.930, Training Time: 187 seconds)
Epoch:  32/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.972, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.938, Training Time: 242 seconds)
Epoch:  32/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.972, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.944, Training Time: 300 seconds)
Epoch:  32/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.923, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.943, Training Time: 309 seconds)
Epoch:  32/100, Validation loss:  7.118, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.925 to  0.923
Epoch:  33/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.884, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.884, Training Time: 42 seconds)
Epoch:  33/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.909, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.896, Training Time: 88 seconds)
Epoch:  33/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.932, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.908, Training Time: 136 seconds)
Epoch:  33/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.960, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.921, Training Time: 187 seconds)
Epoch:  33/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.958, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.928, Training Time: 242 seconds)
Epoch:  33/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.936, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.930, Training Time: 300 seconds)
Epoch:  33/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.897, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.929, Training Time: 309 seconds)
Epoch:  33/100, Validation loss:  7.396, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.923 to  0.921
Training loss improved!
Epoch:  34/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.892, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.892, Training Time: 42 seconds)
Epoch:  34/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.906, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.899, Training Time: 88 seconds)
Epoch:  34/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.929, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.909, Training Time: 136 seconds)
Epoch:  34/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.957, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.921, Training Time: 187 seconds)
Epoch:  34/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.956, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.928, Training Time: 243 seconds)
Epoch:  34/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.932, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.929, Training Time: 301 seconds)
Epoch:  34/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.908, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.928, Training Time: 309 seconds)
Epoch:  34/100, Validation loss:  7.159, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.921 to  0.918
Training loss improved!
Epoch:  35/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.891, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.891, Training Time: 42 seconds)
Epoch:  35/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.901, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.896, Training Time: 88 seconds)
Epoch:  35/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.925, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.906, Training Time: 136 seconds)
Epoch:  35/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.954, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.918, Training Time: 187 seconds)
Epoch:  35/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.952, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.925, Training Time: 242 seconds)
Epoch:  35/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.927, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.925, Training Time: 300 seconds)
Epoch:  35/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.878, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.924, Training Time: 309 seconds)
Epoch:  35/100, Validation loss:  7.204, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.918 to  0.916
Training loss improved!
Epoch:  36/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.879, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.879, Training Time: 42 seconds)
Epoch:  36/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.910, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.895, Training Time: 88 seconds)
Epoch:  36/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.928, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.906, Training Time: 136 seconds)
Epoch:  36/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.953, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.918, Training Time: 187 seconds)
Epoch:  36/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.950, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.924, Training Time: 242 seconds)
Epoch:  36/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.950, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.929, Training Time: 300 seconds)
Epoch:  36/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.916, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.928, Training Time: 309 seconds)
Epoch:  36/100, Validation loss:  7.283, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.916 to  0.914
Epoch:  37/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.884, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.884, Training Time: 42 seconds)
Epoch:  37/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.910, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.897, Training Time: 88 seconds)
Epoch:  37/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.933, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.909, Training Time: 136 seconds)
Epoch:  37/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.964, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.923, Training Time: 187 seconds)
Epoch:  37/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.963, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.931, Training Time: 242 seconds)
Epoch:  37/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.943, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.933, Training Time: 300 seconds)
Epoch:  37/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.905, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.932, Training Time: 309 seconds)
Epoch:  37/100, Validation loss:  7.176, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.914 to  0.912
Epoch:  38/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.881, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.881, Training Time: 42 seconds)
Epoch:  38/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.898, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.890, Training Time: 88 seconds)
Epoch:  38/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.922, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.900, Training Time: 136 seconds)
Epoch:  38/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.950, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.913, Training Time: 187 seconds)
Epoch:  38/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.952, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.920, Training Time: 242 seconds)
Epoch:  38/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.933, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.923, Training Time: 300 seconds)
Epoch:  38/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.896, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.922, Training Time: 309 seconds)
Epoch:  38/100, Validation loss:  7.118, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.912 to  0.909
Training loss improved!
Epoch:  39/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.862, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.862, Training Time: 42 seconds)
Epoch:  39/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.889, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.876, Training Time: 88 seconds)
Epoch:  39/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.912, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.888, Training Time: 136 seconds)
Epoch:  39/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.941, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.901, Training Time: 187 seconds)
Epoch:  39/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.939, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.909, Training Time: 243 seconds)
Epoch:  39/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.914, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.910, Training Time: 300 seconds)
Epoch:  39/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.870, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.909, Training Time: 309 seconds)
Epoch:  39/100, Validation loss:  7.206, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.909 to  0.907
Training loss improved!
Epoch:  40/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.857, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.857, Training Time: 42 seconds)
Epoch:  40/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.876, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.867, Training Time: 88 seconds)
Epoch:  40/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.906, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.880, Training Time: 136 seconds)
Epoch:  40/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.933, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.893, Training Time: 187 seconds)
Epoch:  40/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.929, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.900, Training Time: 243 seconds)
Epoch:  40/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.912, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.902, Training Time: 301 seconds)
Epoch:  40/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.862, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.901, Training Time: 309 seconds)
Epoch:  40/100, Validation loss:  7.271, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.907 to  0.905
Training loss improved!
Epoch:  41/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.840, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.840, Training Time: 42 seconds)
Epoch:  41/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.858, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.849, Training Time: 88 seconds)
Epoch:  41/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.875, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.857, Training Time: 136 seconds)
Epoch:  41/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.902, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.869, Training Time: 187 seconds)
Epoch:  41/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.872, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.869, Training Time: 242 seconds)
Epoch:  41/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.868, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.869, Training Time: 300 seconds)
Epoch:  41/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.841, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.868, Training Time: 309 seconds)
Epoch:  41/100, Validation loss:  7.171, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.905 to  0.902
Training loss improved!
Epoch:  42/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.749, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.749, Training Time: 42 seconds)
Epoch:  42/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.740, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.745, Training Time: 88 seconds)
Epoch:  42/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.745, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.745, Training Time: 136 seconds)
Epoch:  42/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.766, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.750, Training Time: 187 seconds)
Epoch:  42/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.750, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.750, Training Time: 242 seconds)
Epoch:  42/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.721, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.745, Training Time: 300 seconds)
Epoch:  42/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.681, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.744, Training Time: 309 seconds)
Epoch:  42/100, Validation loss:  7.048, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.902 to  0.900
Training loss improved!
Validation loss improved!
Epoch:  43/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.640, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.640, Training Time: 42 seconds)
Epoch:  43/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.630, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.635, Training Time: 88 seconds)
Epoch:  43/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.645, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.638, Training Time: 136 seconds)
Epoch:  43/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.672, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.647, Training Time: 187 seconds)
Epoch:  43/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.663, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.650, Training Time: 242 seconds)
Epoch:  43/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.634, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.647, Training Time: 300 seconds)
Epoch:  43/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.601, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.646, Training Time: 309 seconds)
Epoch:  43/100, Validation loss:  7.037, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.900 to  0.898
Training loss improved!
Validation loss improved!
Epoch:  44/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.573, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.573, Training Time: 42 seconds)
Epoch:  44/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.566, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.569, Training Time: 88 seconds)
Epoch:  44/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.587, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.575, Training Time: 136 seconds)
Epoch:  44/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.612, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.584, Training Time: 187 seconds)
Epoch:  44/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.618, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.591, Training Time: 243 seconds)
Epoch:  44/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.577, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.589, Training Time: 301 seconds)
Epoch:  44/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.531, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.587, Training Time: 309 seconds)
Epoch:  44/100, Validation loss:  6.994, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.898 to  0.896
Training loss improved!
Validation loss improved!
Epoch:  45/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.532, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.532, Training Time: 42 seconds)
Epoch:  45/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.533, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.532, Training Time: 88 seconds)
Epoch:  45/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.553, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.539, Training Time: 136 seconds)
Epoch:  45/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.584, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.551, Training Time: 187 seconds)
Epoch:  45/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.588, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.558, Training Time: 243 seconds)
Epoch:  45/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.547, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.556, Training Time: 301 seconds)
Epoch:  45/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.513, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.555, Training Time: 309 seconds)
Epoch:  45/100, Validation loss:  6.849, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.896 to  0.893
Training loss improved!
Validation loss improved!
Epoch:  46/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.494, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.494, Training Time: 42 seconds)
Epoch:  46/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.491, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.492, Training Time: 88 seconds)
Epoch:  46/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.511, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.499, Training Time: 136 seconds)
Epoch:  46/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.545, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.510, Training Time: 187 seconds)
Epoch:  46/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.558, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.520, Training Time: 242 seconds)
Epoch:  46/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.522, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.520, Training Time: 300 seconds)
Epoch:  46/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.463, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.519, Training Time: 309 seconds)
Epoch:  46/100, Validation loss:  6.859, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.893 to  0.891
Training loss improved!
Epoch:  47/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.463, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.463, Training Time: 42 seconds)
Epoch:  47/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.455, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.459, Training Time: 88 seconds)
Epoch:  47/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.477, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.465, Training Time: 136 seconds)
Epoch:  47/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.513, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.477, Training Time: 187 seconds)
Epoch:  47/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.513, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.484, Training Time: 242 seconds)
Epoch:  47/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.485, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.484, Training Time: 300 seconds)
Epoch:  47/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.458, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.484, Training Time: 309 seconds)
Epoch:  47/100, Validation loss:  6.829, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.891 to  0.889
Training loss improved!
Validation loss improved!
Epoch:  48/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.433, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.433, Training Time: 42 seconds)
Epoch:  48/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.423, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.428, Training Time: 88 seconds)
Epoch:  48/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.452, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.436, Training Time: 136 seconds)
Epoch:  48/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.492, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.450, Training Time: 187 seconds)
Epoch:  48/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.491, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.458, Training Time: 242 seconds)
Epoch:  48/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.452, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.457, Training Time: 300 seconds)
Epoch:  48/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.412, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.456, Training Time: 309 seconds)
Epoch:  48/100, Validation loss:  6.760, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.889 to  0.887
Training loss improved!
Validation loss improved!
Epoch:  49/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.396, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.396, Training Time: 42 seconds)
Epoch:  49/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.388, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.392, Training Time: 88 seconds)
Epoch:  49/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.421, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.402, Training Time: 136 seconds)
Epoch:  49/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.456, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.415, Training Time: 187 seconds)
Epoch:  49/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.471, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.426, Training Time: 242 seconds)
Epoch:  49/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.427, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.426, Training Time: 300 seconds)
Epoch:  49/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.375, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.425, Training Time: 309 seconds)
Epoch:  49/100, Validation loss:  6.764, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.887 to  0.885
Training loss improved!
Epoch:  50/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.365, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.365, Training Time: 42 seconds)
Epoch:  50/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.365, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.365, Training Time: 88 seconds)
Epoch:  50/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.388, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.373, Training Time: 136 seconds)
Epoch:  50/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.431, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.387, Training Time: 187 seconds)
Epoch:  50/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.435, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.397, Training Time: 242 seconds)
Epoch:  50/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.405, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.398, Training Time: 300 seconds)
Epoch:  50/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.331, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.396, Training Time: 309 seconds)
Epoch:  50/100, Validation loss:  6.744, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.885 to  0.882
Training loss improved!
Validation loss improved!
Epoch:  51/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.335, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.335, Training Time: 42 seconds)
Epoch:  51/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.330, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.333, Training Time: 88 seconds)
Epoch:  51/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.359, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.341, Training Time: 136 seconds)
Epoch:  51/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.402, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.357, Training Time: 187 seconds)
Epoch:  51/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.401, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.366, Training Time: 242 seconds)
Epoch:  51/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.382, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.368, Training Time: 300 seconds)
Epoch:  51/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.302, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.367, Training Time: 309 seconds)
Epoch:  51/100, Validation loss:  6.792, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.882 to  0.880
Training loss improved!
Epoch:  52/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.308, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.308, Training Time: 42 seconds)
Epoch:  52/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.299, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.303, Training Time: 88 seconds)
Epoch:  52/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.331, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.312, Training Time: 136 seconds)
Epoch:  52/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.381, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.330, Training Time: 187 seconds)
Epoch:  52/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.389, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.341, Training Time: 243 seconds)
Epoch:  52/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.345, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.342, Training Time: 301 seconds)
Epoch:  52/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.332, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.342, Training Time: 309 seconds)
Epoch:  52/100, Validation loss:  6.864, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.880 to  0.878
Training loss improved!
Epoch:  53/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.272, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.272, Training Time: 42 seconds)
Epoch:  53/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.271, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.271, Training Time: 88 seconds)
Epoch:  53/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.303, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.282, Training Time: 136 seconds)
Epoch:  53/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.348, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.298, Training Time: 187 seconds)
Epoch:  53/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.348, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.308, Training Time: 243 seconds)
Epoch:  53/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.313, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.309, Training Time: 301 seconds)
Epoch:  53/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.281, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.308, Training Time: 309 seconds)
Epoch:  53/100, Validation loss:  6.825, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.878 to  0.876
Training loss improved!
Epoch:  54/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.246, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.246, Training Time: 42 seconds)
Epoch:  54/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.236, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.241, Training Time: 88 seconds)
Epoch:  54/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.274, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.252, Training Time: 136 seconds)
Epoch:  54/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.319, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.269, Training Time: 187 seconds)
Epoch:  54/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.326, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.280, Training Time: 243 seconds)
Epoch:  54/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.293, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.282, Training Time: 301 seconds)
Epoch:  54/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.237, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.281, Training Time: 309 seconds)
Epoch:  54/100, Validation loss:  6.834, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.876 to  0.874
Training loss improved!
Epoch:  55/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.218, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.218, Training Time: 42 seconds)
Epoch:  55/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.212, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.215, Training Time: 88 seconds)
Epoch:  55/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.255, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.229, Training Time: 136 seconds)
Epoch:  55/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.291, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.244, Training Time: 187 seconds)
Epoch:  55/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.299, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.255, Training Time: 242 seconds)
Epoch:  55/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.262, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.256, Training Time: 300 seconds)
Epoch:  55/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.226, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.256, Training Time: 309 seconds)
Epoch:  55/100, Validation loss:  6.617, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.874 to  0.871
Training loss improved!
Validation loss improved!
Epoch:  56/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.186, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.186, Training Time: 42 seconds)
Epoch:  56/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.170, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.178, Training Time: 88 seconds)
Epoch:  56/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.214, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.190, Training Time: 136 seconds)
Epoch:  56/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.260, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.208, Training Time: 187 seconds)
Epoch:  56/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.269, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.220, Training Time: 242 seconds)
Epoch:  56/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.226, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.221, Training Time: 300 seconds)
Epoch:  56/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.192, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.220, Training Time: 309 seconds)
Epoch:  56/100, Validation loss:  6.754, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.871 to  0.869
Training loss improved!
Epoch:  57/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.151, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.151, Training Time: 42 seconds)
Epoch:  57/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.133, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.142, Training Time: 88 seconds)
Epoch:  57/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.181, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.155, Training Time: 136 seconds)
Epoch:  57/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.223, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.172, Training Time: 187 seconds)
Epoch:  57/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.242, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.186, Training Time: 243 seconds)
Epoch:  57/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.247, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.196, Training Time: 301 seconds)
Epoch:  57/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.178, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.196, Training Time: 309 seconds)
Epoch:  57/100, Validation loss:  6.729, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.869 to  0.867
Training loss improved!
Epoch:  58/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.116, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.116, Training Time: 42 seconds)
Epoch:  58/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.101, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.109, Training Time: 88 seconds)
Epoch:  58/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.158, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.125, Training Time: 136 seconds)
Epoch:  58/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.199, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.144, Training Time: 187 seconds)
Epoch:  58/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.210, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.157, Training Time: 243 seconds)
Epoch:  58/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.171, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.159, Training Time: 301 seconds)
Epoch:  58/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.153, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.159, Training Time: 309 seconds)
Epoch:  58/100, Validation loss:  6.773, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.867 to  0.865
Training loss improved!
Epoch:  59/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.083, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.083, Training Time: 42 seconds)
Epoch:  59/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.055, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.069, Training Time: 88 seconds)
Epoch:  59/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.112, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.083, Training Time: 136 seconds)
Epoch:  59/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.156, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.102, Training Time: 187 seconds)
Epoch:  59/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.172, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.116, Training Time: 243 seconds)
Epoch:  59/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.135, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.119, Training Time: 301 seconds)
Epoch:  59/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.076, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.118, Training Time: 309 seconds)
Epoch:  59/100, Validation loss:  6.653, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.865 to  0.863
Training loss improved!
Epoch:  60/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.042, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.042, Training Time: 42 seconds)
Epoch:  60/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.013, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.028, Training Time: 88 seconds)
Epoch:  60/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.063, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.039, Training Time: 136 seconds)
Epoch:  60/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.119, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.059, Training Time: 187 seconds)
Epoch:  60/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.121, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.072, Training Time: 242 seconds)
Epoch:  60/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.100, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.076, Training Time: 301 seconds)
Epoch:  60/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.057, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.076, Training Time: 309 seconds)
Epoch:  60/100, Validation loss:  6.903, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.863 to  0.861
Training loss improved!
Epoch:  61/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.001, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.001, Training Time: 42 seconds)
Epoch:  61/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.975, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.988, Training Time: 88 seconds)
Epoch:  61/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.024, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.000, Training Time: 136 seconds)
Epoch:  61/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.085, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.021, Training Time: 187 seconds)
Epoch:  61/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.092, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.035, Training Time: 243 seconds)
Epoch:  61/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.079, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.043, Training Time: 301 seconds)
Epoch:  61/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.013, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.042, Training Time: 309 seconds)
Epoch:  61/100, Validation loss:  6.749, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.861 to  0.858
Training loss improved!
Epoch:  62/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.969, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.969, Training Time: 42 seconds)
Epoch:  62/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.936, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.952, Training Time: 88 seconds)
Epoch:  62/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.987, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.964, Training Time: 136 seconds)
Epoch:  62/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.048, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.985, Training Time: 187 seconds)
Epoch:  62/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.067, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.001, Training Time: 243 seconds)
Epoch:  62/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.038, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.007, Training Time: 301 seconds)
Epoch:  62/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.977, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.007, Training Time: 309 seconds)
Epoch:  62/100, Validation loss:  7.114, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.858 to  0.856
Training loss improved!
Epoch:  63/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.927, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.927, Training Time: 42 seconds)
Epoch:  63/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.901, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.914, Training Time: 88 seconds)
Epoch:  63/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.969, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.932, Training Time: 136 seconds)
Epoch:  63/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.033, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.958, Training Time: 187 seconds)
Epoch:  63/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.039, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.974, Training Time: 243 seconds)
Epoch:  63/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.013, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.980, Training Time: 301 seconds)
Epoch:  63/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.006, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.981, Training Time: 309 seconds)
Epoch:  63/100, Validation loss:  6.921, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.856 to  0.854
Training loss improved!
Epoch:  64/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.878, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.878, Training Time: 42 seconds)
Epoch:  64/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.863, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.870, Training Time: 88 seconds)
Epoch:  64/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.914, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.885, Training Time: 136 seconds)
Epoch:  64/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.984, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.910, Training Time: 187 seconds)
Epoch:  64/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.993, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.926, Training Time: 243 seconds)
Epoch:  64/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.012, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.941, Training Time: 301 seconds)
Epoch:  64/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.956, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.941, Training Time: 309 seconds)
Epoch:  64/100, Validation loss:  7.166, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.854 to  0.852
Training loss improved!
Epoch:  65/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.846, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.846, Training Time: 42 seconds)
Epoch:  65/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.825, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.835, Training Time: 88 seconds)
Epoch:  65/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.890, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.854, Training Time: 136 seconds)
Epoch:  65/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.956, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.879, Training Time: 187 seconds)
Epoch:  65/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.965, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.896, Training Time: 242 seconds)
Epoch:  65/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.947, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.905, Training Time: 300 seconds)
Epoch:  65/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.912, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.905, Training Time: 309 seconds)
Epoch:  65/100, Validation loss:  6.543, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.852 to  0.850
Training loss improved!
Validation loss improved!
Epoch:  66/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.815, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.815, Training Time: 42 seconds)
Epoch:  66/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.781, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.798, Training Time: 88 seconds)
Epoch:  66/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.838, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.812, Training Time: 136 seconds)
Epoch:  66/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.912, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.837, Training Time: 187 seconds)
Epoch:  66/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.937, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.857, Training Time: 243 seconds)
Epoch:  66/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.918, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.867, Training Time: 301 seconds)
Epoch:  66/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.858, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.867, Training Time: 309 seconds)
Epoch:  66/100, Validation loss:  6.692, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.850 to  0.848
Training loss improved!
Epoch:  67/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.770, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.770, Training Time: 42 seconds)
Epoch:  67/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.738, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.754, Training Time: 88 seconds)
Epoch:  67/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.796, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.768, Training Time: 136 seconds)
Epoch:  67/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.876, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.795, Training Time: 187 seconds)
Epoch:  67/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.885, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.813, Training Time: 243 seconds)
Epoch:  67/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.896, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.827, Training Time: 301 seconds)
Epoch:  67/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.952, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.830, Training Time: 309 seconds)
Epoch:  67/100, Validation loss:  6.470, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.848 to  0.846
Training loss improved!
Validation loss improved!
Epoch:  68/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.725, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.725, Training Time: 42 seconds)
Epoch:  68/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.695, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.710, Training Time: 88 seconds)
Epoch:  68/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.755, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.725, Training Time: 136 seconds)
Epoch:  68/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.845, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.755, Training Time: 187 seconds)
Epoch:  68/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.859, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.776, Training Time: 242 seconds)
Epoch:  68/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.850, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.788, Training Time: 300 seconds)
Epoch:  68/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.786, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.788, Training Time: 309 seconds)
Epoch:  68/100, Validation loss:  6.626, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.846 to  0.843
Training loss improved!
Epoch:  69/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.683, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.683, Training Time: 42 seconds)
Epoch:  69/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.653, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.668, Training Time: 88 seconds)
Epoch:  69/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.715, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.684, Training Time: 136 seconds)
Epoch:  69/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.804, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.714, Training Time: 187 seconds)
Epoch:  69/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.824, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.736, Training Time: 242 seconds)
Epoch:  69/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.828, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.751, Training Time: 300 seconds)
Epoch:  69/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.797, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.752, Training Time: 309 seconds)
Epoch:  69/100, Validation loss:  6.454, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.843 to  0.841
Training loss improved!
Validation loss improved!
Epoch:  70/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.637, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.637, Training Time: 42 seconds)
Epoch:  70/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.615, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.626, Training Time: 88 seconds)
Epoch:  70/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.681, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.644, Training Time: 136 seconds)
Epoch:  70/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.768, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.675, Training Time: 187 seconds)
Epoch:  70/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.790, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.698, Training Time: 242 seconds)
Epoch:  70/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.789, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.713, Training Time: 300 seconds)
Epoch:  70/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.737, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.714, Training Time: 309 seconds)
Epoch:  70/100, Validation loss:  6.497, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.841 to  0.839
Training loss improved!
Epoch:  71/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.590, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.590, Training Time: 42 seconds)
Epoch:  71/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.568, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.579, Training Time: 88 seconds)
Epoch:  71/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.643, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.600, Training Time: 136 seconds)
Epoch:  71/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.731, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.633, Training Time: 187 seconds)
Epoch:  71/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.764, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.659, Training Time: 243 seconds)
Epoch:  71/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.755, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.675, Training Time: 301 seconds)
Epoch:  71/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.691, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.676, Training Time: 309 seconds)
Epoch:  71/100, Validation loss:  6.455, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.839 to  0.837
Training loss improved!
Epoch:  72/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.537, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.537, Training Time: 42 seconds)
Epoch:  72/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.530, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.534, Training Time: 88 seconds)
Epoch:  72/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.591, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.553, Training Time: 136 seconds)
Epoch:  72/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.697, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.589, Training Time: 187 seconds)
Epoch:  72/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.717, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.614, Training Time: 243 seconds)
Epoch:  72/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.742, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.636, Training Time: 301 seconds)
Epoch:  72/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.719, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.638, Training Time: 309 seconds)
Epoch:  72/100, Validation loss:  6.764, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.837 to  0.835
Training loss improved!
Epoch:  73/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.506, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.506, Training Time: 42 seconds)
Epoch:  73/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.485, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.495, Training Time: 88 seconds)
Epoch:  73/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.560, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.517, Training Time: 136 seconds)
Epoch:  73/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.665, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.554, Training Time: 187 seconds)
Epoch:  73/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.687, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.581, Training Time: 243 seconds)
Epoch:  73/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.716, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.603, Training Time: 300 seconds)
Epoch:  73/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.627, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.604, Training Time: 308 seconds)
Epoch:  73/100, Validation loss:  6.458, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.835 to  0.833
Training loss improved!
Epoch:  74/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.454, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.454, Training Time: 42 seconds)
Epoch:  74/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.438, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.446, Training Time: 87 seconds)
Epoch:  74/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.516, Training Time: 47 seconds), Stats for epoch: (Training Loss:  5.470, Training Time: 135 seconds)
Epoch:  74/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.628, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.509, Training Time: 186 seconds)
Epoch:  74/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.653, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.538, Training Time: 242 seconds)
Epoch:  74/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.666, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.559, Training Time: 299 seconds)
Epoch:  74/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.605, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.560, Training Time: 308 seconds)
Epoch:  74/100, Validation loss:  6.444, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.833 to  0.831
Training loss improved!
Validation loss improved!
Epoch:  75/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.408, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.408, Training Time: 42 seconds)
Epoch:  75/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.392, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.400, Training Time: 87 seconds)
Epoch:  75/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.475, Training Time: 47 seconds), Stats for epoch: (Training Loss:  5.425, Training Time: 135 seconds)
Epoch:  75/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.591, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.466, Training Time: 186 seconds)
Epoch:  75/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.626, Training Time: 54 seconds), Stats for epoch: (Training Loss:  5.498, Training Time: 240 seconds)
Epoch:  75/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.639, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.522, Training Time: 298 seconds)
Epoch:  75/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.628, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.524, Training Time: 306 seconds)
Epoch:  75/100, Validation loss:  6.621, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.831 to  0.829
Training loss improved!
Epoch:  76/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.357, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.357, Training Time: 42 seconds)
Epoch:  76/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.348, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.353, Training Time: 87 seconds)
Epoch:  76/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.438, Training Time: 47 seconds), Stats for epoch: (Training Loss:  5.381, Training Time: 135 seconds)
Epoch:  76/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.572, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.429, Training Time: 185 seconds)
Epoch:  76/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.592, Training Time: 54 seconds), Stats for epoch: (Training Loss:  5.461, Training Time: 240 seconds)
Epoch:  76/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.619, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.488, Training Time: 298 seconds)
Epoch:  76/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.549, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.489, Training Time: 306 seconds)
Epoch:  76/100, Validation loss:  6.365, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.829 to  0.827
Training loss improved!
Validation loss improved!
Epoch:  77/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.304, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.304, Training Time: 42 seconds)
Epoch:  77/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.305, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.304, Training Time: 87 seconds)
Epoch:  77/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.394, Training Time: 47 seconds), Stats for epoch: (Training Loss:  5.334, Training Time: 135 seconds)
Epoch:  77/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.525, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.382, Training Time: 185 seconds)
Epoch:  77/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.564, Training Time: 54 seconds), Stats for epoch: (Training Loss:  5.418, Training Time: 240 seconds)
Epoch:  77/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.573, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.444, Training Time: 298 seconds)
Epoch:  77/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.491, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.445, Training Time: 306 seconds)
Epoch:  77/100, Validation loss:  6.405, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.827 to  0.825
Training loss improved!
Epoch:  78/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.259, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.259, Training Time: 42 seconds)
Epoch:  78/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.259, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.259, Training Time: 87 seconds)
Epoch:  78/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.352, Training Time: 47 seconds), Stats for epoch: (Training Loss:  5.290, Training Time: 135 seconds)
Epoch:  78/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.489, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.340, Training Time: 186 seconds)
Epoch:  78/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.521, Training Time: 54 seconds), Stats for epoch: (Training Loss:  5.376, Training Time: 240 seconds)
Epoch:  78/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.555, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.406, Training Time: 298 seconds)
Epoch:  78/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.525, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.409, Training Time: 306 seconds)
Epoch:  78/100, Validation loss:  6.344, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.825 to  0.823
Training loss improved!
Validation loss improved!
Epoch:  79/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.213, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.213, Training Time: 42 seconds)
Epoch:  79/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.211, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.212, Training Time: 87 seconds)
Epoch:  79/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.317, Training Time: 47 seconds), Stats for epoch: (Training Loss:  5.247, Training Time: 135 seconds)
Epoch:  79/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.457, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.299, Training Time: 186 seconds)
Epoch:  79/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.492, Training Time: 54 seconds), Stats for epoch: (Training Loss:  5.338, Training Time: 241 seconds)
Epoch:  79/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.528, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.370, Training Time: 298 seconds)
Epoch:  79/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.423, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.371, Training Time: 306 seconds)
Epoch:  79/100, Validation loss:  6.242, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.823 to  0.821
Training loss improved!
Validation loss improved!
Epoch:  80/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.158, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.158, Training Time: 42 seconds)
Epoch:  80/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.170, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.164, Training Time: 87 seconds)
Epoch:  80/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.265, Training Time: 47 seconds), Stats for epoch: (Training Loss:  5.197, Training Time: 135 seconds)
Epoch:  80/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.420, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.253, Training Time: 185 seconds)
Epoch:  80/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.460, Training Time: 54 seconds), Stats for epoch: (Training Loss:  5.294, Training Time: 240 seconds)
Epoch:  80/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.494, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.328, Training Time: 298 seconds)
Epoch:  80/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.398, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.329, Training Time: 306 seconds)
Epoch:  80/100, Validation loss:  6.430, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.821 to  0.819
Training loss improved!
Epoch:  81/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.110, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.110, Training Time: 42 seconds)
Epoch:  81/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.126, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.118, Training Time: 87 seconds)
Epoch:  81/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.223, Training Time: 47 seconds), Stats for epoch: (Training Loss:  5.153, Training Time: 135 seconds)
Epoch:  81/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.379, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.209, Training Time: 186 seconds)
Epoch:  81/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.434, Training Time: 54 seconds), Stats for epoch: (Training Loss:  5.254, Training Time: 240 seconds)
Epoch:  81/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.457, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.288, Training Time: 298 seconds)
Epoch:  81/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.380, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.290, Training Time: 306 seconds)
Epoch:  81/100, Validation loss:  6.318, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.819 to  0.816
Training loss improved!
Epoch:  82/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.056, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.056, Training Time: 42 seconds)
Epoch:  82/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.084, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.070, Training Time: 87 seconds)
Epoch:  82/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.182, Training Time: 47 seconds), Stats for epoch: (Training Loss:  5.107, Training Time: 135 seconds)
Epoch:  82/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.343, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.166, Training Time: 185 seconds)
Epoch:  82/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.401, Training Time: 54 seconds), Stats for epoch: (Training Loss:  5.213, Training Time: 240 seconds)
Epoch:  82/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.421, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.248, Training Time: 298 seconds)
Epoch:  82/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.372, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.251, Training Time: 306 seconds)
Epoch:  82/100, Validation loss:  6.318, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.816 to  0.814
Training loss improved!
Epoch:  83/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.996, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.996, Training Time: 42 seconds)
Epoch:  83/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.024, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.010, Training Time: 87 seconds)
Epoch:  83/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.138, Training Time: 47 seconds), Stats for epoch: (Training Loss:  5.053, Training Time: 135 seconds)
Epoch:  83/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.302, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.115, Training Time: 185 seconds)
Epoch:  83/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.358, Training Time: 54 seconds), Stats for epoch: (Training Loss:  5.164, Training Time: 240 seconds)
Epoch:  83/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.411, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.205, Training Time: 298 seconds)
Epoch:  83/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.353, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.208, Training Time: 306 seconds)
Epoch:  83/100, Validation loss:  6.266, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.814 to  0.812
Training loss improved!
Epoch:  84/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.953, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.953, Training Time: 42 seconds)
Epoch:  84/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.980, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.966, Training Time: 87 seconds)
Epoch:  84/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.101, Training Time: 47 seconds), Stats for epoch: (Training Loss:  5.011, Training Time: 135 seconds)
Epoch:  84/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.268, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.075, Training Time: 185 seconds)
Epoch:  84/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.326, Training Time: 54 seconds), Stats for epoch: (Training Loss:  5.126, Training Time: 240 seconds)
Epoch:  84/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.372, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.167, Training Time: 298 seconds)
Epoch:  84/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.273, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.169, Training Time: 306 seconds)
Epoch:  84/100, Validation loss:  6.462, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.812 to  0.810
Training loss improved!
Epoch:  85/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.902, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.902, Training Time: 42 seconds)
Epoch:  85/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.934, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.918, Training Time: 87 seconds)
Epoch:  85/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.054, Training Time: 47 seconds), Stats for epoch: (Training Loss:  4.963, Training Time: 135 seconds)
Epoch:  85/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.239, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.032, Training Time: 186 seconds)
Epoch:  85/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.297, Training Time: 54 seconds), Stats for epoch: (Training Loss:  5.085, Training Time: 241 seconds)
Epoch:  85/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.336, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.127, Training Time: 298 seconds)
Epoch:  85/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.246, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.130, Training Time: 306 seconds)
Epoch:  85/100, Validation loss:  6.343, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.810 to  0.808
Training loss improved!
Epoch:  86/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.848, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.848, Training Time: 42 seconds)
Epoch:  86/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.886, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.867, Training Time: 87 seconds)
Epoch:  86/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.011, Training Time: 47 seconds), Stats for epoch: (Training Loss:  4.915, Training Time: 135 seconds)
Epoch:  86/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.195, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.985, Training Time: 185 seconds)
Epoch:  86/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.257, Training Time: 54 seconds), Stats for epoch: (Training Loss:  5.040, Training Time: 240 seconds)
Epoch:  86/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.310, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.085, Training Time: 298 seconds)
Epoch:  86/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.243, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.089, Training Time: 306 seconds)
Epoch:  86/100, Validation loss:  6.633, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.808 to  0.806
Training loss improved!
Epoch:  87/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.794, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.794, Training Time: 42 seconds)
Epoch:  87/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.842, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.818, Training Time: 87 seconds)
Epoch:  87/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.961, Training Time: 47 seconds), Stats for epoch: (Training Loss:  4.866, Training Time: 135 seconds)
Epoch:  87/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.167, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.941, Training Time: 186 seconds)
Epoch:  87/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.231, Training Time: 54 seconds), Stats for epoch: (Training Loss:  4.999, Training Time: 240 seconds)
Epoch:  87/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.296, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.049, Training Time: 298 seconds)
Epoch:  87/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.181, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.052, Training Time: 306 seconds)
Epoch:  87/100, Validation loss:  6.392, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.806 to  0.804
Training loss improved!
Epoch:  88/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.735, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.735, Training Time: 42 seconds)
Epoch:  88/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.791, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.763, Training Time: 87 seconds)
Epoch:  88/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.915, Training Time: 47 seconds), Stats for epoch: (Training Loss:  4.814, Training Time: 135 seconds)
Epoch:  88/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.130, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.893, Training Time: 186 seconds)
Epoch:  88/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.190, Training Time: 54 seconds), Stats for epoch: (Training Loss:  4.952, Training Time: 240 seconds)
Epoch:  88/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.254, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.003, Training Time: 298 seconds)
Epoch:  88/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.160, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.006, Training Time: 306 seconds)
Epoch:  88/100, Validation loss:  6.421, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.804 to  0.802
Training loss improved!
Epoch:  89/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.693, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.693, Training Time: 42 seconds)
Epoch:  89/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.737, Training Time: 44 seconds), Stats for epoch: (Training Loss:  4.715, Training Time: 87 seconds)
Epoch:  89/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.872, Training Time: 47 seconds), Stats for epoch: (Training Loss:  4.767, Training Time: 135 seconds)
Epoch:  89/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.084, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.847, Training Time: 185 seconds)
Epoch:  89/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.154, Training Time: 54 seconds), Stats for epoch: (Training Loss:  4.908, Training Time: 240 seconds)
Epoch:  89/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.220, Training Time: 57 seconds), Stats for epoch: (Training Loss:  4.960, Training Time: 298 seconds)
Epoch:  89/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.124, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.964, Training Time: 306 seconds)
Epoch:  89/100, Validation loss:  6.443, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.802 to  0.800
Training loss improved!
Epoch:  90/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.634, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.634, Training Time: 42 seconds)
Epoch:  90/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.685, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.660, Training Time: 87 seconds)
Epoch:  90/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.828, Training Time: 47 seconds), Stats for epoch: (Training Loss:  4.716, Training Time: 135 seconds)
Epoch:  90/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.043, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.797, Training Time: 185 seconds)
Epoch:  90/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.128, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.863, Training Time: 240 seconds)
Epoch:  90/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.200, Training Time: 57 seconds), Stats for epoch: (Training Loss:  4.919, Training Time: 298 seconds)
Epoch:  90/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.195, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.926, Training Time: 306 seconds)
Epoch:  90/100, Validation loss:  6.398, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.800 to  0.798
Training loss improved!
Epoch:  91/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.581, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.581, Training Time: 42 seconds)
Epoch:  91/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.638, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.610, Training Time: 87 seconds)
Epoch:  91/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.775, Training Time: 47 seconds), Stats for epoch: (Training Loss:  4.665, Training Time: 135 seconds)
Epoch:  91/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.005, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.750, Training Time: 185 seconds)
Epoch:  91/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.092, Training Time: 54 seconds), Stats for epoch: (Training Loss:  4.818, Training Time: 240 seconds)
Epoch:  91/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.156, Training Time: 57 seconds), Stats for epoch: (Training Loss:  4.874, Training Time: 298 seconds)
Epoch:  91/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.096, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.880, Training Time: 306 seconds)
Epoch:  91/100, Validation loss:  6.564, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.798 to  0.796
Training loss improved!
Epoch:  92/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.528, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.528, Training Time: 42 seconds)
Epoch:  92/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.604, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.566, Training Time: 87 seconds)
Epoch:  92/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.736, Training Time: 47 seconds), Stats for epoch: (Training Loss:  4.623, Training Time: 135 seconds)
Epoch:  92/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.964, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.708, Training Time: 186 seconds)
Epoch:  92/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.055, Training Time: 54 seconds), Stats for epoch: (Training Loss:  4.777, Training Time: 240 seconds)
Epoch:  92/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.130, Training Time: 57 seconds), Stats for epoch: (Training Loss:  4.836, Training Time: 298 seconds)
Epoch:  92/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.048, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.841, Training Time: 306 seconds)
Epoch:  92/100, Validation loss:  6.423, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.796 to  0.794
Training loss improved!
Epoch:  93/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.474, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.474, Training Time: 42 seconds)
Epoch:  93/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.555, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.515, Training Time: 87 seconds)
Epoch:  93/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.695, Training Time: 47 seconds), Stats for epoch: (Training Loss:  4.575, Training Time: 135 seconds)
Epoch:  93/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.920, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.661, Training Time: 186 seconds)
Epoch:  93/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.024, Training Time: 54 seconds), Stats for epoch: (Training Loss:  4.734, Training Time: 241 seconds)
Epoch:  93/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.095, Training Time: 57 seconds), Stats for epoch: (Training Loss:  4.794, Training Time: 298 seconds)
Epoch:  93/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.060, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.800, Training Time: 306 seconds)
Epoch:  93/100, Validation loss:  6.504, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.794 to  0.792
Training loss improved!
Epoch:  94/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.430, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.430, Training Time: 42 seconds)
Epoch:  94/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.502, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.466, Training Time: 87 seconds)
Epoch:  94/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.650, Training Time: 47 seconds), Stats for epoch: (Training Loss:  4.528, Training Time: 135 seconds)
Epoch:  94/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.884, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.617, Training Time: 186 seconds)
Epoch:  94/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.981, Training Time: 54 seconds), Stats for epoch: (Training Loss:  4.690, Training Time: 240 seconds)
Epoch:  94/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.077, Training Time: 57 seconds), Stats for epoch: (Training Loss:  4.754, Training Time: 298 seconds)
Epoch:  94/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.999, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.760, Training Time: 306 seconds)
Epoch:  94/100, Validation loss:  6.418, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.792 to  0.790
Training loss improved!
Epoch:  95/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.356, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.356, Training Time: 42 seconds)
Epoch:  95/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.455, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.406, Training Time: 87 seconds)
Epoch:  95/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.602, Training Time: 47 seconds), Stats for epoch: (Training Loss:  4.471, Training Time: 135 seconds)
Epoch:  95/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.843, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.564, Training Time: 186 seconds)
Epoch:  95/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.947, Training Time: 54 seconds), Stats for epoch: (Training Loss:  4.641, Training Time: 241 seconds)
Epoch:  95/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.043, Training Time: 57 seconds), Stats for epoch: (Training Loss:  4.708, Training Time: 298 seconds)
Epoch:  95/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.940, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.713, Training Time: 306 seconds)
Epoch:  95/100, Validation loss:  6.470, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.790 to  0.788
Training loss improved!
Epoch:  96/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.311, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.311, Training Time: 42 seconds)
Epoch:  96/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.409, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.360, Training Time: 87 seconds)
Epoch:  96/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.553, Training Time: 47 seconds), Stats for epoch: (Training Loss:  4.424, Training Time: 135 seconds)
Epoch:  96/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.806, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.520, Training Time: 185 seconds)
Epoch:  96/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.919, Training Time: 54 seconds), Stats for epoch: (Training Loss:  4.600, Training Time: 240 seconds)
Epoch:  96/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.011, Training Time: 57 seconds), Stats for epoch: (Training Loss:  4.668, Training Time: 298 seconds)
Epoch:  96/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.951, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.675, Training Time: 306 seconds)
Epoch:  96/100, Validation loss:  6.404, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.788 to  0.786
Training loss improved!
Epoch:  97/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.255, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.255, Training Time: 42 seconds)
Epoch:  97/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.350, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.303, Training Time: 87 seconds)
Epoch:  97/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.510, Training Time: 47 seconds), Stats for epoch: (Training Loss:  4.372, Training Time: 135 seconds)
Epoch:  97/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.770, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.471, Training Time: 186 seconds)
Epoch:  97/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.881, Training Time: 54 seconds), Stats for epoch: (Training Loss:  4.553, Training Time: 241 seconds)
Epoch:  97/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  4.979, Training Time: 57 seconds), Stats for epoch: (Training Loss:  4.624, Training Time: 298 seconds)
Epoch:  97/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.886, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.631, Training Time: 306 seconds)
Epoch:  97/100, Validation loss:  6.456, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.786 to  0.784
Training loss improved!
Epoch:  98/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.205, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.205, Training Time: 42 seconds)
Epoch:  98/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.307, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.256, Training Time: 87 seconds)
Epoch:  98/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.464, Training Time: 47 seconds), Stats for epoch: (Training Loss:  4.325, Training Time: 135 seconds)
Epoch:  98/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.728, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.426, Training Time: 186 seconds)
Epoch:  98/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.839, Training Time: 54 seconds), Stats for epoch: (Training Loss:  4.508, Training Time: 241 seconds)
Epoch:  98/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  4.951, Training Time: 57 seconds), Stats for epoch: (Training Loss:  4.582, Training Time: 298 seconds)
Epoch:  98/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.866, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.589, Training Time: 306 seconds)
Epoch:  98/100, Validation loss:  6.540, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.784 to  0.782
Training loss improved!
Epoch:  99/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.143, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.143, Training Time: 42 seconds)
Epoch:  99/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.259, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.201, Training Time: 87 seconds)
Epoch:  99/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.433, Training Time: 47 seconds), Stats for epoch: (Training Loss:  4.278, Training Time: 135 seconds)
Epoch:  99/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.683, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.380, Training Time: 186 seconds)
Epoch:  99/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.809, Training Time: 54 seconds), Stats for epoch: (Training Loss:  4.465, Training Time: 240 seconds)
Epoch:  99/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  4.925, Training Time: 57 seconds), Stats for epoch: (Training Loss:  4.542, Training Time: 298 seconds)
Epoch:  99/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.869, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.550, Training Time: 306 seconds)
Epoch:  99/100, Validation loss:  6.446, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.782 to  0.781
Training loss improved!
Epoch: 100/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.095, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.095, Training Time: 42 seconds)
Epoch: 100/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.229, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.162, Training Time: 87 seconds)
Epoch: 100/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.383, Training Time: 47 seconds), Stats for epoch: (Training Loss:  4.236, Training Time: 135 seconds)
Epoch: 100/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.644, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.338, Training Time: 185 seconds)
Epoch: 100/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.772, Training Time: 54 seconds), Stats for epoch: (Training Loss:  4.425, Training Time: 240 seconds)
Epoch: 100/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  4.909, Training Time: 57 seconds), Stats for epoch: (Training Loss:  4.505, Training Time: 298 seconds)
Epoch: 100/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.837, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.514, Training Time: 306 seconds)
Epoch: 100/100, Validation loss:  6.508, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.781 to  0.779
Training loss improved!
Training Complete!
