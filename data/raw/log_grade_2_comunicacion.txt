
Reading dataset 'csv'...

Final shared vocab size: 68880

Splitting 98264 samples into training & validation sets (20.0% used for validation)...
Training set: 78612 samples. Validation set: 19652 samples.
Sorting training & validation sets to increase training efficiency...
Initializing model...


Creating checkpoint batch files...
Initializing training...
Epochs: 100
Batch Size: 128
Optimizer: sgd
Epoch:   1/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  9.043, Training Time: 45 seconds), Stats for epoch: (Training Loss:  9.043, Training Time: 45 seconds)
Epoch:   1/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.738, Training Time: 45 seconds), Stats for epoch: (Training Loss:  8.390, Training Time: 91 seconds)
Epoch:   1/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.548, Training Time: 48 seconds), Stats for epoch: (Training Loss:  8.109, Training Time: 139 seconds)
Epoch:   1/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.450, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.944, Training Time: 190 seconds)
Epoch:   1/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.406, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.837, Training Time: 245 seconds)
Epoch:   1/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.335, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.753, Training Time: 303 seconds)
Epoch:   1/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.290, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.742, Training Time: 311 seconds)
Epoch:   1/100, Validation loss:  7.360, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  1.000 to  0.998
Training loss improved!
Validation loss improved!
Epoch:   2/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.379, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.379, Training Time: 42 seconds)
Epoch:   2/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.234, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.306, Training Time: 88 seconds)
Epoch:   2/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.234, Training Time: 47 seconds), Stats for epoch: (Training Loss:  7.282, Training Time: 136 seconds)
Epoch:   2/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.220, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.267, Training Time: 186 seconds)
Epoch:   2/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.212, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.256, Training Time: 242 seconds)
Epoch:   2/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.187, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.244, Training Time: 300 seconds)
Epoch:   2/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.167, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.242, Training Time: 308 seconds)
Epoch:   2/100, Validation loss:  7.184, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.998 to  0.995
Training loss improved!
Validation loss improved!
Epoch:   3/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.252, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.252, Training Time: 42 seconds)
Epoch:   3/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.138, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.195, Training Time: 88 seconds)
Epoch:   3/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.134, Training Time: 47 seconds), Stats for epoch: (Training Loss:  7.175, Training Time: 135 seconds)
Epoch:   3/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.142, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.167, Training Time: 187 seconds)
Epoch:   3/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.148, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.163, Training Time: 242 seconds)
Epoch:   3/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.120, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.156, Training Time: 300 seconds)
Epoch:   3/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.123, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.155, Training Time: 308 seconds)
Epoch:   3/100, Validation loss:  7.154, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.995 to  0.993
Training loss improved!
Validation loss improved!
Epoch:   4/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.163, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.163, Training Time: 42 seconds)
Epoch:   4/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.090, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.126, Training Time: 88 seconds)
Epoch:   4/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.096, Training Time: 47 seconds), Stats for epoch: (Training Loss:  7.116, Training Time: 136 seconds)
Epoch:   4/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.102, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.113, Training Time: 187 seconds)
Epoch:   4/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.114, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.113, Training Time: 242 seconds)
Epoch:   4/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.086, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.108, Training Time: 300 seconds)
Epoch:   4/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.088, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.108, Training Time: 308 seconds)
Epoch:   4/100, Validation loss:  7.132, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.993 to  0.990
Training loss improved!
Validation loss improved!
Epoch:   5/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.110, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.110, Training Time: 42 seconds)
Epoch:   5/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.063, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.086, Training Time: 88 seconds)
Epoch:   5/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.077, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.083, Training Time: 136 seconds)
Epoch:   5/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.081, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.083, Training Time: 187 seconds)
Epoch:   5/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.087, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.084, Training Time: 242 seconds)
Epoch:   5/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.067, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.081, Training Time: 300 seconds)
Epoch:   5/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.070, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.081, Training Time: 308 seconds)
Epoch:   5/100, Validation loss:  7.123, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.990 to  0.988
Training loss improved!
Validation loss improved!
Epoch:   6/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.120, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.120, Training Time: 42 seconds)
Epoch:   6/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.033, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.076, Training Time: 87 seconds)
Epoch:   6/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.060, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.071, Training Time: 136 seconds)
Epoch:   6/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.063, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.069, Training Time: 187 seconds)
Epoch:   6/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.069, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.069, Training Time: 242 seconds)
Epoch:   6/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.056, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.067, Training Time: 300 seconds)
Epoch:   6/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.049, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.066, Training Time: 308 seconds)
Epoch:   6/100, Validation loss:  7.105, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.988 to  0.985
Training loss improved!
Validation loss improved!
Epoch:   7/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.097, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.097, Training Time: 42 seconds)
Epoch:   7/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.022, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.060, Training Time: 88 seconds)
Epoch:   7/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.044, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.055, Training Time: 136 seconds)
Epoch:   7/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.052, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.054, Training Time: 187 seconds)
Epoch:   7/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.055, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.054, Training Time: 242 seconds)
Epoch:   7/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.047, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.053, Training Time: 300 seconds)
Epoch:   7/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.038, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.053, Training Time: 308 seconds)
Epoch:   7/100, Validation loss:  7.100, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.985 to  0.983
Training loss improved!
Validation loss improved!
Epoch:   8/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.056, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.056, Training Time: 42 seconds)
Epoch:   8/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.025, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.040, Training Time: 88 seconds)
Epoch:   8/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.032, Training Time: 47 seconds), Stats for epoch: (Training Loss:  7.038, Training Time: 136 seconds)
Epoch:   8/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.043, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.039, Training Time: 187 seconds)
Epoch:   8/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.049, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.041, Training Time: 242 seconds)
Epoch:   8/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.035, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.040, Training Time: 300 seconds)
Epoch:   8/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.023, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.039, Training Time: 308 seconds)
Epoch:   8/100, Validation loss:  7.080, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.983 to  0.980
Training loss improved!
Validation loss improved!
Epoch:   9/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.048, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.048, Training Time: 42 seconds)
Epoch:   9/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  7.001, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.024, Training Time: 88 seconds)
Epoch:   9/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.023, Training Time: 47 seconds), Stats for epoch: (Training Loss:  7.024, Training Time: 136 seconds)
Epoch:   9/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.035, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.026, Training Time: 187 seconds)
Epoch:   9/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.042, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.030, Training Time: 242 seconds)
Epoch:   9/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.031, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.030, Training Time: 300 seconds)
Epoch:   9/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.020, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.030, Training Time: 308 seconds)
Epoch:   9/100, Validation loss:  7.096, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.980 to  0.978
Training loss improved!
Epoch:  10/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.052, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.052, Training Time: 42 seconds)
Epoch:  10/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.993, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.022, Training Time: 88 seconds)
Epoch:  10/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.015, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.020, Training Time: 136 seconds)
Epoch:  10/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.028, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.022, Training Time: 187 seconds)
Epoch:  10/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.035, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.024, Training Time: 242 seconds)
Epoch:  10/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.021, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.024, Training Time: 300 seconds)
Epoch:  10/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.030, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.024, Training Time: 308 seconds)
Epoch:  10/100, Validation loss:  7.085, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.978 to  0.975
Training loss improved!
Epoch:  11/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.015, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.015, Training Time: 42 seconds)
Epoch:  11/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.987, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.001, Training Time: 88 seconds)
Epoch:  11/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.008, Training Time: 47 seconds), Stats for epoch: (Training Loss:  7.003, Training Time: 136 seconds)
Epoch:  11/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.015, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.006, Training Time: 187 seconds)
Epoch:  11/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.028, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.011, Training Time: 242 seconds)
Epoch:  11/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.015, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.011, Training Time: 300 seconds)
Epoch:  11/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.012, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.011, Training Time: 308 seconds)
Epoch:  11/100, Validation loss:  7.092, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.975 to  0.973
Training loss improved!
Epoch:  12/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  7.021, Training Time: 42 seconds), Stats for epoch: (Training Loss:  7.021, Training Time: 42 seconds)
Epoch:  12/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.987, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.004, Training Time: 88 seconds)
Epoch:  12/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  7.000, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.002, Training Time: 136 seconds)
Epoch:  12/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.011, Training Time: 51 seconds), Stats for epoch: (Training Loss:  7.004, Training Time: 187 seconds)
Epoch:  12/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.025, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.009, Training Time: 242 seconds)
Epoch:  12/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.010, Training Time: 57 seconds), Stats for epoch: (Training Loss:  7.009, Training Time: 300 seconds)
Epoch:  12/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.007, Training Time: 8 seconds), Stats for epoch: (Training Loss:  7.009, Training Time: 308 seconds)
Epoch:  12/100, Validation loss:  7.094, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.973 to  0.970
Training loss improved!
Epoch:  13/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.987, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.987, Training Time: 42 seconds)
Epoch:  13/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.975, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.981, Training Time: 88 seconds)
Epoch:  13/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.991, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.985, Training Time: 136 seconds)
Epoch:  13/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.002, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.989, Training Time: 187 seconds)
Epoch:  13/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.010, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.993, Training Time: 242 seconds)
Epoch:  13/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.990, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.992, Training Time: 300 seconds)
Epoch:  13/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.969, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.992, Training Time: 308 seconds)
Epoch:  13/100, Validation loss:  7.151, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.970 to  0.968
Training loss improved!
Epoch:  14/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.996, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.996, Training Time: 42 seconds)
Epoch:  14/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.979, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.987, Training Time: 88 seconds)
Epoch:  14/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.988, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.988, Training Time: 136 seconds)
Epoch:  14/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.997, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.990, Training Time: 187 seconds)
Epoch:  14/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.021, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.996, Training Time: 242 seconds)
Epoch:  14/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.010, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.999, Training Time: 300 seconds)
Epoch:  14/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.988, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.998, Training Time: 309 seconds)
Epoch:  14/100, Validation loss:  7.080, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.968 to  0.966
Validation loss improved!
Epoch:  15/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.990, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.990, Training Time: 42 seconds)
Epoch:  15/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.969, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.980, Training Time: 88 seconds)
Epoch:  15/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.989, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.983, Training Time: 136 seconds)
Epoch:  15/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  7.006, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.989, Training Time: 187 seconds)
Epoch:  15/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.020, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.995, Training Time: 242 seconds)
Epoch:  15/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  7.000, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.996, Training Time: 300 seconds)
Epoch:  15/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  7.000, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.996, Training Time: 308 seconds)
Epoch:  15/100, Validation loss:  7.115, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.966 to  0.963
Epoch:  16/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.968, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.968, Training Time: 42 seconds)
Epoch:  16/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.964, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.966, Training Time: 88 seconds)
Epoch:  16/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.983, Training Time: 47 seconds), Stats for epoch: (Training Loss:  6.972, Training Time: 136 seconds)
Epoch:  16/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.992, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.977, Training Time: 187 seconds)
Epoch:  16/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.002, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.982, Training Time: 242 seconds)
Epoch:  16/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.979, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.981, Training Time: 300 seconds)
Epoch:  16/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.956, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.981, Training Time: 309 seconds)
Epoch:  16/100, Validation loss:  7.138, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.963 to  0.961
Training loss improved!
Epoch:  17/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.964, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.964, Training Time: 42 seconds)
Epoch:  17/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.954, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.959, Training Time: 88 seconds)
Epoch:  17/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.975, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.964, Training Time: 136 seconds)
Epoch:  17/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.988, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.970, Training Time: 187 seconds)
Epoch:  17/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.002, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.977, Training Time: 242 seconds)
Epoch:  17/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.976, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.976, Training Time: 300 seconds)
Epoch:  17/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.951, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.976, Training Time: 309 seconds)
Epoch:  17/100, Validation loss:  7.247, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.961 to  0.958
Training loss improved!
Epoch:  18/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.955, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.955, Training Time: 42 seconds)
Epoch:  18/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.948, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.952, Training Time: 88 seconds)
Epoch:  18/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.972, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.958, Training Time: 136 seconds)
Epoch:  18/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.986, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.965, Training Time: 187 seconds)
Epoch:  18/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.995, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.971, Training Time: 242 seconds)
Epoch:  18/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.973, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.971, Training Time: 300 seconds)
Epoch:  18/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.944, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.971, Training Time: 309 seconds)
Epoch:  18/100, Validation loss: 10.062, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.958 to  0.956
Training loss improved!
Epoch:  19/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.953, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.953, Training Time: 42 seconds)
Epoch:  19/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.962, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.958, Training Time: 88 seconds)
Epoch:  19/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.982, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.966, Training Time: 136 seconds)
Epoch:  19/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.992, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.972, Training Time: 187 seconds)
Epoch:  19/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.002, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.978, Training Time: 242 seconds)
Epoch:  19/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.988, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.980, Training Time: 300 seconds)
Epoch:  19/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.977, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.980, Training Time: 309 seconds)
Epoch:  19/100, Validation loss: 10.017, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.956 to  0.954
Epoch:  20/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.948, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.948, Training Time: 42 seconds)
Epoch:  20/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.947, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.948, Training Time: 88 seconds)
Epoch:  20/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.968, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.954, Training Time: 136 seconds)
Epoch:  20/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.981, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.961, Training Time: 187 seconds)
Epoch:  20/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.990, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.967, Training Time: 242 seconds)
Epoch:  20/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.969, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.967, Training Time: 300 seconds)
Epoch:  20/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.929, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.966, Training Time: 308 seconds)
Epoch:  20/100, Validation loss:  7.167, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.954 to  0.951
Training loss improved!
Epoch:  21/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.935, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.935, Training Time: 42 seconds)
Epoch:  21/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.941, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.938, Training Time: 88 seconds)
Epoch:  21/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.963, Training Time: 47 seconds), Stats for epoch: (Training Loss:  6.946, Training Time: 136 seconds)
Epoch:  21/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.978, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.954, Training Time: 187 seconds)
Epoch:  21/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  7.007, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.965, Training Time: 242 seconds)
Epoch:  21/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.987, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.968, Training Time: 300 seconds)
Epoch:  21/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.946, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.968, Training Time: 308 seconds)
Epoch:  21/100, Validation loss:  7.163, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.951 to  0.949
Epoch:  22/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.947, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.947, Training Time: 42 seconds)
Epoch:  22/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.947, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.947, Training Time: 88 seconds)
Epoch:  22/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.972, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.955, Training Time: 136 seconds)
Epoch:  22/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.986, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.963, Training Time: 187 seconds)
Epoch:  22/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.999, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.970, Training Time: 242 seconds)
Epoch:  22/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.977, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.971, Training Time: 300 seconds)
Epoch:  22/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.947, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.971, Training Time: 309 seconds)
Epoch:  22/100, Validation loss:  7.132, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.949 to  0.946
Epoch:  23/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.939, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.939, Training Time: 42 seconds)
Epoch:  23/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.938, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.939, Training Time: 88 seconds)
Epoch:  23/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.962, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.946, Training Time: 136 seconds)
Epoch:  23/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.976, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.954, Training Time: 187 seconds)
Epoch:  23/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.989, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.961, Training Time: 242 seconds)
Epoch:  23/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.977, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.964, Training Time: 300 seconds)
Epoch:  23/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.961, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.964, Training Time: 309 seconds)
Epoch:  23/100, Validation loss:  7.178, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.946 to  0.944
Training loss improved!
Epoch:  24/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.921, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.921, Training Time: 42 seconds)
Epoch:  24/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.930, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.926, Training Time: 88 seconds)
Epoch:  24/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.957, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.936, Training Time: 136 seconds)
Epoch:  24/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.972, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.945, Training Time: 187 seconds)
Epoch:  24/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.980, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.952, Training Time: 242 seconds)
Epoch:  24/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.958, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.953, Training Time: 300 seconds)
Epoch:  24/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.897, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.951, Training Time: 309 seconds)
Epoch:  24/100, Validation loss:  7.214, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.944 to  0.942
Training loss improved!
Epoch:  25/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.923, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.923, Training Time: 42 seconds)
Epoch:  25/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.929, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.926, Training Time: 88 seconds)
Epoch:  25/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.953, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.935, Training Time: 136 seconds)
Epoch:  25/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.968, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.943, Training Time: 187 seconds)
Epoch:  25/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.976, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.950, Training Time: 242 seconds)
Epoch:  25/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.953, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.950, Training Time: 300 seconds)
Epoch:  25/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.892, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.949, Training Time: 309 seconds)
Epoch:  25/100, Validation loss:  7.249, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.942 to  0.939
Training loss improved!
Epoch:  26/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.908, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.908, Training Time: 42 seconds)
Epoch:  26/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.922, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.915, Training Time: 88 seconds)
Epoch:  26/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.949, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.926, Training Time: 136 seconds)
Epoch:  26/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.964, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.936, Training Time: 187 seconds)
Epoch:  26/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.972, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.943, Training Time: 242 seconds)
Epoch:  26/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.955, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.945, Training Time: 300 seconds)
Epoch:  26/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.888, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.944, Training Time: 309 seconds)
Epoch:  26/100, Validation loss:  7.334, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.939 to  0.937
Training loss improved!
Epoch:  27/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.911, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.911, Training Time: 42 seconds)
Epoch:  27/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.921, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.916, Training Time: 88 seconds)
Epoch:  27/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.945, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.926, Training Time: 136 seconds)
Epoch:  27/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.961, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.935, Training Time: 187 seconds)
Epoch:  27/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.969, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.941, Training Time: 242 seconds)
Epoch:  27/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.964, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.945, Training Time: 300 seconds)
Epoch:  27/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.911, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.944, Training Time: 309 seconds)
Epoch:  27/100, Validation loss:  7.249, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.937 to  0.935
Epoch:  28/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.902, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.902, Training Time: 42 seconds)
Epoch:  28/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.923, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.913, Training Time: 88 seconds)
Epoch:  28/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.946, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.924, Training Time: 136 seconds)
Epoch:  28/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.961, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.933, Training Time: 187 seconds)
Epoch:  28/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.969, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.940, Training Time: 242 seconds)
Epoch:  28/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.964, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.944, Training Time: 300 seconds)
Epoch:  28/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.903, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.943, Training Time: 309 seconds)
Epoch:  28/100, Validation loss:  7.327, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.935 to  0.932
Training loss improved!
Epoch:  29/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.902, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.902, Training Time: 42 seconds)
Epoch:  29/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.916, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.909, Training Time: 88 seconds)
Epoch:  29/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.938, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.919, Training Time: 136 seconds)
Epoch:  29/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.954, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.928, Training Time: 187 seconds)
Epoch:  29/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.962, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.935, Training Time: 242 seconds)
Epoch:  29/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.970, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.940, Training Time: 300 seconds)
Epoch:  29/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.897, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.939, Training Time: 309 seconds)
Epoch:  29/100, Validation loss:  7.297, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.932 to  0.930
Training loss improved!
Epoch:  30/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.897, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.897, Training Time: 42 seconds)
Epoch:  30/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.912, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.904, Training Time: 88 seconds)
Epoch:  30/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.935, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.914, Training Time: 136 seconds)
Epoch:  30/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.950, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.923, Training Time: 187 seconds)
Epoch:  30/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.969, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.933, Training Time: 243 seconds)
Epoch:  30/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.944, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.935, Training Time: 300 seconds)
Epoch:  30/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.882, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.933, Training Time: 309 seconds)
Epoch:  30/100, Validation loss:  7.350, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.930 to  0.928
Training loss improved!
Epoch:  31/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.895, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.895, Training Time: 42 seconds)
Epoch:  31/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.904, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.899, Training Time: 88 seconds)
Epoch:  31/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.928, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.909, Training Time: 136 seconds)
Epoch:  31/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.946, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.918, Training Time: 187 seconds)
Epoch:  31/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.954, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.925, Training Time: 242 seconds)
Epoch:  31/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.938, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.927, Training Time: 300 seconds)
Epoch:  31/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.871, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.926, Training Time: 309 seconds)
Epoch:  31/100, Validation loss:  7.254, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.928 to  0.925
Training loss improved!
Epoch:  32/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.882, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.882, Training Time: 42 seconds)
Epoch:  32/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.894, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.888, Training Time: 88 seconds)
Epoch:  32/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.921, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.899, Training Time: 136 seconds)
Epoch:  32/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.937, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.908, Training Time: 187 seconds)
Epoch:  32/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.944, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.915, Training Time: 242 seconds)
Epoch:  32/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.944, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.920, Training Time: 300 seconds)
Epoch:  32/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.906, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.920, Training Time: 308 seconds)
Epoch:  32/100, Validation loss:  7.245, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.925 to  0.923
Training loss improved!
Epoch:  33/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.869, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.869, Training Time: 42 seconds)
Epoch:  33/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.888, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.878, Training Time: 88 seconds)
Epoch:  33/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.912, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.890, Training Time: 136 seconds)
Epoch:  33/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.928, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.899, Training Time: 187 seconds)
Epoch:  33/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.931, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.905, Training Time: 242 seconds)
Epoch:  33/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.922, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.908, Training Time: 300 seconds)
Epoch:  33/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.857, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.907, Training Time: 308 seconds)
Epoch:  33/100, Validation loss:  7.145, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.923 to  0.921
Training loss improved!
Epoch:  34/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.847, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.847, Training Time: 42 seconds)
Epoch:  34/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.843, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.845, Training Time: 88 seconds)
Epoch:  34/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.848, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.846, Training Time: 136 seconds)
Epoch:  34/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.834, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.843, Training Time: 187 seconds)
Epoch:  34/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.826, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.839, Training Time: 242 seconds)
Epoch:  34/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.793, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.832, Training Time: 300 seconds)
Epoch:  34/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.770, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.830, Training Time: 309 seconds)
Epoch:  34/100, Validation loss:  7.017, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.921 to  0.918
Training loss improved!
Validation loss improved!
Epoch:  35/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.725, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.725, Training Time: 42 seconds)
Epoch:  35/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.705, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.715, Training Time: 88 seconds)
Epoch:  35/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.715, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.715, Training Time: 136 seconds)
Epoch:  35/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.718, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.716, Training Time: 187 seconds)
Epoch:  35/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.724, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.717, Training Time: 242 seconds)
Epoch:  35/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.693, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.713, Training Time: 300 seconds)
Epoch:  35/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.649, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.712, Training Time: 309 seconds)
Epoch:  35/100, Validation loss:  7.083, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.918 to  0.916
Training loss improved!
Epoch:  36/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.630, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.630, Training Time: 42 seconds)
Epoch:  36/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.614, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.622, Training Time: 88 seconds)
Epoch:  36/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.626, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.624, Training Time: 136 seconds)
Epoch:  36/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.634, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.626, Training Time: 187 seconds)
Epoch:  36/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.657, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.632, Training Time: 243 seconds)
Epoch:  36/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.630, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.632, Training Time: 301 seconds)
Epoch:  36/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.610, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.631, Training Time: 309 seconds)
Epoch:  36/100, Validation loss:  7.305, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.916 to  0.914
Training loss improved!
Epoch:  37/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.565, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.565, Training Time: 42 seconds)
Epoch:  37/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.553, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.559, Training Time: 88 seconds)
Epoch:  37/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.579, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.566, Training Time: 136 seconds)
Epoch:  37/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.597, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.574, Training Time: 187 seconds)
Epoch:  37/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.604, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.580, Training Time: 242 seconds)
Epoch:  37/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.586, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.581, Training Time: 300 seconds)
Epoch:  37/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.517, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.579, Training Time: 309 seconds)
Epoch:  37/100, Validation loss:  7.194, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.914 to  0.912
Training loss improved!
Epoch:  38/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.533, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.533, Training Time: 42 seconds)
Epoch:  38/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.514, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.523, Training Time: 88 seconds)
Epoch:  38/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.542, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.530, Training Time: 136 seconds)
Epoch:  38/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.559, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.537, Training Time: 187 seconds)
Epoch:  38/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.571, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.544, Training Time: 242 seconds)
Epoch:  38/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.552, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.545, Training Time: 300 seconds)
Epoch:  38/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.517, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.545, Training Time: 309 seconds)
Epoch:  38/100, Validation loss:  7.082, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.912 to  0.909
Training loss improved!
Epoch:  39/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.496, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.496, Training Time: 42 seconds)
Epoch:  39/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.480, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.488, Training Time: 88 seconds)
Epoch:  39/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.510, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.495, Training Time: 136 seconds)
Epoch:  39/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.528, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.503, Training Time: 187 seconds)
Epoch:  39/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.535, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.510, Training Time: 242 seconds)
Epoch:  39/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.514, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.510, Training Time: 300 seconds)
Epoch:  39/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.458, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.509, Training Time: 309 seconds)
Epoch:  39/100, Validation loss:  6.952, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.909 to  0.907
Training loss improved!
Validation loss improved!
Epoch:  40/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.465, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.465, Training Time: 42 seconds)
Epoch:  40/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.449, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.457, Training Time: 88 seconds)
Epoch:  40/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.480, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.465, Training Time: 136 seconds)
Epoch:  40/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.499, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.473, Training Time: 187 seconds)
Epoch:  40/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.504, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.479, Training Time: 242 seconds)
Epoch:  40/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.481, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.480, Training Time: 300 seconds)
Epoch:  40/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.411, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.478, Training Time: 309 seconds)
Epoch:  40/100, Validation loss:  6.893, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.907 to  0.905
Training loss improved!
Validation loss improved!
Epoch:  41/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.435, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.435, Training Time: 42 seconds)
Epoch:  41/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.414, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.424, Training Time: 88 seconds)
Epoch:  41/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.450, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.433, Training Time: 136 seconds)
Epoch:  41/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.467, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.442, Training Time: 187 seconds)
Epoch:  41/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.477, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.449, Training Time: 242 seconds)
Epoch:  41/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.463, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.451, Training Time: 300 seconds)
Epoch:  41/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.407, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.450, Training Time: 309 seconds)
Epoch:  41/100, Validation loss:  6.896, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.905 to  0.902
Training loss improved!
Epoch:  42/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.396, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.396, Training Time: 42 seconds)
Epoch:  42/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.380, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.388, Training Time: 88 seconds)
Epoch:  42/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.415, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.397, Training Time: 136 seconds)
Epoch:  42/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.435, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.406, Training Time: 187 seconds)
Epoch:  42/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.442, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.414, Training Time: 242 seconds)
Epoch:  42/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.427, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.416, Training Time: 300 seconds)
Epoch:  42/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.361, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.414, Training Time: 309 seconds)
Epoch:  42/100, Validation loss:  6.963, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.902 to  0.900
Training loss improved!
Epoch:  43/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.356, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.356, Training Time: 42 seconds)
Epoch:  43/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.346, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.351, Training Time: 88 seconds)
Epoch:  43/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.391, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.364, Training Time: 136 seconds)
Epoch:  43/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.404, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.374, Training Time: 187 seconds)
Epoch:  43/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.418, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.383, Training Time: 242 seconds)
Epoch:  43/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.411, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.388, Training Time: 300 seconds)
Epoch:  43/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.317, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.386, Training Time: 309 seconds)
Epoch:  43/100, Validation loss:  7.112, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.900 to  0.898
Training loss improved!
Epoch:  44/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.336, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.336, Training Time: 42 seconds)
Epoch:  44/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.319, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.327, Training Time: 88 seconds)
Epoch:  44/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.358, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.337, Training Time: 136 seconds)
Epoch:  44/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.375, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.347, Training Time: 187 seconds)
Epoch:  44/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.380, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.353, Training Time: 243 seconds)
Epoch:  44/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.383, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.358, Training Time: 300 seconds)
Epoch:  44/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.373, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.359, Training Time: 309 seconds)
Epoch:  44/100, Validation loss:  6.761, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.898 to  0.896
Training loss improved!
Validation loss improved!
Epoch:  45/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.303, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.303, Training Time: 42 seconds)
Epoch:  45/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.283, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.293, Training Time: 88 seconds)
Epoch:  45/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.326, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.304, Training Time: 136 seconds)
Epoch:  45/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.344, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.314, Training Time: 187 seconds)
Epoch:  45/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.353, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.322, Training Time: 242 seconds)
Epoch:  45/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.331, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.323, Training Time: 300 seconds)
Epoch:  45/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.258, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.322, Training Time: 309 seconds)
Epoch:  45/100, Validation loss:  6.812, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.896 to  0.893
Training loss improved!
Epoch:  46/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.273, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.273, Training Time: 42 seconds)
Epoch:  46/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.253, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.263, Training Time: 88 seconds)
Epoch:  46/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.296, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.274, Training Time: 136 seconds)
Epoch:  46/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.315, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.284, Training Time: 187 seconds)
Epoch:  46/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.320, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.291, Training Time: 242 seconds)
Epoch:  46/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.298, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.292, Training Time: 300 seconds)
Epoch:  46/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.282, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.292, Training Time: 309 seconds)
Epoch:  46/100, Validation loss:  6.725, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.893 to  0.891
Training loss improved!
Validation loss improved!
Epoch:  47/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.240, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.240, Training Time: 42 seconds)
Epoch:  47/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.218, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.229, Training Time: 88 seconds)
Epoch:  47/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.262, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.240, Training Time: 136 seconds)
Epoch:  47/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.283, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.250, Training Time: 187 seconds)
Epoch:  47/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.290, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.258, Training Time: 242 seconds)
Epoch:  47/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.307, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.266, Training Time: 300 seconds)
Epoch:  47/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.210, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.265, Training Time: 308 seconds)
Epoch:  47/100, Validation loss:  7.177, Batch Validation Time: 24 seconds
Learning rate decay: adjusting from  0.891 to  0.889
Training loss improved!
Epoch:  48/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.208, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.208, Training Time: 42 seconds)
Epoch:  48/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.188, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.198, Training Time: 88 seconds)
Epoch:  48/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.234, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.210, Training Time: 136 seconds)
Epoch:  48/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.255, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.221, Training Time: 187 seconds)
Epoch:  48/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.262, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.230, Training Time: 242 seconds)
Epoch:  48/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.295, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.240, Training Time: 300 seconds)
Epoch:  48/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.252, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.241, Training Time: 309 seconds)
Epoch:  48/100, Validation loss:  6.947, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.889 to  0.887
Training loss improved!
Epoch:  49/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.182, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.182, Training Time: 42 seconds)
Epoch:  49/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.150, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.166, Training Time: 88 seconds)
Epoch:  49/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.204, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.179, Training Time: 136 seconds)
Epoch:  49/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.227, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.191, Training Time: 187 seconds)
Epoch:  49/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.244, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.201, Training Time: 242 seconds)
Epoch:  49/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.239, Training Time: 57 seconds), Stats for epoch: (Training Loss:  6.208, Training Time: 300 seconds)
Epoch:  49/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.203, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.208, Training Time: 308 seconds)
Epoch:  49/100, Validation loss:  7.090, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.887 to  0.885
Training loss improved!
Epoch:  50/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.152, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.152, Training Time: 42 seconds)
Epoch:  50/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.118, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.135, Training Time: 88 seconds)
Epoch:  50/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.170, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.147, Training Time: 136 seconds)
Epoch:  50/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.199, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.160, Training Time: 187 seconds)
Epoch:  50/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.206, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.169, Training Time: 243 seconds)
Epoch:  50/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.209, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.176, Training Time: 301 seconds)
Epoch:  50/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.269, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.178, Training Time: 309 seconds)
Epoch:  50/100, Validation loss:  6.614, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.885 to  0.882
Training loss improved!
Validation loss improved!
Epoch:  51/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.112, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.112, Training Time: 42 seconds)
Epoch:  51/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.081, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.097, Training Time: 88 seconds)
Epoch:  51/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.141, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.112, Training Time: 136 seconds)
Epoch:  51/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.165, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.125, Training Time: 187 seconds)
Epoch:  51/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.174, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.135, Training Time: 243 seconds)
Epoch:  51/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.171, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.141, Training Time: 301 seconds)
Epoch:  51/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.104, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.140, Training Time: 309 seconds)
Epoch:  51/100, Validation loss:  7.058, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.882 to  0.880
Training loss improved!
Epoch:  52/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.086, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.086, Training Time: 42 seconds)
Epoch:  52/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.047, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.066, Training Time: 88 seconds)
Epoch:  52/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.102, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.078, Training Time: 136 seconds)
Epoch:  52/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.132, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.092, Training Time: 188 seconds)
Epoch:  52/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.141, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.101, Training Time: 243 seconds)
Epoch:  52/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.144, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.109, Training Time: 301 seconds)
Epoch:  52/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.121, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.109, Training Time: 309 seconds)
Epoch:  52/100, Validation loss:  6.967, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.880 to  0.878
Training loss improved!
Epoch:  53/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.045, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.045, Training Time: 42 seconds)
Epoch:  53/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  6.013, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.029, Training Time: 88 seconds)
Epoch:  53/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.069, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.043, Training Time: 136 seconds)
Epoch:  53/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.099, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.057, Training Time: 187 seconds)
Epoch:  53/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.109, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.067, Training Time: 243 seconds)
Epoch:  53/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.122, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.076, Training Time: 301 seconds)
Epoch:  53/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.032, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.075, Training Time: 309 seconds)
Epoch:  53/100, Validation loss:  6.777, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.878 to  0.876
Training loss improved!
Epoch:  54/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  6.010, Training Time: 42 seconds), Stats for epoch: (Training Loss:  6.010, Training Time: 42 seconds)
Epoch:  54/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.971, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.990, Training Time: 88 seconds)
Epoch:  54/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  6.027, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.002, Training Time: 136 seconds)
Epoch:  54/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.064, Training Time: 51 seconds), Stats for epoch: (Training Loss:  6.018, Training Time: 187 seconds)
Epoch:  54/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.088, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.032, Training Time: 243 seconds)
Epoch:  54/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.093, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.042, Training Time: 301 seconds)
Epoch:  54/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.073, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.043, Training Time: 309 seconds)
Epoch:  54/100, Validation loss:  6.713, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.876 to  0.874
Training loss improved!
Epoch:  55/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.978, Training Time: 43 seconds), Stats for epoch: (Training Loss:  5.978, Training Time: 43 seconds)
Epoch:  55/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.927, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.952, Training Time: 88 seconds)
Epoch:  55/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.990, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.965, Training Time: 136 seconds)
Epoch:  55/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  6.026, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.980, Training Time: 187 seconds)
Epoch:  55/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.039, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.992, Training Time: 243 seconds)
Epoch:  55/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.057, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.003, Training Time: 301 seconds)
Epoch:  55/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.002, Training Time: 8 seconds), Stats for epoch: (Training Loss:  6.003, Training Time: 309 seconds)
Epoch:  55/100, Validation loss:  6.839, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.874 to  0.871
Training loss improved!
Epoch:  56/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.924, Training Time: 43 seconds), Stats for epoch: (Training Loss:  5.924, Training Time: 43 seconds)
Epoch:  56/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.888, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.906, Training Time: 88 seconds)
Epoch:  56/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.950, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.921, Training Time: 136 seconds)
Epoch:  56/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.989, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.938, Training Time: 187 seconds)
Epoch:  56/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  6.006, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.951, Training Time: 243 seconds)
Epoch:  56/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  6.022, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.963, Training Time: 301 seconds)
Epoch:  56/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.961, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.963, Training Time: 309 seconds)
Epoch:  56/100, Validation loss:  6.807, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.871 to  0.869
Training loss improved!
Epoch:  57/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.892, Training Time: 43 seconds), Stats for epoch: (Training Loss:  5.892, Training Time: 43 seconds)
Epoch:  57/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.847, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.869, Training Time: 88 seconds)
Epoch:  57/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.921, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.886, Training Time: 136 seconds)
Epoch:  57/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.958, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.904, Training Time: 187 seconds)
Epoch:  57/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.976, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.919, Training Time: 243 seconds)
Epoch:  57/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.989, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.930, Training Time: 301 seconds)
Epoch:  57/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  6.021, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.933, Training Time: 310 seconds)
Epoch:  57/100, Validation loss:  6.755, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.869 to  0.867
Training loss improved!
Epoch:  58/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.860, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.860, Training Time: 42 seconds)
Epoch:  58/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.817, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.838, Training Time: 88 seconds)
Epoch:  58/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.877, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.851, Training Time: 136 seconds)
Epoch:  58/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.922, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.869, Training Time: 187 seconds)
Epoch:  58/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.940, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.883, Training Time: 243 seconds)
Epoch:  58/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.961, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.896, Training Time: 301 seconds)
Epoch:  58/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.883, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.896, Training Time: 309 seconds)
Epoch:  58/100, Validation loss:  6.688, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.867 to  0.865
Training loss improved!
Epoch:  59/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.820, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.820, Training Time: 42 seconds)
Epoch:  59/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.775, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.797, Training Time: 88 seconds)
Epoch:  59/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.844, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.813, Training Time: 136 seconds)
Epoch:  59/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.889, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.832, Training Time: 187 seconds)
Epoch:  59/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.909, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.847, Training Time: 243 seconds)
Epoch:  59/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.934, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.862, Training Time: 301 seconds)
Epoch:  59/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.929, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.863, Training Time: 309 seconds)
Epoch:  59/100, Validation loss:  6.699, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.865 to  0.863
Training loss improved!
Epoch:  60/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.769, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.769, Training Time: 42 seconds)
Epoch:  60/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.734, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.752, Training Time: 88 seconds)
Epoch:  60/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.809, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.771, Training Time: 136 seconds)
Epoch:  60/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.857, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.792, Training Time: 187 seconds)
Epoch:  60/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.880, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.810, Training Time: 243 seconds)
Epoch:  60/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.902, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.825, Training Time: 301 seconds)
Epoch:  60/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.833, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.825, Training Time: 309 seconds)
Epoch:  60/100, Validation loss:  6.917, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.863 to  0.861
Training loss improved!
Epoch:  61/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.731, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.731, Training Time: 42 seconds)
Epoch:  61/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.692, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.712, Training Time: 88 seconds)
Epoch:  61/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.768, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.731, Training Time: 136 seconds)
Epoch:  61/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.824, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.754, Training Time: 187 seconds)
Epoch:  61/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.847, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.773, Training Time: 243 seconds)
Epoch:  61/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.866, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.788, Training Time: 301 seconds)
Epoch:  61/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.836, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.789, Training Time: 309 seconds)
Epoch:  61/100, Validation loss:  6.886, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.861 to  0.858
Training loss improved!
Epoch:  62/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.692, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.692, Training Time: 42 seconds)
Epoch:  62/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.652, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.672, Training Time: 88 seconds)
Epoch:  62/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.739, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.694, Training Time: 136 seconds)
Epoch:  62/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.792, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.719, Training Time: 187 seconds)
Epoch:  62/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.816, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.738, Training Time: 243 seconds)
Epoch:  62/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.836, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.754, Training Time: 301 seconds)
Epoch:  62/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.759, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.755, Training Time: 309 seconds)
Epoch:  62/100, Validation loss:  6.410, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.858 to  0.856
Training loss improved!
Validation loss improved!
Epoch:  63/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.646, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.646, Training Time: 42 seconds)
Epoch:  63/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.611, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.629, Training Time: 88 seconds)
Epoch:  63/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.696, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.651, Training Time: 136 seconds)
Epoch:  63/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.767, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.680, Training Time: 187 seconds)
Epoch:  63/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.796, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.703, Training Time: 243 seconds)
Epoch:  63/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.815, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.722, Training Time: 300 seconds)
Epoch:  63/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.726, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.722, Training Time: 309 seconds)
Epoch:  63/100, Validation loss:  6.663, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.856 to  0.854
Training loss improved!
Epoch:  64/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.610, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.610, Training Time: 42 seconds)
Epoch:  64/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.575, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.592, Training Time: 88 seconds)
Epoch:  64/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.659, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.615, Training Time: 136 seconds)
Epoch:  64/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.728, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.643, Training Time: 187 seconds)
Epoch:  64/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.754, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.665, Training Time: 243 seconds)
Epoch:  64/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.784, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.685, Training Time: 301 seconds)
Epoch:  64/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.722, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.686, Training Time: 309 seconds)
Epoch:  64/100, Validation loss:  7.121, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.854 to  0.852
Training loss improved!
Epoch:  65/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.570, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.570, Training Time: 42 seconds)
Epoch:  65/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.539, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.555, Training Time: 88 seconds)
Epoch:  65/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.625, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.578, Training Time: 136 seconds)
Epoch:  65/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.701, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.609, Training Time: 187 seconds)
Epoch:  65/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.736, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.634, Training Time: 243 seconds)
Epoch:  65/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.763, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.656, Training Time: 301 seconds)
Epoch:  65/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.686, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.657, Training Time: 309 seconds)
Epoch:  65/100, Validation loss:  6.629, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.852 to  0.850
Training loss improved!
Epoch:  66/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.515, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.515, Training Time: 42 seconds)
Epoch:  66/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.489, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.502, Training Time: 88 seconds)
Epoch:  66/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.588, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.530, Training Time: 136 seconds)
Epoch:  66/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.662, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.563, Training Time: 187 seconds)
Epoch:  66/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.694, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.589, Training Time: 243 seconds)
Epoch:  66/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.740, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.614, Training Time: 301 seconds)
Epoch:  66/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.682, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.616, Training Time: 309 seconds)
Epoch:  66/100, Validation loss:  6.490, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.850 to  0.848
Training loss improved!
Epoch:  67/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.474, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.474, Training Time: 42 seconds)
Epoch:  67/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.450, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.462, Training Time: 88 seconds)
Epoch:  67/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.545, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.490, Training Time: 136 seconds)
Epoch:  67/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.628, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.524, Training Time: 187 seconds)
Epoch:  67/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.660, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.551, Training Time: 243 seconds)
Epoch:  67/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.700, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.576, Training Time: 301 seconds)
Epoch:  67/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.721, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.579, Training Time: 309 seconds)
Epoch:  67/100, Validation loss:  6.693, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.848 to  0.846
Training loss improved!
Epoch:  68/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.442, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.442, Training Time: 42 seconds)
Epoch:  68/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.415, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.428, Training Time: 88 seconds)
Epoch:  68/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.506, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.454, Training Time: 136 seconds)
Epoch:  68/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.598, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.490, Training Time: 187 seconds)
Epoch:  68/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.634, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.519, Training Time: 242 seconds)
Epoch:  68/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.662, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.543, Training Time: 300 seconds)
Epoch:  68/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.620, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.544, Training Time: 309 seconds)
Epoch:  68/100, Validation loss:  6.316, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.846 to  0.843
Training loss improved!
Validation loss improved!
Epoch:  69/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.385, Training Time: 43 seconds), Stats for epoch: (Training Loss:  5.385, Training Time: 43 seconds)
Epoch:  69/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.365, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.375, Training Time: 88 seconds)
Epoch:  69/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.467, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.406, Training Time: 136 seconds)
Epoch:  69/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.561, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.444, Training Time: 187 seconds)
Epoch:  69/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.596, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.475, Training Time: 243 seconds)
Epoch:  69/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.652, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.504, Training Time: 301 seconds)
Epoch:  69/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.566, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.506, Training Time: 309 seconds)
Epoch:  69/100, Validation loss:  6.581, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.843 to  0.841
Training loss improved!
Epoch:  70/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.321, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.321, Training Time: 42 seconds)
Epoch:  70/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.321, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.321, Training Time: 88 seconds)
Epoch:  70/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.423, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.355, Training Time: 136 seconds)
Epoch:  70/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.526, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.398, Training Time: 187 seconds)
Epoch:  70/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.563, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.431, Training Time: 243 seconds)
Epoch:  70/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.625, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.463, Training Time: 301 seconds)
Epoch:  70/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.543, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.465, Training Time: 309 seconds)
Epoch:  70/100, Validation loss:  6.592, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.841 to  0.839
Training loss improved!
Epoch:  71/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.280, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.280, Training Time: 42 seconds)
Epoch:  71/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.277, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.278, Training Time: 88 seconds)
Epoch:  71/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.380, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.312, Training Time: 136 seconds)
Epoch:  71/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.492, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.357, Training Time: 187 seconds)
Epoch:  71/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.535, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.393, Training Time: 243 seconds)
Epoch:  71/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.587, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.425, Training Time: 301 seconds)
Epoch:  71/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.484, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.427, Training Time: 309 seconds)
Epoch:  71/100, Validation loss:  6.283, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.839 to  0.837
Training loss improved!
Validation loss improved!
Epoch:  72/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.238, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.238, Training Time: 42 seconds)
Epoch:  72/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.231, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.235, Training Time: 88 seconds)
Epoch:  72/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.348, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.272, Training Time: 136 seconds)
Epoch:  72/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.454, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.318, Training Time: 187 seconds)
Epoch:  72/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.497, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.354, Training Time: 242 seconds)
Epoch:  72/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.562, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.388, Training Time: 300 seconds)
Epoch:  72/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.457, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.390, Training Time: 309 seconds)
Epoch:  72/100, Validation loss:  6.450, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.837 to  0.835
Training loss improved!
Epoch:  73/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.188, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.188, Training Time: 42 seconds)
Epoch:  73/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.179, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.184, Training Time: 88 seconds)
Epoch:  73/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.302, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.223, Training Time: 136 seconds)
Epoch:  73/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.421, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.272, Training Time: 187 seconds)
Epoch:  73/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.461, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.310, Training Time: 242 seconds)
Epoch:  73/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.514, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.344, Training Time: 300 seconds)
Epoch:  73/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.412, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.346, Training Time: 309 seconds)
Epoch:  73/100, Validation loss:  6.406, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.835 to  0.833
Training loss improved!
Epoch:  74/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.129, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.129, Training Time: 42 seconds)
Epoch:  74/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.140, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.134, Training Time: 88 seconds)
Epoch:  74/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.258, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.175, Training Time: 136 seconds)
Epoch:  74/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.387, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.228, Training Time: 187 seconds)
Epoch:  74/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.431, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.269, Training Time: 243 seconds)
Epoch:  74/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.492, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.306, Training Time: 301 seconds)
Epoch:  74/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.388, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.308, Training Time: 309 seconds)
Epoch:  74/100, Validation loss:  6.439, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.833 to  0.831
Training loss improved!
Epoch:  75/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.095, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.095, Training Time: 42 seconds)
Epoch:  75/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.092, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.094, Training Time: 88 seconds)
Epoch:  75/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.210, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.132, Training Time: 136 seconds)
Epoch:  75/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.349, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.187, Training Time: 187 seconds)
Epoch:  75/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.395, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.228, Training Time: 243 seconds)
Epoch:  75/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.448, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.265, Training Time: 301 seconds)
Epoch:  75/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.396, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.268, Training Time: 309 seconds)
Epoch:  75/100, Validation loss:  6.702, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.831 to  0.829
Training loss improved!
Epoch:  76/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  5.034, Training Time: 42 seconds), Stats for epoch: (Training Loss:  5.034, Training Time: 42 seconds)
Epoch:  76/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  5.053, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.044, Training Time: 88 seconds)
Epoch:  76/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.175, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.087, Training Time: 136 seconds)
Epoch:  76/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.309, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.143, Training Time: 187 seconds)
Epoch:  76/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.364, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.187, Training Time: 242 seconds)
Epoch:  76/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.434, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.228, Training Time: 300 seconds)
Epoch:  76/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.328, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.231, Training Time: 309 seconds)
Epoch:  76/100, Validation loss:  6.350, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.829 to  0.827
Training loss improved!
Epoch:  77/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.987, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.987, Training Time: 42 seconds)
Epoch:  77/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.995, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.991, Training Time: 88 seconds)
Epoch:  77/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.130, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.038, Training Time: 136 seconds)
Epoch:  77/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.271, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.096, Training Time: 187 seconds)
Epoch:  77/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.327, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.142, Training Time: 243 seconds)
Epoch:  77/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.391, Training Time: 57 seconds), Stats for epoch: (Training Loss:  5.184, Training Time: 301 seconds)
Epoch:  77/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.322, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.187, Training Time: 309 seconds)
Epoch:  77/100, Validation loss:  6.370, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.827 to  0.825
Training loss improved!
Epoch:  78/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.925, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.925, Training Time: 42 seconds)
Epoch:  78/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.958, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.941, Training Time: 88 seconds)
Epoch:  78/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.086, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.990, Training Time: 136 seconds)
Epoch:  78/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.232, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.050, Training Time: 187 seconds)
Epoch:  78/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.295, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.099, Training Time: 242 seconds)
Epoch:  78/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.359, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.143, Training Time: 300 seconds)
Epoch:  78/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.262, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.145, Training Time: 309 seconds)
Epoch:  78/100, Validation loss:  6.316, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.825 to  0.823
Training loss improved!
Epoch:  79/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.875, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.875, Training Time: 42 seconds)
Epoch:  79/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.898, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.886, Training Time: 88 seconds)
Epoch:  79/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.038, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.937, Training Time: 136 seconds)
Epoch:  79/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.199, Training Time: 51 seconds), Stats for epoch: (Training Loss:  5.002, Training Time: 187 seconds)
Epoch:  79/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.261, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.054, Training Time: 243 seconds)
Epoch:  79/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.324, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.099, Training Time: 301 seconds)
Epoch:  79/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.256, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.103, Training Time: 309 seconds)
Epoch:  79/100, Validation loss:  6.328, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.823 to  0.821
Training loss improved!
Epoch:  80/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.817, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.817, Training Time: 42 seconds)
Epoch:  80/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.857, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.837, Training Time: 88 seconds)
Epoch:  80/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  5.001, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.892, Training Time: 136 seconds)
Epoch:  80/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.155, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.958, Training Time: 187 seconds)
Epoch:  80/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.220, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.010, Training Time: 243 seconds)
Epoch:  80/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.306, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.059, Training Time: 301 seconds)
Epoch:  80/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.217, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.063, Training Time: 309 seconds)
Epoch:  80/100, Validation loss:  6.444, Batch Validation Time: 26 seconds
Learning rate decay: adjusting from  0.821 to  0.819
Training loss improved!
Epoch:  81/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.763, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.763, Training Time: 42 seconds)
Epoch:  81/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.803, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.783, Training Time: 88 seconds)
Epoch:  81/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.945, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.837, Training Time: 136 seconds)
Epoch:  81/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.120, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.908, Training Time: 187 seconds)
Epoch:  81/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.186, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.963, Training Time: 243 seconds)
Epoch:  81/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.280, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.016, Training Time: 301 seconds)
Epoch:  81/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.198, Training Time: 8 seconds), Stats for epoch: (Training Loss:  5.021, Training Time: 309 seconds)
Epoch:  81/100, Validation loss:  6.358, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.819 to  0.816
Training loss improved!
Epoch:  82/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.711, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.711, Training Time: 42 seconds)
Epoch:  82/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.759, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.735, Training Time: 88 seconds)
Epoch:  82/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.905, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.792, Training Time: 136 seconds)
Epoch:  82/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.086, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.865, Training Time: 187 seconds)
Epoch:  82/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.151, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.923, Training Time: 243 seconds)
Epoch:  82/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.240, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.976, Training Time: 301 seconds)
Epoch:  82/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.148, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.980, Training Time: 309 seconds)
Epoch:  82/100, Validation loss:  6.368, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.816 to  0.814
Training loss improved!
Epoch:  83/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.670, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.670, Training Time: 42 seconds)
Epoch:  83/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.721, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.695, Training Time: 88 seconds)
Epoch:  83/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.856, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.749, Training Time: 136 seconds)
Epoch:  83/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.047, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.823, Training Time: 187 seconds)
Epoch:  83/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.119, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.882, Training Time: 242 seconds)
Epoch:  83/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.211, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.937, Training Time: 301 seconds)
Epoch:  83/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.144, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.942, Training Time: 309 seconds)
Epoch:  83/100, Validation loss:  6.484, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.814 to  0.812
Training loss improved!
Epoch:  84/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.598, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.598, Training Time: 42 seconds)
Epoch:  84/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.671, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.635, Training Time: 88 seconds)
Epoch:  84/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.816, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.695, Training Time: 136 seconds)
Epoch:  84/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  5.001, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.771, Training Time: 187 seconds)
Epoch:  84/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.083, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.834, Training Time: 243 seconds)
Epoch:  84/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.180, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.891, Training Time: 301 seconds)
Epoch:  84/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.122, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.897, Training Time: 309 seconds)
Epoch:  84/100, Validation loss:  6.401, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.812 to  0.810
Training loss improved!
Epoch:  85/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.549, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.549, Training Time: 42 seconds)
Epoch:  85/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.620, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.584, Training Time: 88 seconds)
Epoch:  85/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.763, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.644, Training Time: 136 seconds)
Epoch:  85/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.962, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.723, Training Time: 187 seconds)
Epoch:  85/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.046, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.788, Training Time: 243 seconds)
Epoch:  85/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.145, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.847, Training Time: 301 seconds)
Epoch:  85/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.070, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.853, Training Time: 309 seconds)
Epoch:  85/100, Validation loss:  6.477, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.810 to  0.808
Training loss improved!
Epoch:  86/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.496, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.496, Training Time: 42 seconds)
Epoch:  86/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.572, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.534, Training Time: 88 seconds)
Epoch:  86/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.715, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.594, Training Time: 136 seconds)
Epoch:  86/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.926, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.677, Training Time: 187 seconds)
Epoch:  86/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  5.010, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.744, Training Time: 243 seconds)
Epoch:  86/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.108, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.805, Training Time: 301 seconds)
Epoch:  86/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.032, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.810, Training Time: 309 seconds)
Epoch:  86/100, Validation loss:  6.421, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.808 to  0.806
Training loss improved!
Epoch:  87/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.438, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.438, Training Time: 42 seconds)
Epoch:  87/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.523, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.480, Training Time: 88 seconds)
Epoch:  87/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.676, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.545, Training Time: 136 seconds)
Epoch:  87/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.882, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.630, Training Time: 187 seconds)
Epoch:  87/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.973, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.698, Training Time: 243 seconds)
Epoch:  87/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.087, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.763, Training Time: 301 seconds)
Epoch:  87/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  5.010, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.769, Training Time: 309 seconds)
Epoch:  87/100, Validation loss:  6.490, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.806 to  0.804
Training loss improved!
Epoch:  88/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.397, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.397, Training Time: 42 seconds)
Epoch:  88/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.473, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.435, Training Time: 88 seconds)
Epoch:  88/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.637, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.503, Training Time: 136 seconds)
Epoch:  88/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.842, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.587, Training Time: 187 seconds)
Epoch:  88/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.933, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.657, Training Time: 243 seconds)
Epoch:  88/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.054, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.723, Training Time: 301 seconds)
Epoch:  88/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.996, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.729, Training Time: 309 seconds)
Epoch:  88/100, Validation loss:  6.477, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.804 to  0.802
Training loss improved!
Epoch:  89/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.334, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.334, Training Time: 42 seconds)
Epoch:  89/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.419, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.377, Training Time: 88 seconds)
Epoch:  89/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.594, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.449, Training Time: 136 seconds)
Epoch:  89/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.806, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.539, Training Time: 187 seconds)
Epoch:  89/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.892, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.609, Training Time: 243 seconds)
Epoch:  89/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  5.022, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.678, Training Time: 301 seconds)
Epoch:  89/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.976, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.685, Training Time: 309 seconds)
Epoch:  89/100, Validation loss:  6.472, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.802 to  0.800
Training loss improved!
Epoch:  90/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.290, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.290, Training Time: 42 seconds)
Epoch:  90/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.377, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.334, Training Time: 88 seconds)
Epoch:  90/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.547, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.405, Training Time: 136 seconds)
Epoch:  90/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.774, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.497, Training Time: 187 seconds)
Epoch:  90/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.857, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.569, Training Time: 243 seconds)
Epoch:  90/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  4.988, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.639, Training Time: 301 seconds)
Epoch:  90/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.916, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.646, Training Time: 309 seconds)
Epoch:  90/100, Validation loss:  6.507, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.800 to  0.798
Training loss improved!
Epoch:  91/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.229, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.229, Training Time: 42 seconds)
Epoch:  91/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.329, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.279, Training Time: 88 seconds)
Epoch:  91/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.502, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.353, Training Time: 136 seconds)
Epoch:  91/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.731, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.448, Training Time: 187 seconds)
Epoch:  91/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.819, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.522, Training Time: 243 seconds)
Epoch:  91/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  4.962, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.595, Training Time: 301 seconds)
Epoch:  91/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.960, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.604, Training Time: 309 seconds)
Epoch:  91/100, Validation loss:  6.558, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.798 to  0.796
Training loss improved!
Epoch:  92/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.175, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.175, Training Time: 42 seconds)
Epoch:  92/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.282, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.228, Training Time: 88 seconds)
Epoch:  92/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.460, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.306, Training Time: 136 seconds)
Epoch:  92/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.691, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.402, Training Time: 187 seconds)
Epoch:  92/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.779, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.477, Training Time: 243 seconds)
Epoch:  92/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  4.926, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.552, Training Time: 301 seconds)
Epoch:  92/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.918, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.561, Training Time: 309 seconds)
Epoch:  92/100, Validation loss:  6.500, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.796 to  0.794
Training loss improved!
Epoch:  93/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.118, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.118, Training Time: 42 seconds)
Epoch:  93/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.235, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.177, Training Time: 88 seconds)
Epoch:  93/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.410, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.255, Training Time: 136 seconds)
Epoch:  93/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.652, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.354, Training Time: 187 seconds)
Epoch:  93/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.746, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.433, Training Time: 243 seconds)
Epoch:  93/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  4.910, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.512, Training Time: 301 seconds)
Epoch:  93/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.835, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.520, Training Time: 309 seconds)
Epoch:  93/100, Validation loss:  6.541, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.794 to  0.792
Training loss improved!
Epoch:  94/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.077, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.077, Training Time: 42 seconds)
Epoch:  94/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.186, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.132, Training Time: 88 seconds)
Epoch:  94/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.365, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.209, Training Time: 136 seconds)
Epoch:  94/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.605, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.308, Training Time: 187 seconds)
Epoch:  94/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.711, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.389, Training Time: 243 seconds)
Epoch:  94/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  4.863, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.468, Training Time: 301 seconds)
Epoch:  94/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.841, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.477, Training Time: 309 seconds)
Epoch:  94/100, Validation loss:  6.578, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.792 to  0.790
Training loss improved!
Epoch:  95/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  4.040, Training Time: 42 seconds), Stats for epoch: (Training Loss:  4.040, Training Time: 42 seconds)
Epoch:  95/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.151, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.095, Training Time: 88 seconds)
Epoch:  95/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.318, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.169, Training Time: 136 seconds)
Epoch:  95/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.575, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.271, Training Time: 187 seconds)
Epoch:  95/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.675, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.352, Training Time: 243 seconds)
Epoch:  95/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  4.837, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.433, Training Time: 301 seconds)
Epoch:  95/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.791, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.441, Training Time: 309 seconds)
Epoch:  95/100, Validation loss:  6.591, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.790 to  0.788
Training loss improved!
Epoch:  96/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  3.991, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.991, Training Time: 42 seconds)
Epoch:  96/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.104, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.048, Training Time: 88 seconds)
Epoch:  96/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.278, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.125, Training Time: 136 seconds)
Epoch:  96/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.536, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.227, Training Time: 187 seconds)
Epoch:  96/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.641, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.310, Training Time: 243 seconds)
Epoch:  96/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  4.800, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.392, Training Time: 301 seconds)
Epoch:  96/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.816, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.402, Training Time: 309 seconds)
Epoch:  96/100, Validation loss:  6.587, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.788 to  0.786
Training loss improved!
Epoch:  97/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  3.925, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.925, Training Time: 42 seconds)
Epoch:  97/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.053, Training Time: 45 seconds), Stats for epoch: (Training Loss:  3.989, Training Time: 88 seconds)
Epoch:  97/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.236, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.071, Training Time: 136 seconds)
Epoch:  97/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.487, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.175, Training Time: 187 seconds)
Epoch:  97/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.617, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.263, Training Time: 243 seconds)
Epoch:  97/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  4.769, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.348, Training Time: 301 seconds)
Epoch:  97/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.776, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.358, Training Time: 309 seconds)
Epoch:  97/100, Validation loss:  6.707, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.786 to  0.784
Training loss improved!
Epoch:  98/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  3.873, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.873, Training Time: 42 seconds)
Epoch:  98/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  4.006, Training Time: 45 seconds), Stats for epoch: (Training Loss:  3.939, Training Time: 88 seconds)
Epoch:  98/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.188, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.022, Training Time: 136 seconds)
Epoch:  98/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.450, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.129, Training Time: 187 seconds)
Epoch:  98/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.579, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.219, Training Time: 242 seconds)
Epoch:  98/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  4.740, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.306, Training Time: 301 seconds)
Epoch:  98/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.709, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.316, Training Time: 309 seconds)
Epoch:  98/100, Validation loss:  6.671, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.784 to  0.782
Training loss improved!
Epoch:  99/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  3.825, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.825, Training Time: 42 seconds)
Epoch:  99/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  3.961, Training Time: 45 seconds), Stats for epoch: (Training Loss:  3.893, Training Time: 88 seconds)
Epoch:  99/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.144, Training Time: 48 seconds), Stats for epoch: (Training Loss:  3.977, Training Time: 136 seconds)
Epoch:  99/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.413, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.086, Training Time: 187 seconds)
Epoch:  99/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.539, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.176, Training Time: 243 seconds)
Epoch:  99/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  4.713, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.266, Training Time: 301 seconds)
Epoch:  99/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.661, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.275, Training Time: 309 seconds)
Epoch:  99/100, Validation loss:  6.643, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.782 to  0.781
Training loss improved!
Epoch: 100/100, Batch:  100/615, Stats for last 100 batches: (Training Loss:  3.779, Training Time: 42 seconds), Stats for epoch: (Training Loss:  3.779, Training Time: 42 seconds)
Epoch: 100/100, Batch:  200/615, Stats for last 100 batches: (Training Loss:  3.927, Training Time: 45 seconds), Stats for epoch: (Training Loss:  3.853, Training Time: 88 seconds)
Epoch: 100/100, Batch:  300/615, Stats for last 100 batches: (Training Loss:  4.111, Training Time: 48 seconds), Stats for epoch: (Training Loss:  3.939, Training Time: 136 seconds)
Epoch: 100/100, Batch:  400/615, Stats for last 100 batches: (Training Loss:  4.368, Training Time: 51 seconds), Stats for epoch: (Training Loss:  4.047, Training Time: 187 seconds)
Epoch: 100/100, Batch:  500/615, Stats for last 100 batches: (Training Loss:  4.505, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.138, Training Time: 243 seconds)
Epoch: 100/100, Batch:  600/615, Stats for last 100 batches: (Training Loss:  4.691, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.230, Training Time: 301 seconds)
Epoch: 100/100, Batch:  615/615, Stats for last 15 batches: (Training Loss:  4.635, Training Time: 8 seconds), Stats for epoch: (Training Loss:  4.240, Training Time: 309 seconds)
Epoch: 100/100, Validation loss:  6.664, Batch Validation Time: 25 seconds
Learning rate decay: adjusting from  0.781 to  0.779
Training loss improved!
Training Complete!
