
Reading dataset 'csv'...

Final shared vocab size: 76669

Splitting 120264 samples into training & validation sets (20.0% used for validation)...
Training set: 96212 samples. Validation set: 24052 samples.
Sorting training & validation sets to increase training efficiency...
Initializing model...


Creating checkpoint batch files...
Initializing training...
Epochs: 100
Batch Size: 128
Optimizer: sgd
Epoch:   1/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  8.921, Training Time: 48 seconds), Stats for epoch: (Training Loss:  8.921, Training Time: 48 seconds)
Epoch:   1/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.871, Training Time: 48 seconds), Stats for epoch: (Training Loss:  8.396, Training Time: 96 seconds)
Epoch:   1/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.648, Training Time: 50 seconds), Stats for epoch: (Training Loss:  8.146, Training Time: 146 seconds)
Epoch:   1/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.544, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.996, Training Time: 199 seconds)
Epoch:   1/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.478, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.892, Training Time: 254 seconds)
Epoch:   1/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.457, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.820, Training Time: 313 seconds)
Epoch:   1/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.390, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.758, Training Time: 374 seconds)
Epoch:   1/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.337, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.729, Training Time: 406 seconds)
Epoch:   1/100, Validation loss:  7.410, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  1.000 to  0.998
Training loss improved!
Validation loss improved!
Epoch:   2/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.454, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.454, Training Time: 45 seconds)
Epoch:   2/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.332, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.393, Training Time: 93 seconds)
Epoch:   2/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.287, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.358, Training Time: 143 seconds)
Epoch:   2/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.280, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.338, Training Time: 196 seconds)
Epoch:   2/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.269, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.324, Training Time: 251 seconds)
Epoch:   2/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.274, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.316, Training Time: 310 seconds)
Epoch:   2/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.233, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.304, Training Time: 371 seconds)
Epoch:   2/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.200, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.297, Training Time: 403 seconds)
Epoch:   2/100, Validation loss:  7.272, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.998 to  0.995
Training loss improved!
Validation loss improved!
Epoch:   3/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.264, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.264, Training Time: 45 seconds)
Epoch:   3/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.252, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.258, Training Time: 93 seconds)
Epoch:   3/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.192, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.236, Training Time: 143 seconds)
Epoch:   3/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.202, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.227, Training Time: 196 seconds)
Epoch:   3/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.198, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.221, Training Time: 251 seconds)
Epoch:   3/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.205, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.219, Training Time: 310 seconds)
Epoch:   3/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.167, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.211, Training Time: 371 seconds)
Epoch:   3/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.154, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.207, Training Time: 403 seconds)
Epoch:   3/100, Validation loss:  7.197, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.995 to  0.993
Training loss improved!
Validation loss improved!
Epoch:   4/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.251, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.251, Training Time: 45 seconds)
Epoch:   4/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.188, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.219, Training Time: 93 seconds)
Epoch:   4/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.150, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.196, Training Time: 143 seconds)
Epoch:   4/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.160, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.187, Training Time: 196 seconds)
Epoch:   4/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.158, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.181, Training Time: 251 seconds)
Epoch:   4/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.177, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.181, Training Time: 310 seconds)
Epoch:   4/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.140, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.175, Training Time: 371 seconds)
Epoch:   4/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.122, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.171, Training Time: 403 seconds)
Epoch:   4/100, Validation loss:  7.166, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.993 to  0.990
Training loss improved!
Validation loss improved!
Epoch:   5/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.223, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.223, Training Time: 45 seconds)
Epoch:   5/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.140, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.181, Training Time: 93 seconds)
Epoch:   5/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.133, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.165, Training Time: 143 seconds)
Epoch:   5/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.145, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.160, Training Time: 196 seconds)
Epoch:   5/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.137, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.156, Training Time: 251 seconds)
Epoch:   5/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.154, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.155, Training Time: 310 seconds)
Epoch:   5/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.110, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.149, Training Time: 371 seconds)
Epoch:   5/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.104, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.146, Training Time: 403 seconds)
Epoch:   5/100, Validation loss:  7.161, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.990 to  0.988
Training loss improved!
Validation loss improved!
Epoch:   6/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.147, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.147, Training Time: 45 seconds)
Epoch:   6/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.116, Training Time: 47 seconds), Stats for epoch: (Training Loss:  7.131, Training Time: 93 seconds)
Epoch:   6/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.103, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.122, Training Time: 143 seconds)
Epoch:   6/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.116, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.120, Training Time: 196 seconds)
Epoch:   6/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.122, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.121, Training Time: 251 seconds)
Epoch:   6/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.137, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.123, Training Time: 310 seconds)
Epoch:   6/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.096, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.120, Training Time: 371 seconds)
Epoch:   6/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.091, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.118, Training Time: 403 seconds)
Epoch:   6/100, Validation loss:  7.159, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.988 to  0.985
Training loss improved!
Validation loss improved!
Epoch:   7/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.139, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.139, Training Time: 45 seconds)
Epoch:   7/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.097, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.118, Training Time: 93 seconds)
Epoch:   7/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.087, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.108, Training Time: 143 seconds)
Epoch:   7/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.102, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.106, Training Time: 196 seconds)
Epoch:   7/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.110, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.107, Training Time: 251 seconds)
Epoch:   7/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.126, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.110, Training Time: 310 seconds)
Epoch:   7/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.084, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.106, Training Time: 371 seconds)
Epoch:   7/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.081, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.105, Training Time: 403 seconds)
Epoch:   7/100, Validation loss:  7.141, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.985 to  0.983
Training loss improved!
Validation loss improved!
Epoch:   8/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.124, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.124, Training Time: 45 seconds)
Epoch:   8/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.090, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.107, Training Time: 93 seconds)
Epoch:   8/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.079, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.098, Training Time: 143 seconds)
Epoch:   8/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.090, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.096, Training Time: 196 seconds)
Epoch:   8/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.095, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.096, Training Time: 251 seconds)
Epoch:   8/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.111, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.098, Training Time: 310 seconds)
Epoch:   8/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.084, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.096, Training Time: 371 seconds)
Epoch:   8/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.068, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.094, Training Time: 403 seconds)
Epoch:   8/100, Validation loss:  7.156, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.983 to  0.980
Training loss improved!
Epoch:   9/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.095, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.095, Training Time: 45 seconds)
Epoch:   9/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.070, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.082, Training Time: 93 seconds)
Epoch:   9/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.068, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.078, Training Time: 144 seconds)
Epoch:   9/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.079, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.078, Training Time: 196 seconds)
Epoch:   9/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.078, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.078, Training Time: 251 seconds)
Epoch:   9/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.103, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.082, Training Time: 310 seconds)
Epoch:   9/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.084, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.082, Training Time: 371 seconds)
Epoch:   9/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.071, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.082, Training Time: 403 seconds)
Epoch:   9/100, Validation loss:  7.123, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.980 to  0.978
Training loss improved!
Validation loss improved!
Epoch:  10/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.102, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.102, Training Time: 45 seconds)
Epoch:  10/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.069, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.085, Training Time: 93 seconds)
Epoch:  10/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.061, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.077, Training Time: 143 seconds)
Epoch:  10/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.074, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.076, Training Time: 196 seconds)
Epoch:  10/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.075, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.076, Training Time: 251 seconds)
Epoch:  10/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.098, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.080, Training Time: 310 seconds)
Epoch:  10/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.071, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.079, Training Time: 371 seconds)
Epoch:  10/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.062, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.077, Training Time: 403 seconds)
Epoch:  10/100, Validation loss:  7.124, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.978 to  0.975
Training loss improved!
Epoch:  11/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.068, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.068, Training Time: 45 seconds)
Epoch:  11/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.057, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.062, Training Time: 93 seconds)
Epoch:  11/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.052, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.059, Training Time: 143 seconds)
Epoch:  11/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.065, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.060, Training Time: 196 seconds)
Epoch:  11/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.067, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.062, Training Time: 251 seconds)
Epoch:  11/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.091, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.067, Training Time: 310 seconds)
Epoch:  11/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.072, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.067, Training Time: 371 seconds)
Epoch:  11/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.053, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.066, Training Time: 403 seconds)
Epoch:  11/100, Validation loss:  7.184, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.975 to  0.973
Training loss improved!
Epoch:  12/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.069, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.069, Training Time: 45 seconds)
Epoch:  12/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.055, Training Time: 47 seconds), Stats for epoch: (Training Loss:  7.062, Training Time: 93 seconds)
Epoch:  12/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.050, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.058, Training Time: 143 seconds)
Epoch:  12/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.061, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.058, Training Time: 196 seconds)
Epoch:  12/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.063, Training Time: 54 seconds), Stats for epoch: (Training Loss:  7.059, Training Time: 251 seconds)
Epoch:  12/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.081, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.063, Training Time: 310 seconds)
Epoch:  12/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.065, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.063, Training Time: 371 seconds)
Epoch:  12/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.053, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.063, Training Time: 403 seconds)
Epoch:  12/100, Validation loss:  7.153, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.973 to  0.970
Training loss improved!
Epoch:  13/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.070, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.070, Training Time: 45 seconds)
Epoch:  13/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.048, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.059, Training Time: 93 seconds)
Epoch:  13/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.039, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.052, Training Time: 143 seconds)
Epoch:  13/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.056, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.053, Training Time: 196 seconds)
Epoch:  13/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.058, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.054, Training Time: 251 seconds)
Epoch:  13/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.081, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.059, Training Time: 310 seconds)
Epoch:  13/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.056, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.058, Training Time: 371 seconds)
Epoch:  13/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.048, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.058, Training Time: 403 seconds)
Epoch:  13/100, Validation loss:  7.145, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.970 to  0.968
Training loss improved!
Epoch:  14/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.055, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.055, Training Time: 45 seconds)
Epoch:  14/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.048, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.052, Training Time: 93 seconds)
Epoch:  14/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.040, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.048, Training Time: 143 seconds)
Epoch:  14/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.053, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.049, Training Time: 196 seconds)
Epoch:  14/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.055, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.050, Training Time: 251 seconds)
Epoch:  14/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.075, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.054, Training Time: 310 seconds)
Epoch:  14/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.050, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.054, Training Time: 371 seconds)
Epoch:  14/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.044, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.053, Training Time: 403 seconds)
Epoch:  14/100, Validation loss:  7.157, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.968 to  0.966
Training loss improved!
Epoch:  15/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.039, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.039, Training Time: 45 seconds)
Epoch:  15/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.038, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.039, Training Time: 93 seconds)
Epoch:  15/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.031, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.036, Training Time: 143 seconds)
Epoch:  15/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.049, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.039, Training Time: 196 seconds)
Epoch:  15/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.052, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.042, Training Time: 251 seconds)
Epoch:  15/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.070, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.046, Training Time: 310 seconds)
Epoch:  15/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.043, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.046, Training Time: 371 seconds)
Epoch:  15/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.034, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.045, Training Time: 403 seconds)
Epoch:  15/100, Validation loss:  7.155, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.966 to  0.963
Training loss improved!
Epoch:  16/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.030, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.030, Training Time: 45 seconds)
Epoch:  16/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.030, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.030, Training Time: 93 seconds)
Epoch:  16/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.027, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.029, Training Time: 143 seconds)
Epoch:  16/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.046, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.033, Training Time: 196 seconds)
Epoch:  16/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.049, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.036, Training Time: 251 seconds)
Epoch:  16/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.069, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.042, Training Time: 310 seconds)
Epoch:  16/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.042, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.042, Training Time: 372 seconds)
Epoch:  16/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.028, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.041, Training Time: 403 seconds)
Epoch:  16/100, Validation loss:  7.160, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.963 to  0.961
Training loss improved!
Epoch:  17/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.024, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.024, Training Time: 45 seconds)
Epoch:  17/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.025, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.024, Training Time: 93 seconds)
Epoch:  17/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.024, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.024, Training Time: 143 seconds)
Epoch:  17/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.043, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.029, Training Time: 196 seconds)
Epoch:  17/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.046, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.032, Training Time: 251 seconds)
Epoch:  17/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.068, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.038, Training Time: 310 seconds)
Epoch:  17/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.043, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.039, Training Time: 371 seconds)
Epoch:  17/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.025, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.038, Training Time: 403 seconds)
Epoch:  17/100, Validation loss:  7.131, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.961 to  0.958
Training loss improved!
Epoch:  18/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.021, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.021, Training Time: 45 seconds)
Epoch:  18/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.022, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.021, Training Time: 93 seconds)
Epoch:  18/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.021, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.021, Training Time: 144 seconds)
Epoch:  18/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.040, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.026, Training Time: 196 seconds)
Epoch:  18/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.043, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.029, Training Time: 251 seconds)
Epoch:  18/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.064, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.035, Training Time: 310 seconds)
Epoch:  18/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.036, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.035, Training Time: 371 seconds)
Epoch:  18/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.033, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.035, Training Time: 403 seconds)
Epoch:  18/100, Validation loss:  7.157, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.958 to  0.956
Training loss improved!
Epoch:  19/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.015, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.015, Training Time: 45 seconds)
Epoch:  19/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.016, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.015, Training Time: 93 seconds)
Epoch:  19/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.017, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.016, Training Time: 144 seconds)
Epoch:  19/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.037, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.021, Training Time: 196 seconds)
Epoch:  19/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.040, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.025, Training Time: 251 seconds)
Epoch:  19/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.056, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.030, Training Time: 310 seconds)
Epoch:  19/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.020, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.029, Training Time: 371 seconds)
Epoch:  19/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.018, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.028, Training Time: 403 seconds)
Epoch:  19/100, Validation loss:  7.138, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.956 to  0.954
Training loss improved!
Epoch:  20/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.011, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.011, Training Time: 45 seconds)
Epoch:  20/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.018, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.014, Training Time: 93 seconds)
Epoch:  20/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.016, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.015, Training Time: 143 seconds)
Epoch:  20/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.035, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.020, Training Time: 196 seconds)
Epoch:  20/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.037, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.023, Training Time: 251 seconds)
Epoch:  20/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.057, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.029, Training Time: 310 seconds)
Epoch:  20/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.030, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.029, Training Time: 371 seconds)
Epoch:  20/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.023, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.029, Training Time: 403 seconds)
Epoch:  20/100, Validation loss:  7.133, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.954 to  0.951
Epoch:  21/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.011, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.011, Training Time: 45 seconds)
Epoch:  21/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.015, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.013, Training Time: 93 seconds)
Epoch:  21/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.014, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.013, Training Time: 143 seconds)
Epoch:  21/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.033, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.018, Training Time: 196 seconds)
Epoch:  21/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.036, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.022, Training Time: 251 seconds)
Epoch:  21/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.057, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.028, Training Time: 310 seconds)
Epoch:  21/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.021, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.027, Training Time: 371 seconds)
Epoch:  21/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.013, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.026, Training Time: 403 seconds)
Epoch:  21/100, Validation loss:  7.128, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.951 to  0.949
Training loss improved!
Epoch:  22/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.993, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.993, Training Time: 45 seconds)
Epoch:  22/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.006, Training Time: 48 seconds), Stats for epoch: (Training Loss:  7.000, Training Time: 93 seconds)
Epoch:  22/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.007, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.002, Training Time: 143 seconds)
Epoch:  22/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.029, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.009, Training Time: 196 seconds)
Epoch:  22/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.032, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.013, Training Time: 251 seconds)
Epoch:  22/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.048, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.019, Training Time: 310 seconds)
Epoch:  22/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.006, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.017, Training Time: 371 seconds)
Epoch:  22/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.007, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.017, Training Time: 403 seconds)
Epoch:  22/100, Validation loss:  7.141, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.949 to  0.946
Training loss improved!
Epoch:  23/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.995, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.995, Training Time: 45 seconds)
Epoch:  23/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  7.002, Training Time: 47 seconds), Stats for epoch: (Training Loss:  6.998, Training Time: 93 seconds)
Epoch:  23/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.004, Training Time: 50 seconds), Stats for epoch: (Training Loss:  7.000, Training Time: 143 seconds)
Epoch:  23/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.026, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.007, Training Time: 196 seconds)
Epoch:  23/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.030, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.011, Training Time: 251 seconds)
Epoch:  23/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.047, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.017, Training Time: 310 seconds)
Epoch:  23/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.015, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.017, Training Time: 371 seconds)
Epoch:  23/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  7.004, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.016, Training Time: 403 seconds)
Epoch:  23/100, Validation loss:  7.136, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.946 to  0.944
Training loss improved!
Epoch:  24/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.982, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.982, Training Time: 45 seconds)
Epoch:  24/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.999, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.990, Training Time: 93 seconds)
Epoch:  24/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  7.002, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.994, Training Time: 144 seconds)
Epoch:  24/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.023, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.001, Training Time: 196 seconds)
Epoch:  24/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.028, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.007, Training Time: 251 seconds)
Epoch:  24/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.044, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.013, Training Time: 310 seconds)
Epoch:  24/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.000, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.011, Training Time: 371 seconds)
Epoch:  24/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.997, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.010, Training Time: 403 seconds)
Epoch:  24/100, Validation loss:  7.152, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.944 to  0.942
Training loss improved!
Epoch:  25/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.982, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.982, Training Time: 45 seconds)
Epoch:  25/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.995, Training Time: 47 seconds), Stats for epoch: (Training Loss:  6.989, Training Time: 93 seconds)
Epoch:  25/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.999, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.992, Training Time: 143 seconds)
Epoch:  25/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.021, Training Time: 52 seconds), Stats for epoch: (Training Loss:  7.000, Training Time: 196 seconds)
Epoch:  25/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.025, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.005, Training Time: 251 seconds)
Epoch:  25/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.043, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.011, Training Time: 310 seconds)
Epoch:  25/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.997, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.009, Training Time: 371 seconds)
Epoch:  25/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.992, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.008, Training Time: 403 seconds)
Epoch:  25/100, Validation loss:  7.156, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.942 to  0.939
Training loss improved!
Epoch:  26/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.976, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.976, Training Time: 45 seconds)
Epoch:  26/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.993, Training Time: 47 seconds), Stats for epoch: (Training Loss:  6.984, Training Time: 93 seconds)
Epoch:  26/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.995, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.988, Training Time: 143 seconds)
Epoch:  26/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.018, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.995, Training Time: 196 seconds)
Epoch:  26/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.020, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.000, Training Time: 251 seconds)
Epoch:  26/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.037, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.007, Training Time: 310 seconds)
Epoch:  26/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.989, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.004, Training Time: 371 seconds)
Epoch:  26/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.987, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.003, Training Time: 403 seconds)
Epoch:  26/100, Validation loss:  7.169, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.939 to  0.937
Training loss improved!
Epoch:  27/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.969, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.969, Training Time: 45 seconds)
Epoch:  27/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.988, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.979, Training Time: 93 seconds)
Epoch:  27/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.989, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.982, Training Time: 143 seconds)
Epoch:  27/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.013, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.990, Training Time: 196 seconds)
Epoch:  27/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.017, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.995, Training Time: 251 seconds)
Epoch:  27/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.034, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.002, Training Time: 310 seconds)
Epoch:  27/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.984, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.999, Training Time: 371 seconds)
Epoch:  27/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.981, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.998, Training Time: 403 seconds)
Epoch:  27/100, Validation loss:  7.171, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.937 to  0.935
Training loss improved!
Epoch:  28/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.964, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.964, Training Time: 45 seconds)
Epoch:  28/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.985, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.975, Training Time: 93 seconds)
Epoch:  28/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.985, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.978, Training Time: 143 seconds)
Epoch:  28/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.009, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.986, Training Time: 196 seconds)
Epoch:  28/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.014, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.992, Training Time: 251 seconds)
Epoch:  28/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.038, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.999, Training Time: 310 seconds)
Epoch:  28/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  7.002, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.000, Training Time: 371 seconds)
Epoch:  28/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.993, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.999, Training Time: 403 seconds)
Epoch:  28/100, Validation loss:  7.158, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.935 to  0.932
Epoch:  29/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  7.000, Training Time: 45 seconds), Stats for epoch: (Training Loss:  7.000, Training Time: 45 seconds)
Epoch:  29/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.991, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.995, Training Time: 93 seconds)
Epoch:  29/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.989, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.993, Training Time: 144 seconds)
Epoch:  29/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.012, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.998, Training Time: 196 seconds)
Epoch:  29/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.013, Training Time: 55 seconds), Stats for epoch: (Training Loss:  7.001, Training Time: 251 seconds)
Epoch:  29/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.031, Training Time: 58 seconds), Stats for epoch: (Training Loss:  7.006, Training Time: 310 seconds)
Epoch:  29/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.998, Training Time: 61 seconds), Stats for epoch: (Training Loss:  7.005, Training Time: 371 seconds)
Epoch:  29/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.986, Training Time: 31 seconds), Stats for epoch: (Training Loss:  7.004, Training Time: 403 seconds)
Epoch:  29/100, Validation loss:  7.142, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.932 to  0.930
Epoch:  30/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.960, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.960, Training Time: 45 seconds)
Epoch:  30/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.978, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.969, Training Time: 93 seconds)
Epoch:  30/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.979, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.972, Training Time: 144 seconds)
Epoch:  30/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  7.004, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.980, Training Time: 196 seconds)
Epoch:  30/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.007, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.986, Training Time: 251 seconds)
Epoch:  30/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.025, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.992, Training Time: 310 seconds)
Epoch:  30/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.985, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.991, Training Time: 371 seconds)
Epoch:  30/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.966, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.989, Training Time: 403 seconds)
Epoch:  30/100, Validation loss:  7.194, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.930 to  0.928
Training loss improved!
Epoch:  31/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.959, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.959, Training Time: 45 seconds)
Epoch:  31/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.975, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.967, Training Time: 93 seconds)
Epoch:  31/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.972, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.969, Training Time: 144 seconds)
Epoch:  31/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.998, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.976, Training Time: 196 seconds)
Epoch:  31/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  7.001, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.981, Training Time: 251 seconds)
Epoch:  31/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.023, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.988, Training Time: 310 seconds)
Epoch:  31/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.972, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.986, Training Time: 371 seconds)
Epoch:  31/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.955, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.984, Training Time: 403 seconds)
Epoch:  31/100, Validation loss:  7.188, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.928 to  0.925
Training loss improved!
Epoch:  32/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.950, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.950, Training Time: 45 seconds)
Epoch:  32/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.969, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.959, Training Time: 93 seconds)
Epoch:  32/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.964, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.961, Training Time: 144 seconds)
Epoch:  32/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.991, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.969, Training Time: 196 seconds)
Epoch:  32/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.993, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.974, Training Time: 251 seconds)
Epoch:  32/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.012, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.980, Training Time: 310 seconds)
Epoch:  32/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.964, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.978, Training Time: 372 seconds)
Epoch:  32/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.948, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.976, Training Time: 403 seconds)
Epoch:  32/100, Validation loss:  7.204, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.925 to  0.923
Training loss improved!
Epoch:  33/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.941, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.941, Training Time: 45 seconds)
Epoch:  33/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.963, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.952, Training Time: 93 seconds)
Epoch:  33/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.953, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.952, Training Time: 144 seconds)
Epoch:  33/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.980, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.959, Training Time: 196 seconds)
Epoch:  33/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.982, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.964, Training Time: 251 seconds)
Epoch:  33/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  7.003, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.970, Training Time: 310 seconds)
Epoch:  33/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.951, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.968, Training Time: 371 seconds)
Epoch:  33/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.939, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.966, Training Time: 403 seconds)
Epoch:  33/100, Validation loss:  7.217, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.923 to  0.921
Training loss improved!
Epoch:  34/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.929, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.929, Training Time: 45 seconds)
Epoch:  34/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.947, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.938, Training Time: 93 seconds)
Epoch:  34/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.937, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.938, Training Time: 144 seconds)
Epoch:  34/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.965, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.945, Training Time: 196 seconds)
Epoch:  34/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.968, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.949, Training Time: 251 seconds)
Epoch:  34/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.985, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.955, Training Time: 310 seconds)
Epoch:  34/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.950, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.955, Training Time: 371 seconds)
Epoch:  34/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.948, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.954, Training Time: 403 seconds)
Epoch:  34/100, Validation loss:  7.236, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.921 to  0.918
Training loss improved!
Epoch:  35/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.918, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.918, Training Time: 45 seconds)
Epoch:  35/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.919, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.918, Training Time: 93 seconds)
Epoch:  35/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.922, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.920, Training Time: 143 seconds)
Epoch:  35/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.948, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.927, Training Time: 196 seconds)
Epoch:  35/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.946, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.931, Training Time: 251 seconds)
Epoch:  35/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.962, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.936, Training Time: 310 seconds)
Epoch:  35/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.907, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.932, Training Time: 371 seconds)
Epoch:  35/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.905, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.930, Training Time: 403 seconds)
Epoch:  35/100, Validation loss:  7.128, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.918 to  0.916
Training loss improved!
Epoch:  36/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.886, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.886, Training Time: 45 seconds)
Epoch:  36/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.860, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.873, Training Time: 93 seconds)
Epoch:  36/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.831, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.859, Training Time: 144 seconds)
Epoch:  36/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.844, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.855, Training Time: 196 seconds)
Epoch:  36/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.830, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.850, Training Time: 251 seconds)
Epoch:  36/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.833, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.847, Training Time: 310 seconds)
Epoch:  36/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.771, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.836, Training Time: 371 seconds)
Epoch:  36/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.778, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.832, Training Time: 403 seconds)
Epoch:  36/100, Validation loss:  6.974, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.916 to  0.914
Training loss improved!
Validation loss improved!
Epoch:  37/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.738, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.738, Training Time: 45 seconds)
Epoch:  37/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.714, Training Time: 47 seconds), Stats for epoch: (Training Loss:  6.726, Training Time: 93 seconds)
Epoch:  37/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.684, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.712, Training Time: 143 seconds)
Epoch:  37/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.709, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.712, Training Time: 196 seconds)
Epoch:  37/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.709, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.711, Training Time: 251 seconds)
Epoch:  37/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.717, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.712, Training Time: 310 seconds)
Epoch:  37/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.649, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.703, Training Time: 371 seconds)
Epoch:  37/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.671, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.701, Training Time: 403 seconds)
Epoch:  37/100, Validation loss:  6.804, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.914 to  0.912
Training loss improved!
Validation loss improved!
Epoch:  38/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.616, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.616, Training Time: 45 seconds)
Epoch:  38/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.614, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.615, Training Time: 93 seconds)
Epoch:  38/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.593, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.608, Training Time: 144 seconds)
Epoch:  38/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.619, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.610, Training Time: 196 seconds)
Epoch:  38/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.628, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.614, Training Time: 251 seconds)
Epoch:  38/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.645, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.619, Training Time: 310 seconds)
Epoch:  38/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.580, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.614, Training Time: 371 seconds)
Epoch:  38/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.604, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.613, Training Time: 403 seconds)
Epoch:  38/100, Validation loss:  6.755, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.912 to  0.909
Training loss improved!
Validation loss improved!
Epoch:  39/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.554, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.554, Training Time: 45 seconds)
Epoch:  39/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.548, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.551, Training Time: 93 seconds)
Epoch:  39/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.528, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.543, Training Time: 143 seconds)
Epoch:  39/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.566, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.549, Training Time: 196 seconds)
Epoch:  39/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.568, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.553, Training Time: 251 seconds)
Epoch:  39/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.590, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.559, Training Time: 310 seconds)
Epoch:  39/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.542, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.557, Training Time: 371 seconds)
Epoch:  39/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.550, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.556, Training Time: 402 seconds)
Epoch:  39/100, Validation loss:  6.737, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.909 to  0.907
Training loss improved!
Validation loss improved!
Epoch:  40/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.510, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.510, Training Time: 45 seconds)
Epoch:  40/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.506, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.508, Training Time: 93 seconds)
Epoch:  40/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.484, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.500, Training Time: 144 seconds)
Epoch:  40/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.526, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.507, Training Time: 196 seconds)
Epoch:  40/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.524, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.510, Training Time: 251 seconds)
Epoch:  40/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.549, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.517, Training Time: 310 seconds)
Epoch:  40/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.497, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.514, Training Time: 371 seconds)
Epoch:  40/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.505, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.513, Training Time: 403 seconds)
Epoch:  40/100, Validation loss:  6.719, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.907 to  0.905
Training loss improved!
Validation loss improved!
Epoch:  41/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.474, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.474, Training Time: 45 seconds)
Epoch:  41/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.462, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.468, Training Time: 93 seconds)
Epoch:  41/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.436, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.457, Training Time: 144 seconds)
Epoch:  41/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.480, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.463, Training Time: 196 seconds)
Epoch:  41/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.481, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.466, Training Time: 251 seconds)
Epoch:  41/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.498, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.472, Training Time: 310 seconds)
Epoch:  41/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.458, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.470, Training Time: 371 seconds)
Epoch:  41/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.485, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.471, Training Time: 403 seconds)
Epoch:  41/100, Validation loss:  6.776, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.905 to  0.902
Training loss improved!
Epoch:  42/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.433, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.433, Training Time: 45 seconds)
Epoch:  42/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.419, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.426, Training Time: 93 seconds)
Epoch:  42/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.389, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.414, Training Time: 144 seconds)
Epoch:  42/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.437, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.420, Training Time: 196 seconds)
Epoch:  42/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.448, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.425, Training Time: 251 seconds)
Epoch:  42/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.465, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.432, Training Time: 310 seconds)
Epoch:  42/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.415, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.429, Training Time: 371 seconds)
Epoch:  42/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.429, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.429, Training Time: 403 seconds)
Epoch:  42/100, Validation loss:  6.689, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.902 to  0.900
Training loss improved!
Validation loss improved!
Epoch:  43/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.386, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.386, Training Time: 45 seconds)
Epoch:  43/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.370, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.378, Training Time: 93 seconds)
Epoch:  43/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.346, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.367, Training Time: 144 seconds)
Epoch:  43/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.395, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.374, Training Time: 196 seconds)
Epoch:  43/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.401, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.380, Training Time: 251 seconds)
Epoch:  43/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.419, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.386, Training Time: 310 seconds)
Epoch:  43/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.368, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.384, Training Time: 371 seconds)
Epoch:  43/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.373, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.383, Training Time: 403 seconds)
Epoch:  43/100, Validation loss:  6.656, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.900 to  0.898
Training loss improved!
Validation loss improved!
Epoch:  44/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.343, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.343, Training Time: 45 seconds)
Epoch:  44/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.323, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.333, Training Time: 93 seconds)
Epoch:  44/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.296, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.320, Training Time: 144 seconds)
Epoch:  44/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.345, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.327, Training Time: 196 seconds)
Epoch:  44/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.346, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.330, Training Time: 251 seconds)
Epoch:  44/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.367, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.337, Training Time: 310 seconds)
Epoch:  44/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.319, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.334, Training Time: 371 seconds)
Epoch:  44/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.323, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.333, Training Time: 403 seconds)
Epoch:  44/100, Validation loss:  6.620, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.898 to  0.896
Training loss improved!
Validation loss improved!
Epoch:  45/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.284, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.284, Training Time: 45 seconds)
Epoch:  45/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.266, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.275, Training Time: 93 seconds)
Epoch:  45/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.239, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.263, Training Time: 143 seconds)
Epoch:  45/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.294, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.271, Training Time: 196 seconds)
Epoch:  45/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.291, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.275, Training Time: 251 seconds)
Epoch:  45/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.313, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.281, Training Time: 310 seconds)
Epoch:  45/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.264, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.279, Training Time: 371 seconds)
Epoch:  45/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.268, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.278, Training Time: 403 seconds)
Epoch:  45/100, Validation loss:  6.617, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.896 to  0.893
Training loss improved!
Validation loss improved!
Epoch:  46/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.229, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.229, Training Time: 45 seconds)
Epoch:  46/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.212, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.220, Training Time: 93 seconds)
Epoch:  46/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.181, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.207, Training Time: 144 seconds)
Epoch:  46/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.232, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.214, Training Time: 196 seconds)
Epoch:  46/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.233, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.217, Training Time: 251 seconds)
Epoch:  46/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.256, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.224, Training Time: 310 seconds)
Epoch:  46/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.213, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.222, Training Time: 371 seconds)
Epoch:  46/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.206, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.221, Training Time: 403 seconds)
Epoch:  46/100, Validation loss:  6.580, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.893 to  0.891
Training loss improved!
Validation loss improved!
Epoch:  47/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.179, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.179, Training Time: 45 seconds)
Epoch:  47/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.158, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.169, Training Time: 93 seconds)
Epoch:  47/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.124, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.154, Training Time: 144 seconds)
Epoch:  47/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.184, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.161, Training Time: 196 seconds)
Epoch:  47/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.181, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.165, Training Time: 251 seconds)
Epoch:  47/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.200, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.171, Training Time: 310 seconds)
Epoch:  47/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.156, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.169, Training Time: 371 seconds)
Epoch:  47/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.159, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.168, Training Time: 403 seconds)
Epoch:  47/100, Validation loss:  6.482, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.891 to  0.889
Training loss improved!
Validation loss improved!
Epoch:  48/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.128, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.128, Training Time: 45 seconds)
Epoch:  48/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.106, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.117, Training Time: 93 seconds)
Epoch:  48/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.077, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.104, Training Time: 144 seconds)
Epoch:  48/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.128, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.110, Training Time: 196 seconds)
Epoch:  48/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.129, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.114, Training Time: 251 seconds)
Epoch:  48/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.153, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.120, Training Time: 310 seconds)
Epoch:  48/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.107, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.118, Training Time: 371 seconds)
Epoch:  48/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.115, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.118, Training Time: 403 seconds)
Epoch:  48/100, Validation loss:  6.520, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.889 to  0.887
Training loss improved!
Epoch:  49/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.082, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.082, Training Time: 45 seconds)
Epoch:  49/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.058, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.070, Training Time: 93 seconds)
Epoch:  49/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  6.018, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.053, Training Time: 144 seconds)
Epoch:  49/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.083, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.060, Training Time: 196 seconds)
Epoch:  49/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.078, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.064, Training Time: 251 seconds)
Epoch:  49/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.107, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.071, Training Time: 310 seconds)
Epoch:  49/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.061, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.069, Training Time: 371 seconds)
Epoch:  49/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.064, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.069, Training Time: 403 seconds)
Epoch:  49/100, Validation loss:  6.484, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.887 to  0.885
Training loss improved!
Epoch:  50/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  6.030, Training Time: 45 seconds), Stats for epoch: (Training Loss:  6.030, Training Time: 45 seconds)
Epoch:  50/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  6.005, Training Time: 48 seconds), Stats for epoch: (Training Loss:  6.018, Training Time: 94 seconds)
Epoch:  50/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.965, Training Time: 50 seconds), Stats for epoch: (Training Loss:  6.000, Training Time: 144 seconds)
Epoch:  50/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  6.031, Training Time: 52 seconds), Stats for epoch: (Training Loss:  6.008, Training Time: 196 seconds)
Epoch:  50/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  6.030, Training Time: 55 seconds), Stats for epoch: (Training Loss:  6.012, Training Time: 252 seconds)
Epoch:  50/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.056, Training Time: 58 seconds), Stats for epoch: (Training Loss:  6.019, Training Time: 310 seconds)
Epoch:  50/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  6.036, Training Time: 61 seconds), Stats for epoch: (Training Loss:  6.022, Training Time: 371 seconds)
Epoch:  50/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  6.012, Training Time: 31 seconds), Stats for epoch: (Training Loss:  6.021, Training Time: 403 seconds)
Epoch:  50/100, Validation loss:  6.449, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.885 to  0.882
Training loss improved!
Validation loss improved!
Epoch:  51/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.982, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.982, Training Time: 45 seconds)
Epoch:  51/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.959, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.971, Training Time: 93 seconds)
Epoch:  51/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.918, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.953, Training Time: 144 seconds)
Epoch:  51/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.986, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.961, Training Time: 196 seconds)
Epoch:  51/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.987, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.966, Training Time: 251 seconds)
Epoch:  51/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  6.008, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.973, Training Time: 310 seconds)
Epoch:  51/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.983, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.975, Training Time: 371 seconds)
Epoch:  51/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.973, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.975, Training Time: 403 seconds)
Epoch:  51/100, Validation loss:  6.390, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.882 to  0.880
Training loss improved!
Validation loss improved!
Epoch:  52/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.941, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.941, Training Time: 45 seconds)
Epoch:  52/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.912, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.927, Training Time: 93 seconds)
Epoch:  52/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.873, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.909, Training Time: 143 seconds)
Epoch:  52/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.938, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.916, Training Time: 196 seconds)
Epoch:  52/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.933, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.919, Training Time: 251 seconds)
Epoch:  52/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.965, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.927, Training Time: 310 seconds)
Epoch:  52/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.927, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.927, Training Time: 371 seconds)
Epoch:  52/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.938, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.928, Training Time: 403 seconds)
Epoch:  52/100, Validation loss:  6.414, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.880 to  0.878
Training loss improved!
Epoch:  53/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.890, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.890, Training Time: 45 seconds)
Epoch:  53/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.866, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.878, Training Time: 93 seconds)
Epoch:  53/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.819, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.858, Training Time: 144 seconds)
Epoch:  53/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.891, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.866, Training Time: 196 seconds)
Epoch:  53/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.888, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.871, Training Time: 251 seconds)
Epoch:  53/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.916, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.878, Training Time: 310 seconds)
Epoch:  53/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.891, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.880, Training Time: 371 seconds)
Epoch:  53/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.889, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.881, Training Time: 403 seconds)
Epoch:  53/100, Validation loss:  6.351, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.878 to  0.876
Training loss improved!
Validation loss improved!
Epoch:  54/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.845, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.845, Training Time: 45 seconds)
Epoch:  54/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.819, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.832, Training Time: 93 seconds)
Epoch:  54/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.771, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.812, Training Time: 144 seconds)
Epoch:  54/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.852, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.822, Training Time: 196 seconds)
Epoch:  54/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.843, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.826, Training Time: 251 seconds)
Epoch:  54/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.873, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.834, Training Time: 310 seconds)
Epoch:  54/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.853, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.837, Training Time: 371 seconds)
Epoch:  54/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.852, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.838, Training Time: 403 seconds)
Epoch:  54/100, Validation loss:  6.340, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.876 to  0.874
Training loss improved!
Validation loss improved!
Epoch:  55/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.797, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.797, Training Time: 45 seconds)
Epoch:  55/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.770, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.784, Training Time: 93 seconds)
Epoch:  55/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.721, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.763, Training Time: 144 seconds)
Epoch:  55/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.804, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.773, Training Time: 196 seconds)
Epoch:  55/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.798, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.778, Training Time: 251 seconds)
Epoch:  55/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.827, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.786, Training Time: 310 seconds)
Epoch:  55/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.825, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.792, Training Time: 371 seconds)
Epoch:  55/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.817, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.793, Training Time: 403 seconds)
Epoch:  55/100, Validation loss:  6.398, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.874 to  0.871
Training loss improved!
Epoch:  56/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.745, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.745, Training Time: 45 seconds)
Epoch:  56/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.734, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.739, Training Time: 93 seconds)
Epoch:  56/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.678, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.719, Training Time: 143 seconds)
Epoch:  56/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.758, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.729, Training Time: 196 seconds)
Epoch:  56/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.756, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.734, Training Time: 251 seconds)
Epoch:  56/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.791, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.744, Training Time: 310 seconds)
Epoch:  56/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.759, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.746, Training Time: 371 seconds)
Epoch:  56/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.778, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.748, Training Time: 403 seconds)
Epoch:  56/100, Validation loss:  6.378, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.871 to  0.869
Training loss improved!
Epoch:  57/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.704, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.704, Training Time: 45 seconds)
Epoch:  57/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.680, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.692, Training Time: 94 seconds)
Epoch:  57/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.624, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.669, Training Time: 144 seconds)
Epoch:  57/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.719, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.682, Training Time: 196 seconds)
Epoch:  57/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.715, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.688, Training Time: 251 seconds)
Epoch:  57/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.748, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.698, Training Time: 310 seconds)
Epoch:  57/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.717, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.701, Training Time: 371 seconds)
Epoch:  57/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.750, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.704, Training Time: 403 seconds)
Epoch:  57/100, Validation loss:  6.360, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.869 to  0.867
Training loss improved!
Epoch:  58/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.656, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.656, Training Time: 45 seconds)
Epoch:  58/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.635, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.645, Training Time: 93 seconds)
Epoch:  58/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.586, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.626, Training Time: 144 seconds)
Epoch:  58/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.673, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.637, Training Time: 196 seconds)
Epoch:  58/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.672, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.644, Training Time: 251 seconds)
Epoch:  58/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.701, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.654, Training Time: 310 seconds)
Epoch:  58/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.674, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.657, Training Time: 371 seconds)
Epoch:  58/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.700, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.660, Training Time: 403 seconds)
Epoch:  58/100, Validation loss:  6.287, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.867 to  0.865
Training loss improved!
Validation loss improved!
Epoch:  59/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.606, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.606, Training Time: 45 seconds)
Epoch:  59/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.599, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.603, Training Time: 93 seconds)
Epoch:  59/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.535, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.580, Training Time: 144 seconds)
Epoch:  59/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.627, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.592, Training Time: 196 seconds)
Epoch:  59/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.631, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.600, Training Time: 251 seconds)
Epoch:  59/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.662, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.610, Training Time: 310 seconds)
Epoch:  59/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.635, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.613, Training Time: 371 seconds)
Epoch:  59/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.675, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.618, Training Time: 403 seconds)
Epoch:  59/100, Validation loss:  6.231, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.865 to  0.863
Training loss improved!
Validation loss improved!
Epoch:  60/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.559, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.559, Training Time: 45 seconds)
Epoch:  60/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.545, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.552, Training Time: 93 seconds)
Epoch:  60/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.493, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.532, Training Time: 144 seconds)
Epoch:  60/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.584, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.545, Training Time: 196 seconds)
Epoch:  60/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.593, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.555, Training Time: 251 seconds)
Epoch:  60/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.625, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.566, Training Time: 310 seconds)
Epoch:  60/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.602, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.571, Training Time: 371 seconds)
Epoch:  60/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.628, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.575, Training Time: 403 seconds)
Epoch:  60/100, Validation loss:  6.190, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.863 to  0.861
Training loss improved!
Validation loss improved!
Epoch:  61/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.516, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.516, Training Time: 45 seconds)
Epoch:  61/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.498, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.507, Training Time: 93 seconds)
Epoch:  61/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.443, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.486, Training Time: 143 seconds)
Epoch:  61/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.543, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.500, Training Time: 196 seconds)
Epoch:  61/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.549, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.510, Training Time: 251 seconds)
Epoch:  61/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.588, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.523, Training Time: 310 seconds)
Epoch:  61/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.544, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.526, Training Time: 371 seconds)
Epoch:  61/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.586, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.530, Training Time: 403 seconds)
Epoch:  61/100, Validation loss:  6.262, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.861 to  0.858
Training loss improved!
Epoch:  62/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.465, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.465, Training Time: 45 seconds)
Epoch:  62/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.454, Training Time: 47 seconds), Stats for epoch: (Training Loss:  5.459, Training Time: 93 seconds)
Epoch:  62/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.397, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.438, Training Time: 144 seconds)
Epoch:  62/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.500, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.454, Training Time: 196 seconds)
Epoch:  62/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.509, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.465, Training Time: 251 seconds)
Epoch:  62/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.550, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.479, Training Time: 310 seconds)
Epoch:  62/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.521, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.485, Training Time: 371 seconds)
Epoch:  62/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.572, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.491, Training Time: 403 seconds)
Epoch:  62/100, Validation loss:  6.143, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.858 to  0.856
Training loss improved!
Validation loss improved!
Epoch:  63/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.412, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.412, Training Time: 45 seconds)
Epoch:  63/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.404, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.408, Training Time: 93 seconds)
Epoch:  63/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.350, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.389, Training Time: 144 seconds)
Epoch:  63/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.465, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.408, Training Time: 196 seconds)
Epoch:  63/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.471, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.420, Training Time: 252 seconds)
Epoch:  63/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.506, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.435, Training Time: 310 seconds)
Epoch:  63/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.481, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.441, Training Time: 372 seconds)
Epoch:  63/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.506, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.446, Training Time: 403 seconds)
Epoch:  63/100, Validation loss:  6.166, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.856 to  0.854
Training loss improved!
Epoch:  64/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.369, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.369, Training Time: 45 seconds)
Epoch:  64/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.358, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.363, Training Time: 93 seconds)
Epoch:  64/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.301, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.342, Training Time: 144 seconds)
Epoch:  64/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.418, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.361, Training Time: 196 seconds)
Epoch:  64/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.431, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.375, Training Time: 251 seconds)
Epoch:  64/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.479, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.393, Training Time: 310 seconds)
Epoch:  64/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.450, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.401, Training Time: 371 seconds)
Epoch:  64/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.486, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.407, Training Time: 403 seconds)
Epoch:  64/100, Validation loss:  6.209, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.854 to  0.852
Training loss improved!
Epoch:  65/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.319, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.319, Training Time: 45 seconds)
Epoch:  65/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.314, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.317, Training Time: 93 seconds)
Epoch:  65/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.259, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.297, Training Time: 144 seconds)
Epoch:  65/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.372, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.316, Training Time: 196 seconds)
Epoch:  65/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.391, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.331, Training Time: 251 seconds)
Epoch:  65/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.436, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.349, Training Time: 310 seconds)
Epoch:  65/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.401, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.356, Training Time: 372 seconds)
Epoch:  65/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.433, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.361, Training Time: 403 seconds)
Epoch:  65/100, Validation loss:  6.249, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.852 to  0.850
Training loss improved!
Epoch:  66/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.272, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.272, Training Time: 45 seconds)
Epoch:  66/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.264, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.268, Training Time: 94 seconds)
Epoch:  66/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.212, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.249, Training Time: 144 seconds)
Epoch:  66/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.335, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.271, Training Time: 196 seconds)
Epoch:  66/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.350, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.287, Training Time: 251 seconds)
Epoch:  66/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.389, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.304, Training Time: 310 seconds)
Epoch:  66/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.368, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.313, Training Time: 371 seconds)
Epoch:  66/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.424, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.320, Training Time: 403 seconds)
Epoch:  66/100, Validation loss:  6.187, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.850 to  0.848
Training loss improved!
Epoch:  67/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.213, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.213, Training Time: 45 seconds)
Epoch:  67/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.220, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.217, Training Time: 94 seconds)
Epoch:  67/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.159, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.197, Training Time: 144 seconds)
Epoch:  67/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.289, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.220, Training Time: 196 seconds)
Epoch:  67/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.311, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.238, Training Time: 252 seconds)
Epoch:  67/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.352, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.257, Training Time: 311 seconds)
Epoch:  67/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.333, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.268, Training Time: 372 seconds)
Epoch:  67/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.399, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.277, Training Time: 403 seconds)
Epoch:  67/100, Validation loss:  6.311, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.848 to  0.846
Training loss improved!
Epoch:  68/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.166, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.166, Training Time: 45 seconds)
Epoch:  68/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.173, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.169, Training Time: 94 seconds)
Epoch:  68/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.120, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.153, Training Time: 144 seconds)
Epoch:  68/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.244, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.176, Training Time: 196 seconds)
Epoch:  68/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.274, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.195, Training Time: 251 seconds)
Epoch:  68/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.323, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.217, Training Time: 310 seconds)
Epoch:  68/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.307, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.230, Training Time: 372 seconds)
Epoch:  68/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.353, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.238, Training Time: 403 seconds)
Epoch:  68/100, Validation loss:  6.186, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.846 to  0.843
Training loss improved!
Epoch:  69/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.111, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.111, Training Time: 45 seconds)
Epoch:  69/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.125, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.118, Training Time: 93 seconds)
Epoch:  69/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.067, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.101, Training Time: 144 seconds)
Epoch:  69/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.201, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.126, Training Time: 196 seconds)
Epoch:  69/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.240, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.149, Training Time: 252 seconds)
Epoch:  69/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.283, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.171, Training Time: 310 seconds)
Epoch:  69/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.262, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.184, Training Time: 372 seconds)
Epoch:  69/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.320, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.193, Training Time: 403 seconds)
Epoch:  69/100, Validation loss:  6.174, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.843 to  0.841
Training loss improved!
Epoch:  70/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.068, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.068, Training Time: 45 seconds)
Epoch:  70/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.080, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.074, Training Time: 94 seconds)
Epoch:  70/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  5.025, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.057, Training Time: 144 seconds)
Epoch:  70/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.158, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.083, Training Time: 196 seconds)
Epoch:  70/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.193, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.105, Training Time: 251 seconds)
Epoch:  70/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.249, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.129, Training Time: 310 seconds)
Epoch:  70/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.227, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.143, Training Time: 371 seconds)
Epoch:  70/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.296, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.153, Training Time: 403 seconds)
Epoch:  70/100, Validation loss:  6.231, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.841 to  0.839
Training loss improved!
Epoch:  71/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  5.005, Training Time: 45 seconds), Stats for epoch: (Training Loss:  5.005, Training Time: 45 seconds)
Epoch:  71/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  5.027, Training Time: 48 seconds), Stats for epoch: (Training Loss:  5.016, Training Time: 93 seconds)
Epoch:  71/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.976, Training Time: 50 seconds), Stats for epoch: (Training Loss:  5.003, Training Time: 144 seconds)
Epoch:  71/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.111, Training Time: 52 seconds), Stats for epoch: (Training Loss:  5.030, Training Time: 196 seconds)
Epoch:  71/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.155, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.055, Training Time: 251 seconds)
Epoch:  71/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.213, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.081, Training Time: 310 seconds)
Epoch:  71/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.178, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.095, Training Time: 371 seconds)
Epoch:  71/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.233, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.105, Training Time: 403 seconds)
Epoch:  71/100, Validation loss:  6.220, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.839 to  0.837
Training loss improved!
Epoch:  72/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.960, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.960, Training Time: 45 seconds)
Epoch:  72/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.980, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.970, Training Time: 93 seconds)
Epoch:  72/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.936, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.959, Training Time: 144 seconds)
Epoch:  72/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.070, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.986, Training Time: 196 seconds)
Epoch:  72/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.113, Training Time: 55 seconds), Stats for epoch: (Training Loss:  5.012, Training Time: 251 seconds)
Epoch:  72/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.181, Training Time: 58 seconds), Stats for epoch: (Training Loss:  5.040, Training Time: 310 seconds)
Epoch:  72/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.156, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.057, Training Time: 371 seconds)
Epoch:  72/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.222, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.068, Training Time: 403 seconds)
Epoch:  72/100, Validation loss:  6.205, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.837 to  0.835
Training loss improved!
Epoch:  73/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.902, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.902, Training Time: 45 seconds)
Epoch:  73/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.939, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.920, Training Time: 93 seconds)
Epoch:  73/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.884, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.908, Training Time: 144 seconds)
Epoch:  73/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  5.022, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.937, Training Time: 196 seconds)
Epoch:  73/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.074, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.964, Training Time: 251 seconds)
Epoch:  73/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.137, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.993, Training Time: 310 seconds)
Epoch:  73/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.115, Training Time: 61 seconds), Stats for epoch: (Training Loss:  5.010, Training Time: 371 seconds)
Epoch:  73/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.179, Training Time: 31 seconds), Stats for epoch: (Training Loss:  5.022, Training Time: 403 seconds)
Epoch:  73/100, Validation loss:  6.299, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.835 to  0.833
Training loss improved!
Epoch:  74/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.851, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.851, Training Time: 45 seconds)
Epoch:  74/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.880, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.866, Training Time: 94 seconds)
Epoch:  74/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.835, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.855, Training Time: 144 seconds)
Epoch:  74/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.988, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.889, Training Time: 196 seconds)
Epoch:  74/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  5.034, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.918, Training Time: 252 seconds)
Epoch:  74/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.096, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.947, Training Time: 310 seconds)
Epoch:  74/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.098, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.969, Training Time: 372 seconds)
Epoch:  74/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.173, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.983, Training Time: 403 seconds)
Epoch:  74/100, Validation loss:  6.285, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.833 to  0.831
Training loss improved!
Epoch:  75/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.793, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.793, Training Time: 45 seconds)
Epoch:  75/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.835, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.814, Training Time: 93 seconds)
Epoch:  75/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.788, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.806, Training Time: 144 seconds)
Epoch:  75/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.940, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.839, Training Time: 196 seconds)
Epoch:  75/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.995, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.870, Training Time: 251 seconds)
Epoch:  75/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.057, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.901, Training Time: 310 seconds)
Epoch:  75/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.050, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.923, Training Time: 371 seconds)
Epoch:  75/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.118, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.936, Training Time: 403 seconds)
Epoch:  75/100, Validation loss:  6.187, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.831 to  0.829
Training loss improved!
Epoch:  76/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.751, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.751, Training Time: 45 seconds)
Epoch:  76/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.786, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.768, Training Time: 94 seconds)
Epoch:  76/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.745, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.760, Training Time: 144 seconds)
Epoch:  76/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.892, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.793, Training Time: 196 seconds)
Epoch:  76/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.964, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.827, Training Time: 251 seconds)
Epoch:  76/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  5.023, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.860, Training Time: 310 seconds)
Epoch:  76/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  5.011, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.882, Training Time: 372 seconds)
Epoch:  76/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.088, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.896, Training Time: 403 seconds)
Epoch:  76/100, Validation loss:  6.312, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.829 to  0.827
Training loss improved!
Epoch:  77/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.689, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.689, Training Time: 45 seconds)
Epoch:  77/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.738, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.714, Training Time: 93 seconds)
Epoch:  77/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.693, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.707, Training Time: 144 seconds)
Epoch:  77/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.858, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.745, Training Time: 196 seconds)
Epoch:  77/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.917, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.779, Training Time: 251 seconds)
Epoch:  77/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.989, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.814, Training Time: 310 seconds)
Epoch:  77/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.977, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.837, Training Time: 371 seconds)
Epoch:  77/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.068, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.853, Training Time: 403 seconds)
Epoch:  77/100, Validation loss:  6.241, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.827 to  0.825
Training loss improved!
Epoch:  78/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.641, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.641, Training Time: 45 seconds)
Epoch:  78/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.686, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.663, Training Time: 93 seconds)
Epoch:  78/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.652, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.660, Training Time: 144 seconds)
Epoch:  78/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.807, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.697, Training Time: 196 seconds)
Epoch:  78/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.882, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.734, Training Time: 251 seconds)
Epoch:  78/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.948, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.770, Training Time: 310 seconds)
Epoch:  78/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.952, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.796, Training Time: 371 seconds)
Epoch:  78/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  5.018, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.811, Training Time: 403 seconds)
Epoch:  78/100, Validation loss:  6.223, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.825 to  0.823
Training loss improved!
Epoch:  79/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.583, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.583, Training Time: 45 seconds)
Epoch:  79/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.636, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.609, Training Time: 93 seconds)
Epoch:  79/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.604, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.608, Training Time: 144 seconds)
Epoch:  79/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.767, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.647, Training Time: 196 seconds)
Epoch:  79/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.839, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.686, Training Time: 251 seconds)
Epoch:  79/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.919, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.725, Training Time: 310 seconds)
Epoch:  79/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.903, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.750, Training Time: 371 seconds)
Epoch:  79/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.978, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.766, Training Time: 403 seconds)
Epoch:  79/100, Validation loss:  6.256, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.823 to  0.821
Training loss improved!
Epoch:  80/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.533, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.533, Training Time: 45 seconds)
Epoch:  80/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.591, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.562, Training Time: 93 seconds)
Epoch:  80/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.554, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.559, Training Time: 144 seconds)
Epoch:  80/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.718, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.599, Training Time: 196 seconds)
Epoch:  80/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.803, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.640, Training Time: 251 seconds)
Epoch:  80/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.880, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.680, Training Time: 310 seconds)
Epoch:  80/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.878, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.708, Training Time: 371 seconds)
Epoch:  80/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.973, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.726, Training Time: 403 seconds)
Epoch:  80/100, Validation loss:  6.326, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.821 to  0.819
Training loss improved!
Epoch:  81/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.481, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.481, Training Time: 45 seconds)
Epoch:  81/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.539, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.510, Training Time: 93 seconds)
Epoch:  81/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.517, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.512, Training Time: 144 seconds)
Epoch:  81/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.678, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.554, Training Time: 196 seconds)
Epoch:  81/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.757, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.595, Training Time: 251 seconds)
Epoch:  81/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.848, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.637, Training Time: 310 seconds)
Epoch:  81/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.860, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.669, Training Time: 371 seconds)
Epoch:  81/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.930, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.687, Training Time: 403 seconds)
Epoch:  81/100, Validation loss:  6.339, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.819 to  0.816
Training loss improved!
Epoch:  82/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.431, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.431, Training Time: 45 seconds)
Epoch:  82/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.482, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.456, Training Time: 93 seconds)
Epoch:  82/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.473, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.462, Training Time: 144 seconds)
Epoch:  82/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.632, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.504, Training Time: 196 seconds)
Epoch:  82/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.720, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.547, Training Time: 251 seconds)
Epoch:  82/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.816, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.592, Training Time: 310 seconds)
Epoch:  82/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.808, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.623, Training Time: 371 seconds)
Epoch:  82/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.908, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.643, Training Time: 403 seconds)
Epoch:  82/100, Validation loss:  6.454, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.816 to  0.814
Training loss improved!
Epoch:  83/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.376, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.376, Training Time: 45 seconds)
Epoch:  83/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.440, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.408, Training Time: 93 seconds)
Epoch:  83/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.423, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.413, Training Time: 144 seconds)
Epoch:  83/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.593, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.458, Training Time: 196 seconds)
Epoch:  83/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.680, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.502, Training Time: 251 seconds)
Epoch:  83/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.766, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.546, Training Time: 310 seconds)
Epoch:  83/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.764, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.577, Training Time: 371 seconds)
Epoch:  83/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.921, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.601, Training Time: 403 seconds)
Epoch:  83/100, Validation loss:  6.324, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.814 to  0.812
Training loss improved!
Epoch:  84/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.331, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.331, Training Time: 45 seconds)
Epoch:  84/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.392, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.361, Training Time: 93 seconds)
Epoch:  84/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.374, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.366, Training Time: 144 seconds)
Epoch:  84/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.547, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.411, Training Time: 196 seconds)
Epoch:  84/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.640, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.457, Training Time: 251 seconds)
Epoch:  84/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.745, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.505, Training Time: 310 seconds)
Epoch:  84/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.762, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.541, Training Time: 371 seconds)
Epoch:  84/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.853, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.563, Training Time: 403 seconds)
Epoch:  84/100, Validation loss:  6.351, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.812 to  0.810
Training loss improved!
Epoch:  85/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.279, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.279, Training Time: 45 seconds)
Epoch:  85/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.347, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.313, Training Time: 94 seconds)
Epoch:  85/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.343, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.323, Training Time: 144 seconds)
Epoch:  85/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.511, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.370, Training Time: 197 seconds)
Epoch:  85/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.603, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.416, Training Time: 252 seconds)
Epoch:  85/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.700, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.464, Training Time: 311 seconds)
Epoch:  85/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.732, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.502, Training Time: 372 seconds)
Epoch:  85/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.839, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.525, Training Time: 404 seconds)
Epoch:  85/100, Validation loss:  6.293, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.810 to  0.808
Training loss improved!
Epoch:  86/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.227, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.227, Training Time: 45 seconds)
Epoch:  86/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.304, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.266, Training Time: 94 seconds)
Epoch:  86/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.290, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.274, Training Time: 144 seconds)
Epoch:  86/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.467, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.322, Training Time: 197 seconds)
Epoch:  86/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.569, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.372, Training Time: 252 seconds)
Epoch:  86/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.664, Training Time: 59 seconds), Stats for epoch: (Training Loss:  4.420, Training Time: 311 seconds)
Epoch:  86/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.689, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.459, Training Time: 372 seconds)
Epoch:  86/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.791, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.482, Training Time: 404 seconds)
Epoch:  86/100, Validation loss:  6.336, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.808 to  0.806
Training loss improved!
Epoch:  87/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.178, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.178, Training Time: 45 seconds)
Epoch:  87/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.249, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.214, Training Time: 94 seconds)
Epoch:  87/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.244, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.224, Training Time: 144 seconds)
Epoch:  87/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.422, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.273, Training Time: 197 seconds)
Epoch:  87/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.525, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.324, Training Time: 252 seconds)
Epoch:  87/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.639, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.376, Training Time: 311 seconds)
Epoch:  87/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.644, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.415, Training Time: 372 seconds)
Epoch:  87/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.783, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.440, Training Time: 404 seconds)
Epoch:  87/100, Validation loss:  6.476, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.806 to  0.804
Training loss improved!
Epoch:  88/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.124, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.124, Training Time: 45 seconds)
Epoch:  88/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.209, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.166, Training Time: 94 seconds)
Epoch:  88/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.202, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.178, Training Time: 144 seconds)
Epoch:  88/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.382, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.229, Training Time: 197 seconds)
Epoch:  88/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.484, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.280, Training Time: 252 seconds)
Epoch:  88/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.595, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.333, Training Time: 311 seconds)
Epoch:  88/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.629, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.375, Training Time: 372 seconds)
Epoch:  88/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.756, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.401, Training Time: 404 seconds)
Epoch:  88/100, Validation loss:  6.402, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.804 to  0.802
Training loss improved!
Epoch:  89/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.071, Training Time: 45 seconds), Stats for epoch: (Training Loss:  4.071, Training Time: 45 seconds)
Epoch:  89/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.156, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.114, Training Time: 94 seconds)
Epoch:  89/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.157, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.128, Training Time: 144 seconds)
Epoch:  89/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.338, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.181, Training Time: 197 seconds)
Epoch:  89/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.448, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.234, Training Time: 252 seconds)
Epoch:  89/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.573, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.291, Training Time: 311 seconds)
Epoch:  89/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.602, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.335, Training Time: 372 seconds)
Epoch:  89/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.719, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.362, Training Time: 404 seconds)
Epoch:  89/100, Validation loss:  6.343, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.802 to  0.800
Training loss improved!
Epoch:  90/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  4.027, Training Time: 46 seconds), Stats for epoch: (Training Loss:  4.027, Training Time: 46 seconds)
Epoch:  90/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.111, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.069, Training Time: 94 seconds)
Epoch:  90/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.111, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.083, Training Time: 144 seconds)
Epoch:  90/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.299, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.137, Training Time: 197 seconds)
Epoch:  90/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.411, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.192, Training Time: 252 seconds)
Epoch:  90/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.534, Training Time: 59 seconds), Stats for epoch: (Training Loss:  4.249, Training Time: 311 seconds)
Epoch:  90/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.561, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.294, Training Time: 373 seconds)
Epoch:  90/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.696, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.321, Training Time: 405 seconds)
Epoch:  90/100, Validation loss:  6.390, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.800 to  0.798
Training loss improved!
Epoch:  91/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  3.973, Training Time: 46 seconds), Stats for epoch: (Training Loss:  3.973, Training Time: 46 seconds)
Epoch:  91/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.067, Training Time: 48 seconds), Stats for epoch: (Training Loss:  4.020, Training Time: 94 seconds)
Epoch:  91/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.064, Training Time: 50 seconds), Stats for epoch: (Training Loss:  4.035, Training Time: 144 seconds)
Epoch:  91/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.268, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.093, Training Time: 197 seconds)
Epoch:  91/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.372, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.149, Training Time: 252 seconds)
Epoch:  91/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.493, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.206, Training Time: 311 seconds)
Epoch:  91/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.521, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.251, Training Time: 373 seconds)
Epoch:  91/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.643, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.278, Training Time: 404 seconds)
Epoch:  91/100, Validation loss:  6.399, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.798 to  0.796
Training loss improved!
Epoch:  92/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  3.930, Training Time: 45 seconds), Stats for epoch: (Training Loss:  3.930, Training Time: 45 seconds)
Epoch:  92/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  4.028, Training Time: 48 seconds), Stats for epoch: (Training Loss:  3.979, Training Time: 94 seconds)
Epoch:  92/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  4.022, Training Time: 50 seconds), Stats for epoch: (Training Loss:  3.993, Training Time: 144 seconds)
Epoch:  92/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.219, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.050, Training Time: 197 seconds)
Epoch:  92/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.340, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.108, Training Time: 252 seconds)
Epoch:  92/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.461, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.167, Training Time: 311 seconds)
Epoch:  92/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.523, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.217, Training Time: 372 seconds)
Epoch:  92/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.648, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.247, Training Time: 404 seconds)
Epoch:  92/100, Validation loss:  6.437, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.796 to  0.794
Training loss improved!
Epoch:  93/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  3.882, Training Time: 45 seconds), Stats for epoch: (Training Loss:  3.882, Training Time: 45 seconds)
Epoch:  93/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  3.983, Training Time: 48 seconds), Stats for epoch: (Training Loss:  3.933, Training Time: 94 seconds)
Epoch:  93/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  3.987, Training Time: 50 seconds), Stats for epoch: (Training Loss:  3.951, Training Time: 144 seconds)
Epoch:  93/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.176, Training Time: 52 seconds), Stats for epoch: (Training Loss:  4.007, Training Time: 197 seconds)
Epoch:  93/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.296, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.065, Training Time: 252 seconds)
Epoch:  93/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.430, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.126, Training Time: 311 seconds)
Epoch:  93/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.475, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.176, Training Time: 372 seconds)
Epoch:  93/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.596, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.205, Training Time: 404 seconds)
Epoch:  93/100, Validation loss:  6.483, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.794 to  0.792
Training loss improved!
Epoch:  94/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  3.849, Training Time: 45 seconds), Stats for epoch: (Training Loss:  3.849, Training Time: 45 seconds)
Epoch:  94/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  3.937, Training Time: 48 seconds), Stats for epoch: (Training Loss:  3.893, Training Time: 94 seconds)
Epoch:  94/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  3.949, Training Time: 50 seconds), Stats for epoch: (Training Loss:  3.912, Training Time: 144 seconds)
Epoch:  94/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.137, Training Time: 52 seconds), Stats for epoch: (Training Loss:  3.968, Training Time: 197 seconds)
Epoch:  94/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.267, Training Time: 55 seconds), Stats for epoch: (Training Loss:  4.028, Training Time: 252 seconds)
Epoch:  94/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.396, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.089, Training Time: 311 seconds)
Epoch:  94/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.446, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.140, Training Time: 372 seconds)
Epoch:  94/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.573, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.170, Training Time: 404 seconds)
Epoch:  94/100, Validation loss:  6.456, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.792 to  0.790
Training loss improved!
Epoch:  95/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  3.793, Training Time: 46 seconds), Stats for epoch: (Training Loss:  3.793, Training Time: 46 seconds)
Epoch:  95/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  3.892, Training Time: 48 seconds), Stats for epoch: (Training Loss:  3.843, Training Time: 94 seconds)
Epoch:  95/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  3.898, Training Time: 50 seconds), Stats for epoch: (Training Loss:  3.861, Training Time: 144 seconds)
Epoch:  95/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.091, Training Time: 52 seconds), Stats for epoch: (Training Loss:  3.918, Training Time: 197 seconds)
Epoch:  95/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.231, Training Time: 55 seconds), Stats for epoch: (Training Loss:  3.981, Training Time: 252 seconds)
Epoch:  95/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.361, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.044, Training Time: 311 seconds)
Epoch:  95/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.440, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.101, Training Time: 372 seconds)
Epoch:  95/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.560, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.133, Training Time: 404 seconds)
Epoch:  95/100, Validation loss:  6.476, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.790 to  0.788
Training loss improved!
Epoch:  96/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  3.752, Training Time: 45 seconds), Stats for epoch: (Training Loss:  3.752, Training Time: 45 seconds)
Epoch:  96/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  3.856, Training Time: 48 seconds), Stats for epoch: (Training Loss:  3.804, Training Time: 94 seconds)
Epoch:  96/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  3.855, Training Time: 50 seconds), Stats for epoch: (Training Loss:  3.821, Training Time: 144 seconds)
Epoch:  96/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.058, Training Time: 52 seconds), Stats for epoch: (Training Loss:  3.880, Training Time: 196 seconds)
Epoch:  96/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.200, Training Time: 55 seconds), Stats for epoch: (Training Loss:  3.944, Training Time: 251 seconds)
Epoch:  96/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.324, Training Time: 58 seconds), Stats for epoch: (Training Loss:  4.007, Training Time: 310 seconds)
Epoch:  96/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.385, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.061, Training Time: 371 seconds)
Epoch:  96/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.530, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.094, Training Time: 403 seconds)
Epoch:  96/100, Validation loss:  6.471, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.788 to  0.786
Training loss improved!
Epoch:  97/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  3.699, Training Time: 45 seconds), Stats for epoch: (Training Loss:  3.699, Training Time: 45 seconds)
Epoch:  97/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  3.805, Training Time: 48 seconds), Stats for epoch: (Training Loss:  3.752, Training Time: 93 seconds)
Epoch:  97/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  3.816, Training Time: 50 seconds), Stats for epoch: (Training Loss:  3.773, Training Time: 144 seconds)
Epoch:  97/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  4.015, Training Time: 52 seconds), Stats for epoch: (Training Loss:  3.834, Training Time: 196 seconds)
Epoch:  97/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.165, Training Time: 55 seconds), Stats for epoch: (Training Loss:  3.900, Training Time: 251 seconds)
Epoch:  97/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.297, Training Time: 58 seconds), Stats for epoch: (Training Loss:  3.966, Training Time: 310 seconds)
Epoch:  97/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.348, Training Time: 61 seconds), Stats for epoch: (Training Loss:  4.021, Training Time: 371 seconds)
Epoch:  97/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.488, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.053, Training Time: 403 seconds)
Epoch:  97/100, Validation loss:  6.558, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.786 to  0.784
Training loss improved!
Epoch:  98/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  3.651, Training Time: 45 seconds), Stats for epoch: (Training Loss:  3.651, Training Time: 45 seconds)
Epoch:  98/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  3.762, Training Time: 48 seconds), Stats for epoch: (Training Loss:  3.706, Training Time: 93 seconds)
Epoch:  98/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  3.785, Training Time: 50 seconds), Stats for epoch: (Training Loss:  3.733, Training Time: 144 seconds)
Epoch:  98/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  3.976, Training Time: 52 seconds), Stats for epoch: (Training Loss:  3.793, Training Time: 196 seconds)
Epoch:  98/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.116, Training Time: 55 seconds), Stats for epoch: (Training Loss:  3.858, Training Time: 251 seconds)
Epoch:  98/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.267, Training Time: 58 seconds), Stats for epoch: (Training Loss:  3.926, Training Time: 310 seconds)
Epoch:  98/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.324, Training Time: 61 seconds), Stats for epoch: (Training Loss:  3.983, Training Time: 371 seconds)
Epoch:  98/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.509, Training Time: 31 seconds), Stats for epoch: (Training Loss:  4.019, Training Time: 403 seconds)
Epoch:  98/100, Validation loss:  6.493, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.784 to  0.782
Training loss improved!
Epoch:  99/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  3.621, Training Time: 45 seconds), Stats for epoch: (Training Loss:  3.621, Training Time: 45 seconds)
Epoch:  99/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  3.722, Training Time: 48 seconds), Stats for epoch: (Training Loss:  3.672, Training Time: 93 seconds)
Epoch:  99/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  3.741, Training Time: 50 seconds), Stats for epoch: (Training Loss:  3.695, Training Time: 144 seconds)
Epoch:  99/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  3.932, Training Time: 52 seconds), Stats for epoch: (Training Loss:  3.754, Training Time: 196 seconds)
Epoch:  99/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.083, Training Time: 55 seconds), Stats for epoch: (Training Loss:  3.820, Training Time: 251 seconds)
Epoch:  99/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.226, Training Time: 58 seconds), Stats for epoch: (Training Loss:  3.888, Training Time: 310 seconds)
Epoch:  99/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.304, Training Time: 61 seconds), Stats for epoch: (Training Loss:  3.947, Training Time: 372 seconds)
Epoch:  99/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.431, Training Time: 31 seconds), Stats for epoch: (Training Loss:  3.980, Training Time: 403 seconds)
Epoch:  99/100, Validation loss:  6.522, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.782 to  0.781
Training loss improved!
Epoch: 100/100, Batch:  100/752, Stats for last 100 batches: (Training Loss:  3.571, Training Time: 45 seconds), Stats for epoch: (Training Loss:  3.571, Training Time: 45 seconds)
Epoch: 100/100, Batch:  200/752, Stats for last 100 batches: (Training Loss:  3.696, Training Time: 48 seconds), Stats for epoch: (Training Loss:  3.633, Training Time: 93 seconds)
Epoch: 100/100, Batch:  300/752, Stats for last 100 batches: (Training Loss:  3.704, Training Time: 50 seconds), Stats for epoch: (Training Loss:  3.657, Training Time: 144 seconds)
Epoch: 100/100, Batch:  400/752, Stats for last 100 batches: (Training Loss:  3.905, Training Time: 52 seconds), Stats for epoch: (Training Loss:  3.719, Training Time: 196 seconds)
Epoch: 100/100, Batch:  500/752, Stats for last 100 batches: (Training Loss:  4.043, Training Time: 55 seconds), Stats for epoch: (Training Loss:  3.784, Training Time: 251 seconds)
Epoch: 100/100, Batch:  600/752, Stats for last 100 batches: (Training Loss:  4.196, Training Time: 58 seconds), Stats for epoch: (Training Loss:  3.852, Training Time: 310 seconds)
Epoch: 100/100, Batch:  700/752, Stats for last 100 batches: (Training Loss:  4.264, Training Time: 61 seconds), Stats for epoch: (Training Loss:  3.911, Training Time: 371 seconds)
Epoch: 100/100, Batch:  752/752, Stats for last 52 batches: (Training Loss:  4.437, Training Time: 31 seconds), Stats for epoch: (Training Loss:  3.947, Training Time: 403 seconds)
Epoch: 100/100, Validation loss:  6.582, Batch Validation Time: 32 seconds
Learning rate decay: adjusting from  0.781 to  0.779
Training loss improved!
Training Complete!
