
Reading dataset 'csv'...

Final shared vocab size: 10410

Splitting 2542 samples into training & validation sets (20.0% used for validation)...
Training set: 2034 samples. Validation set: 508 samples.
Sorting training & validation sets to increase training efficiency...
Initializing model...


Creating checkpoint batch files...
Initializing training...
Epochs: 200
Batch Size: 128
Optimizer: sgd
Epoch:   1/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  9.097, Training Time: 7 seconds), Stats for epoch: (Training Loss:  9.097, Training Time: 7 seconds)
Epoch:   1/200, Validation loss:  8.722, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  1.000 to  0.998
Training loss improved!
Validation loss improved!
Epoch:   2/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  9.496, Training Time: 4 seconds), Stats for epoch: (Training Loss:  9.496, Training Time: 4 seconds)
Epoch:   2/200, Validation loss:  8.566, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.998 to  0.995
Validation loss improved!
Epoch:   3/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  8.986, Training Time: 4 seconds), Stats for epoch: (Training Loss:  8.986, Training Time: 4 seconds)
Epoch:   3/200, Validation loss: 10.640, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.995 to  0.993
Training loss improved!
Epoch:   4/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  8.725, Training Time: 4 seconds), Stats for epoch: (Training Loss:  8.725, Training Time: 4 seconds)
Epoch:   4/200, Validation loss:  8.559, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.993 to  0.990
Training loss improved!
Validation loss improved!
Epoch:   5/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  8.223, Training Time: 4 seconds), Stats for epoch: (Training Loss:  8.223, Training Time: 4 seconds)
Epoch:   5/200, Validation loss:  8.105, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.990 to  0.988
Training loss improved!
Validation loss improved!
Epoch:   6/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  8.085, Training Time: 4 seconds), Stats for epoch: (Training Loss:  8.085, Training Time: 4 seconds)
Epoch:   6/200, Validation loss:  8.059, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.988 to  0.985
Training loss improved!
Validation loss improved!
Epoch:   7/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  8.032, Training Time: 4 seconds), Stats for epoch: (Training Loss:  8.032, Training Time: 4 seconds)
Epoch:   7/200, Validation loss:  8.041, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.985 to  0.983
Training loss improved!
Validation loss improved!
Epoch:   8/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.979, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.979, Training Time: 4 seconds)
Epoch:   8/200, Validation loss:  8.018, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.983 to  0.980
Training loss improved!
Validation loss improved!
Epoch:   9/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.944, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.944, Training Time: 4 seconds)
Epoch:   9/200, Validation loss:  8.007, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.980 to  0.978
Training loss improved!
Validation loss improved!
Epoch:  10/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.906, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.906, Training Time: 4 seconds)
Epoch:  10/200, Validation loss:  7.998, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.978 to  0.975
Training loss improved!
Validation loss improved!
Epoch:  11/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.874, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.874, Training Time: 4 seconds)
Epoch:  11/200, Validation loss:  7.993, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.975 to  0.973
Training loss improved!
Validation loss improved!
Epoch:  12/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.844, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.844, Training Time: 4 seconds)
Epoch:  12/200, Validation loss:  7.991, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.973 to  0.970
Training loss improved!
Validation loss improved!
Epoch:  13/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.815, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.815, Training Time: 4 seconds)
Epoch:  13/200, Validation loss:  7.992, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.970 to  0.968
Training loss improved!
Epoch:  14/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.787, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.787, Training Time: 4 seconds)
Epoch:  14/200, Validation loss:  7.994, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.968 to  0.966
Training loss improved!
Epoch:  15/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.757, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.757, Training Time: 4 seconds)
Epoch:  15/200, Validation loss:  8.000, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.966 to  0.963
Training loss improved!
Epoch:  16/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.727, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.727, Training Time: 4 seconds)
Epoch:  16/200, Validation loss:  8.012, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.963 to  0.961
Training loss improved!
Epoch:  17/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.700, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.700, Training Time: 4 seconds)
Epoch:  17/200, Validation loss:  8.026, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.961 to  0.958
Training loss improved!
Epoch:  18/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.683, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.683, Training Time: 4 seconds)
Epoch:  18/200, Validation loss:  8.130, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.958 to  0.956
Training loss improved!
Epoch:  19/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.677, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.677, Training Time: 4 seconds)
Epoch:  19/200, Validation loss:  8.062, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.956 to  0.954
Training loss improved!
Epoch:  20/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.637, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.637, Training Time: 4 seconds)
Epoch:  20/200, Validation loss:  8.062, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.954 to  0.951
Training loss improved!
Epoch:  21/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.617, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.617, Training Time: 4 seconds)
Epoch:  21/200, Validation loss:  8.183, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.951 to  0.949
Training loss improved!
Epoch:  22/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.583, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.583, Training Time: 4 seconds)
Epoch:  22/200, Validation loss:  8.297, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.949 to  0.946
Training loss improved!
Epoch:  23/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.573, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.573, Training Time: 4 seconds)
Epoch:  23/200, Validation loss:  8.141, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.946 to  0.944
Training loss improved!
Epoch:  24/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.513, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.513, Training Time: 4 seconds)
Epoch:  24/200, Validation loss:  8.227, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.944 to  0.942
Training loss improved!
Epoch:  25/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.524, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.524, Training Time: 4 seconds)
Epoch:  25/200, Validation loss:  8.258, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.942 to  0.939
Epoch:  26/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.466, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.466, Training Time: 4 seconds)
Epoch:  26/200, Validation loss:  8.238, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.939 to  0.937
Training loss improved!
Epoch:  27/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.450, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.450, Training Time: 4 seconds)
Epoch:  27/200, Validation loss:  8.208, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.937 to  0.935
Training loss improved!
Epoch:  28/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.426, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.426, Training Time: 4 seconds)
Epoch:  28/200, Validation loss:  8.214, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.935 to  0.932
Training loss improved!
Epoch:  29/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.429, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.429, Training Time: 4 seconds)
Epoch:  29/200, Validation loss:  8.209, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.932 to  0.930
Epoch:  30/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.378, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.378, Training Time: 4 seconds)
Epoch:  30/200, Validation loss:  8.450, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.930 to  0.928
Training loss improved!
Epoch:  31/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.432, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.432, Training Time: 4 seconds)
Epoch:  31/200, Validation loss:  8.387, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.928 to  0.925
Epoch:  32/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.342, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.342, Training Time: 4 seconds)
Epoch:  32/200, Validation loss:  8.316, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.925 to  0.923
Training loss improved!
Epoch:  33/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.290, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.290, Training Time: 4 seconds)
Epoch:  33/200, Validation loss:  8.314, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.923 to  0.921
Training loss improved!
Epoch:  34/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.270, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.270, Training Time: 4 seconds)
Epoch:  34/200, Validation loss:  8.418, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.921 to  0.918
Training loss improved!
Epoch:  35/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.244, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.244, Training Time: 4 seconds)
Epoch:  35/200, Validation loss:  8.451, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.918 to  0.916
Training loss improved!
Epoch:  36/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.270, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.270, Training Time: 4 seconds)
Epoch:  36/200, Validation loss:  8.482, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.916 to  0.914
Epoch:  37/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.295, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.295, Training Time: 4 seconds)
Epoch:  37/200, Validation loss:  8.513, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.914 to  0.912
Epoch:  38/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.270, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.270, Training Time: 4 seconds)
Epoch:  38/200, Validation loss:  8.396, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.912 to  0.909
Epoch:  39/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.163, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.163, Training Time: 4 seconds)
Epoch:  39/200, Validation loss:  8.455, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.909 to  0.907
Training loss improved!
Epoch:  40/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.166, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.166, Training Time: 4 seconds)
Epoch:  40/200, Validation loss:  8.476, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.907 to  0.905
Epoch:  41/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.088, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.088, Training Time: 4 seconds)
Epoch:  41/200, Validation loss:  8.451, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.905 to  0.902
Training loss improved!
Epoch:  42/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.024, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.024, Training Time: 4 seconds)
Epoch:  42/200, Validation loss:  8.640, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.902 to  0.900
Training loss improved!
Epoch:  43/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.196, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.196, Training Time: 4 seconds)
Epoch:  43/200, Validation loss:  8.836, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.900 to  0.898
Epoch:  44/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.068, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.068, Training Time: 4 seconds)
Epoch:  44/200, Validation loss:  8.683, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.898 to  0.896
Epoch:  45/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.020, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.020, Training Time: 4 seconds)
Epoch:  45/200, Validation loss:  8.583, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.896 to  0.893
Training loss improved!
Epoch:  46/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.960, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.960, Training Time: 4 seconds)
Epoch:  46/200, Validation loss:  8.547, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.893 to  0.891
Training loss improved!
Epoch:  47/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.857, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.857, Training Time: 4 seconds)
Epoch:  47/200, Validation loss:  8.584, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.891 to  0.889
Training loss improved!
Epoch:  48/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.020, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.020, Training Time: 4 seconds)
Epoch:  48/200, Validation loss:  8.829, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.889 to  0.887
Epoch:  49/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.911, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.911, Training Time: 4 seconds)
Epoch:  49/200, Validation loss:  8.800, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.887 to  0.885
Epoch:  50/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.903, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.903, Training Time: 4 seconds)
Epoch:  50/200, Validation loss:  8.939, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.885 to  0.882
Epoch:  51/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.164, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.164, Training Time: 4 seconds)
Epoch:  51/200, Validation loss:  8.903, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.882 to  0.880
Epoch:  52/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  7.059, Training Time: 4 seconds), Stats for epoch: (Training Loss:  7.059, Training Time: 4 seconds)
Epoch:  52/200, Validation loss:  8.910, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.880 to  0.878
Epoch:  53/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.845, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.845, Training Time: 4 seconds)
Epoch:  53/200, Validation loss:  8.838, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.878 to  0.876
Training loss improved!
Epoch:  54/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.822, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.822, Training Time: 4 seconds)
Epoch:  54/200, Validation loss:  8.993, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.876 to  0.874
Training loss improved!
Epoch:  55/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.875, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.875, Training Time: 4 seconds)
Epoch:  55/200, Validation loss:  8.935, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.874 to  0.871
Epoch:  56/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.882, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.882, Training Time: 4 seconds)
Epoch:  56/200, Validation loss:  9.007, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.871 to  0.869
Epoch:  57/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.760, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.760, Training Time: 4 seconds)
Epoch:  57/200, Validation loss:  8.883, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.869 to  0.867
Training loss improved!
Epoch:  58/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.687, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.687, Training Time: 4 seconds)
Epoch:  58/200, Validation loss:  8.977, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.867 to  0.865
Training loss improved!
Epoch:  59/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.692, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.692, Training Time: 4 seconds)
Epoch:  59/200, Validation loss:  8.924, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.865 to  0.863
Epoch:  60/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.770, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.770, Training Time: 4 seconds)
Epoch:  60/200, Validation loss:  9.209, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.863 to  0.861
Epoch:  61/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.637, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.637, Training Time: 4 seconds)
Epoch:  61/200, Validation loss:  9.162, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.861 to  0.858
Training loss improved!
Epoch:  62/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.615, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.615, Training Time: 4 seconds)
Epoch:  62/200, Validation loss:  9.091, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.858 to  0.856
Training loss improved!
Epoch:  63/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.613, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.613, Training Time: 4 seconds)
Epoch:  63/200, Validation loss:  9.119, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.856 to  0.854
Training loss improved!
Epoch:  64/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.676, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.676, Training Time: 4 seconds)
Epoch:  64/200, Validation loss:  9.117, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.854 to  0.852
Epoch:  65/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.584, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.584, Training Time: 4 seconds)
Epoch:  65/200, Validation loss:  9.216, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.852 to  0.850
Training loss improved!
Epoch:  66/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.640, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.640, Training Time: 4 seconds)
Epoch:  66/200, Validation loss:  9.008, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.850 to  0.848
Epoch:  67/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.519, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.519, Training Time: 4 seconds)
Epoch:  67/200, Validation loss:  9.108, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.848 to  0.846
Training loss improved!
Epoch:  68/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.453, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.453, Training Time: 4 seconds)
Epoch:  68/200, Validation loss:  9.165, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.846 to  0.843
Training loss improved!
Epoch:  69/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.502, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.502, Training Time: 4 seconds)
Epoch:  69/200, Validation loss:  9.105, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.843 to  0.841
Epoch:  70/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.478, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.478, Training Time: 4 seconds)
Epoch:  70/200, Validation loss:  9.366, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.841 to  0.839
Epoch:  71/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.614, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.614, Training Time: 4 seconds)
Epoch:  71/200, Validation loss:  9.231, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.839 to  0.837
Epoch:  72/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.449, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.449, Training Time: 4 seconds)
Epoch:  72/200, Validation loss:  9.244, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.837 to  0.835
Training loss improved!
Epoch:  73/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.480, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.480, Training Time: 4 seconds)
Epoch:  73/200, Validation loss:  9.240, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.835 to  0.833
Epoch:  74/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.351, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.351, Training Time: 4 seconds)
Epoch:  74/200, Validation loss:  9.469, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.833 to  0.831
Training loss improved!
Epoch:  75/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.424, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.424, Training Time: 4 seconds)
Epoch:  75/200, Validation loss:  9.334, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.831 to  0.829
Epoch:  76/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.361, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.361, Training Time: 4 seconds)
Epoch:  76/200, Validation loss:  9.719, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.829 to  0.827
Epoch:  77/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.545, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.545, Training Time: 4 seconds)
Epoch:  77/200, Validation loss:  9.641, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.827 to  0.825
Epoch:  78/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.434, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.434, Training Time: 4 seconds)
Epoch:  78/200, Validation loss: 10.114, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.825 to  0.823
Epoch:  79/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.645, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.645, Training Time: 4 seconds)
Epoch:  79/200, Validation loss:  9.325, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.823 to  0.821
Epoch:  80/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.280, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.280, Training Time: 4 seconds)
Epoch:  80/200, Validation loss:  9.423, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.821 to  0.819
Training loss improved!
Epoch:  81/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.357, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.357, Training Time: 4 seconds)
Epoch:  81/200, Validation loss:  9.345, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.819 to  0.816
Epoch:  82/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.195, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.195, Training Time: 4 seconds)
Epoch:  82/200, Validation loss:  9.735, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.816 to  0.814
Training loss improved!
Epoch:  83/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.212, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.212, Training Time: 4 seconds)
Epoch:  83/200, Validation loss:  9.633, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.814 to  0.812
Epoch:  84/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.262, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.262, Training Time: 4 seconds)
Epoch:  84/200, Validation loss: 10.153, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.812 to  0.810
Epoch:  85/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.286, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.286, Training Time: 4 seconds)
Epoch:  85/200, Validation loss:  9.511, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.810 to  0.808
Epoch:  86/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.154, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.154, Training Time: 4 seconds)
Epoch:  86/200, Validation loss:  9.567, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.808 to  0.806
Training loss improved!
Epoch:  87/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.147, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.147, Training Time: 4 seconds)
Epoch:  87/200, Validation loss:  9.451, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.806 to  0.804
Training loss improved!
Epoch:  88/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.995, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.995, Training Time: 4 seconds)
Epoch:  88/200, Validation loss:  9.552, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.804 to  0.802
Training loss improved!
Epoch:  89/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.044, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.044, Training Time: 4 seconds)
Epoch:  89/200, Validation loss:  9.496, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.802 to  0.800
Epoch:  90/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.148, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.148, Training Time: 4 seconds)
Epoch:  90/200, Validation loss:  9.846, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.800 to  0.798
Epoch:  91/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.169, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.169, Training Time: 4 seconds)
Epoch:  91/200, Validation loss:  9.632, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.798 to  0.796
Epoch:  92/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.025, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.025, Training Time: 4 seconds)
Epoch:  92/200, Validation loss:  9.574, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.796 to  0.794
Epoch:  93/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.943, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.943, Training Time: 4 seconds)
Epoch:  93/200, Validation loss:  9.555, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.794 to  0.792
Training loss improved!
Epoch:  94/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.963, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.963, Training Time: 4 seconds)
Epoch:  94/200, Validation loss:  9.796, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.792 to  0.790
Epoch:  95/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.050, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.050, Training Time: 4 seconds)
Epoch:  95/200, Validation loss:  9.653, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.790 to  0.788
Epoch:  96/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.859, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.859, Training Time: 4 seconds)
Epoch:  96/200, Validation loss:  9.735, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.788 to  0.786
Training loss improved!
Epoch:  97/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.944, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.944, Training Time: 4 seconds)
Epoch:  97/200, Validation loss: 10.153, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.786 to  0.784
Epoch:  98/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.946, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.946, Training Time: 4 seconds)
Epoch:  98/200, Validation loss:  9.786, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.784 to  0.782
Epoch:  99/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  6.331, Training Time: 4 seconds), Stats for epoch: (Training Loss:  6.331, Training Time: 4 seconds)
Epoch:  99/200, Validation loss: 10.027, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.782 to  0.781
Epoch: 100/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.958, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.958, Training Time: 4 seconds)
Epoch: 100/200, Validation loss:  9.944, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.781 to  0.779
Epoch: 101/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.900, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.900, Training Time: 4 seconds)
Epoch: 101/200, Validation loss:  9.765, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.779 to  0.777
Epoch: 102/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.913, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.913, Training Time: 4 seconds)
Epoch: 102/200, Validation loss:  9.780, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.777 to  0.775
Epoch: 103/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.881, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.881, Training Time: 4 seconds)
Epoch: 103/200, Validation loss:  9.678, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.775 to  0.773
Epoch: 104/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.713, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.713, Training Time: 4 seconds)
Epoch: 104/200, Validation loss:  9.786, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.773 to  0.771
Training loss improved!
Epoch: 105/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.789, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.789, Training Time: 4 seconds)
Epoch: 105/200, Validation loss: 10.010, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.771 to  0.769
Epoch: 106/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.964, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.964, Training Time: 4 seconds)
Epoch: 106/200, Validation loss:  9.820, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.769 to  0.767
Epoch: 107/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.678, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.678, Training Time: 4 seconds)
Epoch: 107/200, Validation loss: 10.093, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.767 to  0.765
Training loss improved!
Epoch: 108/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.692, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.692, Training Time: 4 seconds)
Epoch: 108/200, Validation loss:  9.965, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.765 to  0.763
Epoch: 109/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.644, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.644, Training Time: 4 seconds)
Epoch: 109/200, Validation loss: 10.069, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.763 to  0.761
Training loss improved!
Epoch: 110/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.687, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.687, Training Time: 4 seconds)
Epoch: 110/200, Validation loss: 10.622, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.761 to  0.759
Epoch: 111/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.705, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.705, Training Time: 4 seconds)
Epoch: 111/200, Validation loss: 10.156, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.759 to  0.757
Epoch: 112/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.743, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.743, Training Time: 4 seconds)
Epoch: 112/200, Validation loss:  9.986, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.757 to  0.756
Epoch: 113/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.776, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.776, Training Time: 4 seconds)
Epoch: 113/200, Validation loss: 10.037, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.756 to  0.754
Epoch: 114/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.654, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.654, Training Time: 4 seconds)
Epoch: 114/200, Validation loss:  9.958, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.754 to  0.752
Epoch: 115/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.686, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.686, Training Time: 4 seconds)
Epoch: 115/200, Validation loss: 10.071, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.752 to  0.750
Epoch: 116/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.575, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.575, Training Time: 4 seconds)
Epoch: 116/200, Validation loss: 10.157, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.750 to  0.748
Training loss improved!
Epoch: 117/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.557, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.557, Training Time: 4 seconds)
Epoch: 117/200, Validation loss: 10.002, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.748 to  0.746
Training loss improved!
Epoch: 118/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.528, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.528, Training Time: 4 seconds)
Epoch: 118/200, Validation loss: 10.212, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.746 to  0.744
Training loss improved!
Epoch: 119/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.558, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.558, Training Time: 4 seconds)
Epoch: 119/200, Validation loss: 10.033, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.744 to  0.742
Epoch: 120/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.479, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.479, Training Time: 4 seconds)
Epoch: 120/200, Validation loss: 10.066, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.742 to  0.741
Training loss improved!
Epoch: 121/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.455, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.455, Training Time: 4 seconds)
Epoch: 121/200, Validation loss: 10.179, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.741 to  0.739
Training loss improved!
Epoch: 122/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.464, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.464, Training Time: 4 seconds)
Epoch: 122/200, Validation loss: 10.246, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.739 to  0.737
Epoch: 123/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.409, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.409, Training Time: 4 seconds)
Epoch: 123/200, Validation loss: 10.170, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.737 to  0.735
Training loss improved!
Epoch: 124/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.488, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.488, Training Time: 4 seconds)
Epoch: 124/200, Validation loss: 10.227, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.735 to  0.733
Epoch: 125/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.554, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.554, Training Time: 4 seconds)
Epoch: 125/200, Validation loss: 10.361, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.733 to  0.731
Epoch: 126/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.538, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.538, Training Time: 4 seconds)
Epoch: 126/200, Validation loss: 10.410, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.731 to  0.730
Epoch: 127/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.407, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.407, Training Time: 4 seconds)
Epoch: 127/200, Validation loss: 10.782, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.730 to  0.728
Training loss improved!
Epoch: 128/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.521, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.521, Training Time: 4 seconds)
Epoch: 128/200, Validation loss: 10.297, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.728 to  0.726
Epoch: 129/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.725, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.725, Training Time: 4 seconds)
Epoch: 129/200, Validation loss: 10.245, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.726 to  0.724
Epoch: 130/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.435, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.435, Training Time: 4 seconds)
Epoch: 130/200, Validation loss: 10.327, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.724 to  0.722
Epoch: 131/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.429, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.429, Training Time: 4 seconds)
Epoch: 131/200, Validation loss: 10.171, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.722 to  0.720
Epoch: 132/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.246, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.246, Training Time: 4 seconds)
Epoch: 132/200, Validation loss: 10.258, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.720 to  0.719
Training loss improved!
Epoch: 133/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.258, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.258, Training Time: 4 seconds)
Epoch: 133/200, Validation loss: 10.191, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.719 to  0.717
Epoch: 134/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.251, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.251, Training Time: 4 seconds)
Epoch: 134/200, Validation loss: 10.394, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.717 to  0.715
Epoch: 135/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.334, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.334, Training Time: 4 seconds)
Epoch: 135/200, Validation loss: 10.453, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.715 to  0.713
Epoch: 136/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.423, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.423, Training Time: 4 seconds)
Epoch: 136/200, Validation loss: 10.445, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.713 to  0.711
Epoch: 137/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.339, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.339, Training Time: 4 seconds)
Epoch: 137/200, Validation loss: 10.564, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.711 to  0.710
Epoch: 138/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.186, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.186, Training Time: 4 seconds)
Epoch: 138/200, Validation loss: 10.575, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.710 to  0.708
Training loss improved!
Epoch: 139/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.401, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.401, Training Time: 4 seconds)
Epoch: 139/200, Validation loss: 10.538, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.708 to  0.706
Epoch: 140/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.254, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.254, Training Time: 4 seconds)
Epoch: 140/200, Validation loss: 10.496, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.706 to  0.704
Epoch: 141/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.352, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.352, Training Time: 4 seconds)
Epoch: 141/200, Validation loss: 10.548, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.704 to  0.703
Epoch: 142/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.260, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.260, Training Time: 4 seconds)
Epoch: 142/200, Validation loss: 10.729, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.703 to  0.701
Epoch: 143/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.143, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.143, Training Time: 4 seconds)
Epoch: 143/200, Validation loss: 10.533, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.701 to  0.699
Training loss improved!
Epoch: 144/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.158, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.158, Training Time: 4 seconds)
Epoch: 144/200, Validation loss: 10.534, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.699 to  0.697
Epoch: 145/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.198, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.198, Training Time: 4 seconds)
Epoch: 145/200, Validation loss: 10.496, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.697 to  0.696
Epoch: 146/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.186, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.186, Training Time: 4 seconds)
Epoch: 146/200, Validation loss: 10.753, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.696 to  0.694
Epoch: 147/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.145, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.145, Training Time: 4 seconds)
Epoch: 147/200, Validation loss: 10.554, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.694 to  0.692
Epoch: 148/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.134, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.134, Training Time: 4 seconds)
Epoch: 148/200, Validation loss: 10.564, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.692 to  0.690
Training loss improved!
Epoch: 149/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.109, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.109, Training Time: 4 seconds)
Epoch: 149/200, Validation loss: 10.582, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.690 to  0.689
Training loss improved!
Epoch: 150/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.185, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.185, Training Time: 4 seconds)
Epoch: 150/200, Validation loss: 10.584, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.689 to  0.687
Epoch: 151/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.140, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.140, Training Time: 4 seconds)
Epoch: 151/200, Validation loss: 10.553, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.687 to  0.685
Epoch: 152/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.033, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.033, Training Time: 4 seconds)
Epoch: 152/200, Validation loss: 10.575, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.685 to  0.684
Training loss improved!
Epoch: 153/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.018, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.018, Training Time: 4 seconds)
Epoch: 153/200, Validation loss: 10.607, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.684 to  0.682
Training loss improved!
Epoch: 154/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.049, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.049, Training Time: 4 seconds)
Epoch: 154/200, Validation loss: 10.766, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.682 to  0.680
Epoch: 155/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.127, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.127, Training Time: 4 seconds)
Epoch: 155/200, Validation loss: 10.870, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.680 to  0.678
Epoch: 156/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.122, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.122, Training Time: 4 seconds)
Epoch: 156/200, Validation loss: 10.688, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.678 to  0.677
Epoch: 157/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.052, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.052, Training Time: 4 seconds)
Epoch: 157/200, Validation loss: 10.823, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.677 to  0.675
Epoch: 158/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.969, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.969, Training Time: 4 seconds)
Epoch: 158/200, Validation loss: 10.972, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.675 to  0.673
Training loss improved!
Epoch: 159/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.005, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.005, Training Time: 4 seconds)
Epoch: 159/200, Validation loss: 10.660, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.673 to  0.672
Epoch: 160/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.012, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.012, Training Time: 4 seconds)
Epoch: 160/200, Validation loss: 10.835, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.672 to  0.670
Epoch: 161/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.015, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.015, Training Time: 4 seconds)
Epoch: 161/200, Validation loss: 10.880, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.670 to  0.668
Epoch: 162/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.132, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.132, Training Time: 4 seconds)
Epoch: 162/200, Validation loss: 10.734, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.668 to  0.667
Epoch: 163/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.022, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.022, Training Time: 4 seconds)
Epoch: 163/200, Validation loss: 10.691, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.667 to  0.665
Epoch: 164/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.015, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.015, Training Time: 4 seconds)
Epoch: 164/200, Validation loss: 10.806, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.665 to  0.663
Epoch: 165/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.878, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.878, Training Time: 4 seconds)
Epoch: 165/200, Validation loss: 10.845, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.663 to  0.662
Training loss improved!
Epoch: 166/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.904, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.904, Training Time: 4 seconds)
Epoch: 166/200, Validation loss: 10.914, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.662 to  0.660
Epoch: 167/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.076, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.076, Training Time: 4 seconds)
Epoch: 167/200, Validation loss: 10.802, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.660 to  0.658
Epoch: 168/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.877, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.877, Training Time: 4 seconds)
Epoch: 168/200, Validation loss: 10.839, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.658 to  0.657
Training loss improved!
Epoch: 169/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.931, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.931, Training Time: 4 seconds)
Epoch: 169/200, Validation loss: 10.822, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.657 to  0.655
Epoch: 170/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  5.050, Training Time: 4 seconds), Stats for epoch: (Training Loss:  5.050, Training Time: 4 seconds)
Epoch: 170/200, Validation loss: 10.713, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.655 to  0.653
Epoch: 171/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.924, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.924, Training Time: 4 seconds)
Epoch: 171/200, Validation loss: 10.805, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.653 to  0.652
Epoch: 172/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.856, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.856, Training Time: 4 seconds)
Epoch: 172/200, Validation loss: 10.759, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.652 to  0.650
Training loss improved!
Epoch: 173/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.813, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.813, Training Time: 4 seconds)
Epoch: 173/200, Validation loss: 10.870, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.650 to  0.649
Training loss improved!
Epoch: 174/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.927, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.927, Training Time: 4 seconds)
Epoch: 174/200, Validation loss: 10.817, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.649 to  0.647
Epoch: 175/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.780, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.780, Training Time: 4 seconds)
Epoch: 175/200, Validation loss: 10.949, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.647 to  0.645
Training loss improved!
Epoch: 176/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.796, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.796, Training Time: 4 seconds)
Epoch: 176/200, Validation loss: 10.922, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.645 to  0.644
Epoch: 177/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.889, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.889, Training Time: 4 seconds)
Epoch: 177/200, Validation loss: 10.931, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.644 to  0.642
Epoch: 178/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.768, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.768, Training Time: 4 seconds)
Epoch: 178/200, Validation loss: 11.000, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.642 to  0.640
Training loss improved!
Epoch: 179/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.805, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.805, Training Time: 4 seconds)
Epoch: 179/200, Validation loss: 11.006, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.640 to  0.639
Epoch: 180/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.804, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.804, Training Time: 4 seconds)
Epoch: 180/200, Validation loss: 11.047, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.639 to  0.637
Epoch: 181/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.834, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.834, Training Time: 4 seconds)
Epoch: 181/200, Validation loss: 11.212, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.637 to  0.636
Epoch: 182/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.824, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.824, Training Time: 4 seconds)
Epoch: 182/200, Validation loss: 11.046, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.636 to  0.634
Epoch: 183/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.843, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.843, Training Time: 4 seconds)
Epoch: 183/200, Validation loss: 11.167, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.634 to  0.633
Epoch: 184/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.794, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.794, Training Time: 4 seconds)
Epoch: 184/200, Validation loss: 11.041, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.633 to  0.631
Epoch: 185/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.759, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.759, Training Time: 4 seconds)
Epoch: 185/200, Validation loss: 11.153, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.631 to  0.629
Training loss improved!
Epoch: 186/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.797, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.797, Training Time: 4 seconds)
Epoch: 186/200, Validation loss: 11.012, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.629 to  0.628
Epoch: 187/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.732, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.732, Training Time: 4 seconds)
Epoch: 187/200, Validation loss: 11.093, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.628 to  0.626
Training loss improved!
Epoch: 188/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.690, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.690, Training Time: 4 seconds)
Epoch: 188/200, Validation loss: 11.198, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.626 to  0.625
Training loss improved!
Epoch: 189/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.763, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.763, Training Time: 4 seconds)
Epoch: 189/200, Validation loss: 11.173, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.625 to  0.623
Epoch: 190/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.697, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.697, Training Time: 4 seconds)
Epoch: 190/200, Validation loss: 11.114, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.623 to  0.622
Epoch: 191/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.863, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.863, Training Time: 4 seconds)
Epoch: 191/200, Validation loss: 11.022, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.622 to  0.620
Epoch: 192/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.736, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.736, Training Time: 4 seconds)
Epoch: 192/200, Validation loss: 11.047, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.620 to  0.618
Epoch: 193/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.786, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.786, Training Time: 4 seconds)
Epoch: 193/200, Validation loss: 10.988, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.618 to  0.617
Epoch: 194/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.650, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.650, Training Time: 4 seconds)
Epoch: 194/200, Validation loss: 10.950, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.617 to  0.615
Training loss improved!
Epoch: 195/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.608, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.608, Training Time: 4 seconds)
Epoch: 195/200, Validation loss: 11.049, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.615 to  0.614
Training loss improved!
Epoch: 196/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.640, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.640, Training Time: 4 seconds)
Epoch: 196/200, Validation loss: 11.049, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.614 to  0.612
Epoch: 197/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.545, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.545, Training Time: 4 seconds)
Epoch: 197/200, Validation loss: 11.087, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.612 to  0.611
Training loss improved!
Epoch: 198/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.512, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.512, Training Time: 4 seconds)
Epoch: 198/200, Validation loss: 11.183, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.611 to  0.609
Training loss improved!
Epoch: 199/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.546, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.546, Training Time: 4 seconds)
Epoch: 199/200, Validation loss: 11.366, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.609 to  0.608
Epoch: 200/200, Batch:   16/16, Stats for last 16 batches: (Training Loss:  4.699, Training Time: 4 seconds), Stats for epoch: (Training Loss:  4.699, Training Time: 4 seconds)
Epoch: 200/200, Validation loss: 11.205, Batch Validation Time: 0 seconds
Learning rate decay: adjusting from  0.608 to  0.606
Training Complete!
