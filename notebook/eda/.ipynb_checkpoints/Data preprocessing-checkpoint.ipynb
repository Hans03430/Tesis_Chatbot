{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hans/Documentos/Tesis_Chatbot/.env/lib/python3.8/site-packages/sqlalchemy/orm/util.py:104: SAWarning: The 'delete-orphan' cascade option requires 'delete'.\n",
      "  util.warn(\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Dict\n",
    "import sys\n",
    "\n",
    "\n",
    "from src.processing.constants import BASE_DIRECTORY\n",
    "from src.preparation.models.obtained_text import ObtainedText\n",
    "from src.preparation.models.descriptive_index import DescriptiveIndex\n",
    "from src.preparation.models.connective_index import ConnectiveIndex\n",
    "from src.preparation.models.lexical_diversity_index import LexicalDiversityIndex\n",
    "from src.preparation.models.readability_index import ReadabilityIndex\n",
    "from src.preparation.models.referential_cohesion_index import ReferentialCohesionIndex\n",
    "from src.preparation.models.syntactic_complexity_index import SyntacticComplexityIndex\n",
    "from src.preparation.models.syntactic_pattern_density_index import SyntacticPatternDensityIndex\n",
    "from src.preparation.models.word_information_index import WordInformationIndex\n",
    "from src.preparation.data_access.obtained_text_da import ObtainedTextDA\n",
    "from src.processing.text_complexity_analyzer import TextComplexityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate text complexity indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atahualpa.txt Ya ha sido procesado anteriormente.\n",
      "avelino_caceres.txt Ya ha sido procesado anteriormente.\n",
      "cartilla_fenomeno_nino.txt Ya ha sido procesado anteriormente.\n",
      "cartilla_inundaciones.txt Ya ha sido procesado anteriormente.\n",
      "enrique_meiggs.txt Ya ha sido procesado anteriormente.\n",
      "epopeya_arica.txt Ya ha sido procesado anteriormente.\n",
      "francisco_bolognesi.txt Ya ha sido procesado anteriormente.\n",
      "francisco_orellana.txt Ya ha sido procesado anteriormente.\n",
      "francisco_toledo.txt Ya ha sido procesado anteriormente.\n",
      "guaman_poma.txt Ya ha sido procesado anteriormente.\n",
      "hipolito_unanue.txt Ya ha sido procesado anteriormente.\n",
      "ines_huaylas.txt Ya ha sido procesado anteriormente.\n",
      "jorge_basadre.txt Ya ha sido procesado anteriormente.\n",
      "jorge_chavez.txt Ya ha sido procesado anteriormente.\n",
      "jose_olaya.txt Ya ha sido procesado anteriormente.\n",
      "jsanchez_carrion.txt Ya ha sido procesado anteriormente.\n",
      "juanita.txt Ya ha sido procesado anteriormente.\n",
      "leoncio_prado.txt Ya ha sido procesado anteriormente.\n",
      "manco_inca.txt Ya ha sido procesado anteriormente.\n",
      "mateo_pumacahua.txt Ya ha sido procesado anteriormente.\n",
      "mi-cuaderno-autoaprendizaje-ps-1.txt Ya ha sido procesado anteriormente.\n",
      "mi-cuaderno-autoaprendizaje-ps-2.txt Ya ha sido procesado anteriormente.\n",
      "mi-cuaderno-autoaprendizaje-ps-3.txt Ya ha sido procesado anteriormente.\n",
      "mi-cuaderno-autoaprendizaje-ps-5.txt Ya ha sido procesado anteriormente.\n",
      "micaela_bastidas.txt Ya ha sido procesado anteriormente.\n",
      "miguel_grau.txt Ya ha sido procesado anteriormente.\n",
      "nicolas_pierola.txt Ya ha sido procesado anteriormente.\n",
      "pachacutec.txt Ya ha sido procesado anteriormente.\n",
      "pedro_peralta.txt Ya ha sido procesado anteriormente.\n",
      "ramon_castilla.txt Ya ha sido procesado anteriormente.\n",
      "santa_cruz.txt Ya ha sido procesado anteriormente.\n",
      "sipan.txt Ya ha sido procesado anteriormente.\n",
      "tupac_amaru.txt Ya ha sido procesado anteriormente.\n",
      "tupac_amaru2.txt Ya ha sido procesado anteriormente.\n",
      "ahorro-energia-eficiente-tecnologias.txt Ya ha sido procesado anteriormente.\n",
      "amigos-amigas-energia-3-4-cartilla-actividad.txt Ya ha sido procesado anteriormente.\n",
      "amigos-amigas-energia-3-4-cuaderno-trabajo.txt Ya ha sido procesado anteriormente.\n",
      "areas-verdes-escuela.txt Ya ha sido procesado anteriormente.\n",
      "cambio-climatico.txt Ya ha sido procesado anteriormente.\n",
      "energizate-planes-clase.txt Ya ha sido procesado anteriormente.\n",
      "guia_espacio_vida_estudiantes_iii.txt Ya ha sido procesado anteriormente.\n",
      "guia_espacio_vida_estudiantes_iv.txt Ya ha sido procesado anteriormente.\n",
      "mi-cuaderno-autoaprendizaje-cya-1.txt Ya ha sido procesado anteriormente.\n",
      "mi-cuaderno-autoaprendizaje-cya-2.txt Ya ha sido procesado anteriormente.\n",
      "mi-cuaderno-autoaprendizaje-cya-3.txt Ya ha sido procesado anteriormente.\n",
      "proyecto-de-aprendizaje-inicial.txt Ya ha sido procesado anteriormente.\n",
      "comunicacion-situaciones-comunicativas-portafolio-inicial-1.txt Ya ha sido procesado anteriormente.\n",
      "comunicacion1-cuaderno-trabajo.txt Ya ha sido procesado anteriormente.\n",
      "comunicacion2-cuaderno-trabajo.txt Ya ha sido procesado anteriormente.\n",
      "comunicacion3-cuaderno-trabajo.txt Ya ha sido procesado anteriormente.\n",
      "comunicacion4-cuaderno-trabajo.txt Ya ha sido procesado anteriormente.\n",
      "comunicacion5-cuaderno-trabajo.txt Ya ha sido procesado anteriormente.\n",
      "comunicacion6-cuaderno-trabajo.txt Ya ha sido procesado anteriormente.\n",
      "cuadernillo-alfabetizacion-inicial-comunicacion1.txt Ya ha sido procesado anteriormente.\n",
      "cuadernillo-alfabetizacion-inicial-comunicacion2.txt Ya ha sido procesado anteriormente.\n",
      "cuadernillo-alfabetizacion-inicial-comunicacion3.txt Ya ha sido procesado anteriormente.\n",
      "cuadernillo_salida2_comunicacion_4to_grado.txt Ya ha sido procesado anteriormente.\n",
      "cuaderno_trabajo_5.txt Ya ha sido procesado anteriormente.\n",
      "cuaderno_trabajo_6.txt Ya ha sido procesado anteriormente.\n",
      "guia-metodologica-primaria.txt Ya ha sido procesado anteriormente.\n",
      "kit-evaluacion-demostrando-aprendimos-2do-primaria-comunicacion-1trimestre-entrada1.txt Ya ha sido procesado anteriormente.\n",
      "kit-evaluacion-demostrando-aprendimos-2do-primaria-comunicacion-1trimestre-entrada2.txt Ya ha sido procesado anteriormente.\n",
      "kit-evaluacion-demostrando-aprendimos-2do-primaria-comunicacion-2trimestre-proceso1.txt Ya ha sido procesado anteriormente.\n",
      "kit-evaluacion-demostrando-aprendimos-2do-primaria-comunicacion-2trimestre-proceso2.txt Ya ha sido procesado anteriormente.\n",
      "kit-evaluacion-demostrando-aprendimos-2do-primaria-comunicacion-3trimestre-salida1.txt Ya ha sido procesado anteriormente.\n",
      "kit-evaluacion-demostrando-aprendimos-2do-primaria-comunicacion-3trimestre-salida2.txt Ya ha sido procesado anteriormente.\n",
      "kit-evaluacion-demostrando-aprendimos-4to-primaria-comunicacion-1trimestre-entrada1.txt Ya ha sido procesado anteriormente.\n",
      "kit-evaluacion-demostrando-aprendimos-4to-primaria-comunicacion-1trimestre-entrada2.txt Ya ha sido procesado anteriormente.\n",
      "kit-evaluacion-demostrando-aprendimos-4to-primaria-comunicacion-2trimestre-proceso1.txt Ya ha sido procesado anteriormente.\n",
      "kit-evaluacion-demostrando-aprendimos-4to-primaria-comunicacion-2trimestre-proceso2.txt Ya ha sido procesado anteriormente.\n",
      "kit-evaluacion-demostrando-aprendimos-4to-primaria-comunicacion-3trimestre-salida.txt Ya ha sido procesado anteriormente.\n",
      "mi-cuaderno-autoaprendizaje-com-1.txt Ya ha sido procesado anteriormente.\n",
      "mi-cuaderno-autoaprendizaje-com-2.txt Ya ha sido procesado anteriormente.\n",
      "mi-cuaderno-autoaprendizaje-com-3.txt Ya ha sido procesado anteriormente.\n",
      "mi-cuaderno-autoaprendizaje-com-4.txt Ya ha sido procesado anteriormente.\n",
      "mi-cuaderno-autoaprendizaje-com-5.txt Ya ha sido procesado anteriormente.\n",
      "mi-cuaderno-autoaprendizaje-com-6.txt Ya ha sido procesado anteriormente.\n",
      "proyecto-aprendizaje-portafolio-inicial.txt Ya ha sido procesado anteriormente.\n",
      "situaciones-comunicativas-comunicacion-texto-1-inicial.txt Ya ha sido procesado anteriormente.\n",
      "situaciones-comunicativas-texto-2-inicial.txt Ya ha sido procesado anteriormente.\n",
      "situaciones-para-aprender-construir-portafolio-2-intermedio.txt Ya ha sido procesado anteriormente.\n",
      "situaciones-para-aprender-construir-portafolio-intermedio-1.txt Ya ha sido procesado anteriormente.\n",
      "situaciones-para-aprender-construir-texto-2-intermedio.txt Ya ha sido procesado anteriormente.\n",
      "situaciones-para-aprender-construir-texto-3-intermedio.txt Ya ha sido procesado anteriormente.\n",
      "situaciones-para-aprender-construir-texto-intermedio-1.txt Ya ha sido procesado anteriormente.\n",
      "Sin t√≠tulo 1.txt Ya ha sido procesado anteriormente.\n",
      "augusto_leguia.txt Ya ha sido procesado anteriormente.\n",
      "canal_panama.txt Ya ha sido procesado anteriormente.\n",
      "carta_jamaica.txt Ya ha sido procesado anteriormente.\n",
      "cuenca_rio_santa.txt Ya ha sido procesado anteriormente.\n",
      "expansion_roma.txt Ya ha sido procesado anteriormente.\n",
      "fernando_belaunde.txt Ya ha sido procesado anteriormente.\n",
      "haya_torre.txt Ya ha sido procesado anteriormente.\n",
      "historia_geografia.txt Ya ha sido procesado anteriormente.\n",
      "la_cuenca_del_amazonas.txt Ya ha sido procesado anteriormente.\n",
      "la_estructura_demografica_de_la_poblacion.txt Ya ha sido procesado anteriormente.\n",
      "la_organizacion_de_la_economia_en_el_imperio_inca.txt Ya ha sido procesado anteriormente.\n",
      "las_corrientes_marinas.txt Ya ha sido procesado anteriormente.\n",
      "las_proyecciones_cartograficas.txt Ya ha sido procesado anteriormente.\n",
      "manuel_valdes.txt Ya ha sido procesado anteriormente.\n",
      "manuela_saenz.txt Ya ha sido procesado anteriormente.\n",
      "maria_granda.txt Ya ha sido procesado anteriormente.\n",
      "mesoamericanos_mayas_aztecas.txt Ya ha sido procesado anteriormente.\n",
      "polis_griega.txt Ya ha sido procesado anteriormente.\n",
      "pueblos_mediterraneo_griegos.txt Ya ha sido procesado anteriormente.\n",
      "agua_suelo.txt Ya ha sido procesado anteriormente.\n",
      "ambiente-salud-unidad-2-portafolio-3-avanzado.txt Ya ha sido procesado anteriormente.\n",
      "ambiente-salud-unidad-2-portafolio-4-avanzado.txt Ya ha sido procesado anteriormente.\n",
      "ambiente-salud-unidad-2-texto-1-avanzado.txt Ya ha sido procesado anteriormente.\n",
      "ambiente-salud-unidad-2-texto-2.txt Ya ha sido procesado anteriormente.\n",
      "ambiente-salud-unidad-2-texto-3-avanzado.txt Ya ha sido procesado anteriormente.\n",
      "ambiente-salud-unidad-2-texto-4-avanzado.txt Ya ha sido procesado anteriormente.\n",
      "biologia.txt Ya ha sido procesado anteriormente.\n",
      "conciencia-ambiental-globe.txt Ya ha sido procesado anteriormente.\n",
      "espacios-vida-esvi.txt Ya ha sido procesado anteriormente.\n",
      "extranos_mundos.txt Ya ha sido procesado anteriormente.\n",
      "fisica.txt Ya ha sido procesado anteriormente.\n",
      "fragmentos_nuevo_mundo.txt Ya ha sido procesado anteriormente.\n",
      "guiso_fantasmagorico.txt Ya ha sido procesado anteriormente.\n",
      "manejo-residuos-solidos-mares.txt Ya ha sido procesado anteriormente.\n",
      "promoviendo-estilos-vida-alimentacion-saludable.txt Ya ha sido procesado anteriormente.\n",
      "vih_sida_es.txt Ya ha sido procesado anteriormente.\n",
      "20_leguas_viaje_submarino.txt Ya ha sido procesado anteriormente.\n",
      "abel_sanchez.txt Ya ha sido procesado anteriormente.\n",
      "amistad_funesta.txt Ya ha sido procesado anteriormente.\n",
      "antologia-literaria-1.txt Ya ha sido procesado anteriormente.\n",
      "antologia-literaria-2.txt Ya ha sido procesado anteriormente.\n",
      "antologia-literaria-4.txt Ya ha sido procesado anteriormente.\n",
      "antologia-literaria-5.txt Ya ha sido procesado anteriormente.\n",
      "antologia_leer_escribir.txt Ya ha sido procesado anteriormente.\n",
      "arguedas.txt Ya ha sido procesado anteriormente.\n",
      "aventuras_robinson_crusoe.txt Ya ha sido procesado anteriormente.\n",
      "aventuras_tom_sawyer.txt Ya ha sido procesado anteriormente.\n",
      "bella_bestia.txt Ya ha sido procesado anteriormente.\n",
      "bibliotecas_escolares.txt Ya ha sido procesado anteriormente.\n",
      "cartas_sor_juana.txt Ya ha sido procesado anteriormente.\n",
      "castillo.txt Ya ha sido procesado anteriormente.\n",
      "cesar_vallejo.txt Ya ha sido procesado anteriormente.\n",
      "charles_dickens.txt Ya ha sido procesado anteriormente.\n",
      "clorinda_matto.txt Ya ha sido procesado anteriormente.\n",
      "comunicacion-secundaria-rural-cuaderno-1.txt Ya ha sido procesado anteriormente.\n",
      "corsario_negro.txt Ya ha sido procesado anteriormente.\n",
      "cuaderno-nivelacion-competencias-com-vi.txt Ya ha sido procesado anteriormente.\n",
      "cumbres_borrascosas.txt Ya ha sido procesado anteriormente.\n",
      "estacion-de-las-letras.txt Ya ha sido procesado anteriormente.\n",
      "extrano_caso_doctor_hyde.txt Ya ha sido procesado anteriormente.\n",
      "frankenstein.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-1-m1.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-1-m2.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-1-m3.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-1-m4.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-2-m1.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-2-m2.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-2-m3.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-2-m4.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-3-m1.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-3-m2.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-3-m3.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-3-m4.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-4-m1.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-4-m2.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-4-m3.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-4-m4.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-5-m1.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-5-m2.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-5-m3.txt Ya ha sido procesado anteriormente.\n",
      "hora-literaria-5-m4.txt Ya ha sido procesado anteriormente.\n",
      "imagenes_frases_atrapan.txt Ya ha sido procesado anteriormente.\n",
      "isla_tesoro.txt Ya ha sido procesado anteriormente.\n",
      "madame_bovary.txt Ya ha sido procesado anteriormente.\n",
      "mariategui.txt Ya ha sido procesado anteriormente.\n",
      "memorias_subsuelo.txt Ya ha sido procesado anteriormente.\n",
      "mirada_lince.txt Ya ha sido procesado anteriormente.\n",
      "moby_dick.txt Ya ha sido procesado anteriormente.\n",
      "poe_crimenes.txt Ya ha sido procesado anteriormente.\n",
      "principe_mendigo.txt Ya ha sido procesado anteriormente.\n",
      "ricardo_palma.txt Ya ha sido procesado anteriormente.\n",
      "romeo_julieta.txt Ya ha sido procesado anteriormente.\n",
      "santos_chocano.txt Ya ha sido procesado anteriormente.\n",
      "seis_personajes_busca.txt Ya ha sido procesado anteriormente.\n",
      "viaje_centro_tierra.txt Ya ha sido procesado anteriormente.\n",
      "viajes_gulliver.txt Ya ha sido procesado anteriormente.\n",
      "vida_lazarillo_tormes.txt Ya ha sido procesado anteriormente.\n",
      "vuelta_mundo_80.txt Ya ha sido procesado anteriormente.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-72a4317ec771>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescriptive_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDescriptiveIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdescriptive_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_information_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordInformationIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_word_information_indices_for_one_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyntactic_pattern_density_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSyntacticPatternDensityIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_syntactic_pattern_density_indices_for_one_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyntactic_complexity_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSyntacticComplexityIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_syntactic_complexity_indices_for_one_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnective_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectiveIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_connective_indices_for_one_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Tesis_Chatbot/src/processing/text_complexity_analyzer.py\u001b[0m in \u001b[0;36mcalculate_syntactic_pattern_density_indices_for_one_text\u001b[0;34m(self, text, workers, word_count)\u001b[0m\n\u001b[1;32m    133\u001b[0m         '''\n\u001b[1;32m    134\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DRNP'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spdi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_noun_phrase_density\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DRVP'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spdi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_verb_phrase_density\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DRNEG'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spdi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_negation_expressions_density\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Tesis_Chatbot/src/processing/coh_metrix_indices/syntactic_pattern_density_indices.py\u001b[0m in \u001b[0;36mget_noun_phrase_density\u001b[0;34m(self, text, word_count, workers)\u001b[0m\n\u001b[1;32m     87\u001b[0m                             if pipe not in ['noun phrase tagger', 'tagger', 'parser']]\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_syntactic_pattern_density\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp_counter_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount_noun_phrases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_verb_phrase_density\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Tesis_Chatbot/src/processing/coh_metrix_indices/syntactic_pattern_density_indices.py\u001b[0m in \u001b[0;36m_get_syntactic_pattern_density\u001b[0;34m(self, text, disable_pipeline, sp_counter_function, word_count, workers)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mdensity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_process\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Calculate with multiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mdensity\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msp_counter_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Tesis_Chatbot/.env/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, texts, as_tuples, n_threads, batch_size, disable, cleanup, component_cfg, n_process)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0moriginal_strings_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0mnr_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Tesis_Chatbot/.env/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m_multiprocessing_pipe\u001b[0;34m(self, texts, pipes, n_process, batch_size)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_doc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbyte_doc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbyte_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Tesis_Chatbot/.env/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;31m# The received object is a batch of byte-encoded docs, so flatten them with chain.from_iterable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0mbyte_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrecv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytedocs_recv_ch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_doc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbyte_doc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbyte_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Tesis_Chatbot/.env/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;31m# Cycle channels not to break the order of docs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;31m# The received object is a batch of byte-encoded docs, so flatten them with chain.from_iterable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0mbyte_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrecv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytedocs_recv_ch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_doc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbyte_doc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbyte_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tca = TextComplexityAnalyzer('es')\n",
    "da = ObtainedTextDA()\n",
    "obtained_texts = da.select_all()\n",
    "\n",
    "for ot in obtained_texts:\n",
    "    if ot.descriptive_index is not None and ot.word_information_index is not None and ot.syntactic_pattern_density_index is not None and ot.syntactic_complexity_index is not None and ot.connective_index is not None and ot.lexical_diversity_index is not None and ot.readability_index is not None and ot.referential_cohesion_index:\n",
    "        print(f'{ot.filename} Ya ha sido procesado anteriormente.')\n",
    "    else:\n",
    "        try:\n",
    "            start = time.time()\n",
    "            descriptive_row = tca.calculate_descriptive_indices_for_one_text(ot.text)\n",
    "            word_count = descriptive_row['DESWC']\n",
    "            mean_words_per_sentence = descriptive_row['DESSL']\n",
    "            mean_syllables_per_word = descriptive_row['DESWLsy']\n",
    "            ot.descriptive_index = DescriptiveIndex(**descriptive_row)\n",
    "            print('finished descriptive')\n",
    "            ot.word_information_index = WordInformationIndex(**tca.calculate_word_information_indices_for_one_text(ot.text, word_count=word_count))\n",
    "            print('finished word information')\n",
    "            ot.syntactic_pattern_density_index = SyntacticPatternDensityIndex(**tca.calculate_syntactic_pattern_density_indices_for_one_text(ot.text, word_count=word_count))\n",
    "            print('finished syntactic pattern')\n",
    "            ot.syntactic_complexity_index = SyntacticComplexityIndex(**tca.calculate_syntactic_complexity_indices_for_one_text(ot.text))\n",
    "            print('finished syntactoc complexity')\n",
    "            ot.connective_index = ConnectiveIndex(**tca.calculate_connective_indices_for_one_text(ot.text, word_count=word_count))\n",
    "            print('finished connective')\n",
    "            ot.lexical_diversity_index = LexicalDiversityIndex(**tca.calculate_lexical_diversity_density_indices_for_one_text(ot.text))\n",
    "            print('finished lexical diversity')\n",
    "            ot.readability_index = ReadabilityIndex(**tca.calculate_readability_indices_for_one_text(ot.text, mean_words_per_sentence=mean_words_per_sentence, mean_syllables_per_word=mean_syllables_per_word))\n",
    "            print('finished readability')\n",
    "            ot.referential_cohesion_index = ReferentialCohesionIndex(**tca.calculate_referential_cohesion_indices_for_one_text(text=ot.text))\n",
    "            print('finished referential cohesion')\n",
    "            end = time.time()\n",
    "            da.update(ot) # Save the indices for the current record       \n",
    "            print(f'Tiempo demorado para {ot.filename}: {end - start} segundos.')\n",
    "        except Exception as e:\n",
    "            print(f'{ot.filename} no pudo ser procesado debido a un error en el procesamiento.')\n",
    "            print(str(e))\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing  the data obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = ObtainedTextDA()\n",
    "obtained_texts = da.select_all_as_dataframe()\n",
    "obtained_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis",
   "language": "python",
   "name": "tesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
